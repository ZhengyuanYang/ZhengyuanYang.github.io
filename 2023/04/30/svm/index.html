

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhengyuan Yang">
  <meta name="keywords" content="">
  
    <meta name="description" content="This article discusses the idea of support vector machine.">
<meta property="og:type" content="article">
<meta property="og:title" content="Support Vector Machine(SVM)">
<meta property="og:url" content="http://example.com/2023/04/30/svm/index.html">
<meta property="og:site_name" content="Data Shore">
<meta property="og:description" content="This article discusses the idea of support vector machine.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/Users/kenny_yang/Downloads/1.png">
<meta property="og:image" content="http://example.com/Users/kenny_yang/Downloads/3.png">
<meta property="og:image" content="http://example.com/Users/kenny_yang/Downloads/2.png">
<meta property="og:image" content="http://example.com/Users/kenny_yang/Downloads/4.png">
<meta property="og:image" content="http://example.com/Users/kenny_yang/Downloads/5.png">
<meta property="article:published_time" content="2023-05-01T06:29:48.000Z">
<meta property="article:modified_time" content="2023-05-01T11:49:09.792Z">
<meta property="article:author" content="Zhengyuan Yang">
<meta property="article:tag" content="SVM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/Users/kenny_yang/Downloads/1.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Support Vector Machine(SVM) - Data Shore</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Data Shore</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner.gif') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Support Vector Machine(SVM)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-04-30 23:29" pubdate>
          April 30, 2023 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          11k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          93 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Support Vector Machine(SVM)</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="support-vector-machinesvm">Support Vector Machine(SVM)</h1>
<h2 id="about-support-vector-machine">1. About Support Vector Machine</h2>
<p>Support Vector Machine (SVM) is one of the most popular machine learning algorithms used for classification and regression analysis. SVM is a supervised learning algorithm that can be used for both linear and non-linear data classification. Traditional SVM assumes that the sample space is linearly separable and can be seen as a parametric model. However, with the introduction of kernel methods, SVM can infinitely expand the feature space to approximate nonlinear decision boundaries and is therefore often seen as a nonparametric model.</p>
<h2 id="hard-margin-svm">2. Hard-Margin SVM</h2>
<h3 id="basic-idea">2.1 Basic Idea</h3>
<p><strong>Hyperplane</strong></p>
<p>SVM works by creating a <strong>hyperplane</strong>, which is a line or a series of lines that separate the data into different classes. The objective of SVM is to create a hyperplane that maximizes the <strong>margin</strong>, which is the distance between the hyperplane and the closest data points from each class. The data points that are closest to the hyperplane are called support vectors.</p>
<p><img src="/Users/kenny_yang/Downloads/1.png" srcset="/img/loading.gif" lazyload></p>
<p>Suppose we have a group of data points <span class="math inline">\(\{(x_ i,y_i)\}\)</span>, where <span class="math inline">\(x\)</span> is a vector of features <span class="math inline">\(x_i = (x_{i,1}...x_{i,m})\)</span>, <span class="math inline">\(y_i \in \{-1,1\}\)</span> is the label of the <span class="math inline">\(i^{th}\)</span> sample. Suppose the expression of the hyperplane in space <span class="math inline">\(x\)</span> is <span class="math inline">\(wx+b = 0\)</span>. The case where the model correctly classified a sample is that the sample lies in the correct side of the hyperplane corresponding to its label:</p>
<p><span class="math display">\[
\left \{ \begin{aligned}
&amp;wx+b &gt; 0 , y=1\\
&amp;wx+b &lt;0 , y=-1
\end{aligned}\right.
\]</span></p>
<p>For all data points, we can rewrite this constraint into:</p>
<p><span class="math display">\[
y_i(wx_i+ b) &gt; 0 \qquad \forall \ i
\]</span></p>
<p><strong>Margin</strong></p>
<p>Since the margin is the distance between the hyperplane and the closest data points, the margin for each side of the hyperplane is:</p>
<p><span class="math display">\[
m = min[dist(x_i,wx+b)] = min\frac{|wx_i+b|}{||w^2||}
\]</span></p>
<p>The margin is the objective we want to maximize, since <span class="math inline">\(y_i\)</span> and <span class="math inline">\(wx_ i +b\)</span> has same sign, we can change the objective function to:</p>
<p><span class="math display">\[
\begin{aligned}
m &amp;= min\frac{y_i(wx_i+b)}{||w^2||}\\
&amp;=\frac{1}{||w^2||}min \ y_i(wx_i+b)
\end{aligned}
\]</span></p>
<p>We know there must exist some constant that <span class="math inline">\(min \ y_ i(w_i+b) = c\)</span>, since the scaler of the hyperplane does not change the results of decision of data points, we can let <span class="math inline">\(c = 1\)</span>. Now the objective function becomes <span class="math inline">\(\frac{1}{||w||}\)</span>, and now since <span class="math inline">\(min \ y_ i(w_i+b) = 1\)</span>, the constraint becomes: <span class="math inline">\(y_i(wx_i+ b) \ge 1 \  \forall \ i\)</span>. Since in optimization, we usually do minimization instead of maximization, we can transform the objective function to <span class="math inline">\(\frac{1}{2}||w ||^2\)</span>. Thus, we transform the SVM into a optimization model:</p>
<p><span class="math display">\[
\begin{aligned}
min \ &amp;\frac{||w||^2}{2}\\
s.t. \ &amp;y_i(wx_i+ b) \ge 1 \qquad   \forall i
\end{aligned}
\]</span></p>
<p>Suppose the optimal found are <span class="math inline">\(w^*,b^*\)</span>, then the optimal hyperplane is <span class="math inline">\(w^*x+b^*\)</span>, the final decision boundary would be <span class="math inline">\(f(x) = sign(w^*x+b^*)\)</span></p>
<h3 id="dual-problem-of-svm">2.2 Dual Problem of SVM</h3>
<p>The optimization proposed in the last section is a convex optimization problem, we can solve it by solving its dual problem. By using the Lagrange multiplier method, we construct <span class="math inline">\(L(w,b,\lambda) = \frac{||w||^2}{2} +\sum_i^n\lambda_i(1-y_i(wx_i+b))\)</span>, we can write the dual form of the original problem：</p>
<p><span class="math display">\[
\begin{aligned}
max \ &amp;-\frac{1}{2}\sum_i^n\sum_j^n \lambda_i\lambda_jy_iy_jx_i\cdot x_j^T + \sum_i^n \lambda_i\\
s.t. \qquad \lambda_i &amp;\ge 0 \qquad  \forall \ i\\
\sum_i^n\lambda_iy_i &amp;= 0 \\
\end{aligned}
\]</span></p>
<p>In fact, we can prove that the primal problem and the dual problem are both QP problems, so they have strong duality and satisfy the KKT conditions:</p>
<p><span class="math display">\[
\left \{\begin{aligned}
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial b} = \frac{\partial L}{\partial \lambda} = 0\\
\lambda_i(1-y_i(wx_i+b))=0\\
\lambda_ i \ge 0\\
1-y_i(wx_i+b) \le 0
\end{aligned}\right.
\]</span></p>
<p>These conditions promise the following properties:</p>
<ul>
<li>For any points that <span class="math inline">\(wx_i+b &gt; 1\)</span>, we have <span class="math inline">\(1-y_i(wx_i+b) &lt; 0\)</span>, then <span class="math inline">\(\lambda_i = 0\)</span>, this point has no contribution to the optimization problem.</li>
<li>For any points that <span class="math inline">\(wx_i+b = 1\)</span>, <span class="math inline">\(\lambda_i\)</span> can be greater than 0. These points are the support vectors, they have contribution to the optimization</li>
<li><span class="math inline">\(w^ * = \sum_ i\lambda_iy_ix_i, \ b^* = y_k - \sum_i\lambda_iy_ix_i \cdot x_k^T\)</span>, where <span class="math inline">\((x_k,y_k)\)</span> is any points that is a support vector.</li>
</ul>
<p>These properties indicate that the optimal solution of the dual problem is sparse, and only support vectors are meaningful for optimization. As optimization progresses, the support vectors are gradually found, and the calculation of the objective function will become faster.</p>
<p>The mainly benefits of using dual problem is that it is easier to solve. The number of variables in the original problem depends on the feature dimensionality <span class="math inline">\(d\)</span> of the data points. When we use kernel methods and other techniques, the number of variables to be solved is very large. However, after converting to the dual problem, the number of variables to be solved depends on the number of samples <span class="math inline">\(n\)</span>, and the calculation of inner products in the objective function can be simplified using kernel tricks. Therefore, the dual problem is often easier to solve.</p>
<h2 id="soft-margin-svm">3. Soft-Margin SVM</h2>
<p>Hard-Margin SVM assumes that the dataset is linearly separable. However, in practical applications, this assumption is often not satisfied due to noise and other factors. To address this issue, Soft-Margin SVM is introduced, which allows for some misclassified points to exist between support vectors. It uses the idea of regularization, relaxes the constraints of the optimization problem, and penalizes misclassified points as a penalty term in the objective function:</p>
<p><span class="math display">\[
min\ \frac{1}{2}||w||^2 + \sum_iloss(x_i)
\]</span></p>
<p>If we directly use the number of misclassified points as the penalty term, then <span class="math inline">\(loss(x_i)\)</span> will be discontinuous with respect to <span class="math inline">\(w\)</span>. Therefore, we define:</p>
<p><span class="math display">\[
\xi_i = loss(x_i) = \left \{ \begin{aligned}
&amp;0  \ &amp;y_i(wx_i+b) \ge 1\\
&amp;1-y_i(wx_i+b)  \ &amp;y_i(wx_i+b) &lt; 1\\
\end{aligned}\right.
\]</span></p>
<p>we can rewrite it into:</p>
<p><span class="math display">\[
\xi_i = max[0,1-y_i(wx_i+b)]
\]</span></p>
<p>We can use a parameter C in <span class="math inline">\([0,\infty)\)</span> to control the extent of penalty. The objective function and constraints now become:</p>
<p><span class="math display">\[
min\ \frac{1}{2}||w||^2 + C\sum_i\xi_i
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
min\ &amp;\frac{1}{2}||w||^2 + C\sum_i\xi_i\\
s.t. \ &amp;y_i(wx_i+ b) \ge 1-\xi_ i \qquad   \forall i
\end{aligned}
\]</span></p>
<p>This function is also called <strong>hinge loss</strong></p>
<h2 id="kernel-method-in-svm">4. Kernel Method in SVM</h2>
<h3 id="kernel-method">4.1 Kernel Method</h3>
<p>SVM assumes the dataset is linear separable. However, in most application it is not the case. If there’s only slight non-linearity in the dataset, we can use soft margin SVM to solve problem. However , when the dataset is strictly not linear separable, we need to introduce kernel method. Kernel Method is a way to solve nonlinear problems by mapping the original data to a higher-dimensional space where the problem becomes linearly separable.</p>
<p>Suppose the original feature space is <span class="math inline">\(d = (x_1,x_2)\)</span>, let <span class="math inline">\(\phi(x)\)</span> denote a high dimensional mapping that <span class="math inline">\(\phi(x) = (x_1,x_2,(x_1-x_2)^2)\)</span>), then we would get such effect:</p>
<p><img src="/Users/kenny_yang/Downloads/3.png" srcset="/img/loading.gif" lazyload></p>
<p>Note that every dimension of the new space can be a non-linear transformation(usually non-linear), we do have to retain the original feature x. By using kernel methods, SVM can handle nonlinear problems and provide high accuracy for a wide range of applications.</p>
<p><img src="/Users/kenny_yang/Downloads/2.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="kernel-function">4.2 Kernel Function</h3>
<p>By transforming SVM into a dual problem, we make the variables to be optimized dependent on the number of samples <span class="math inline">\(n\)</span>, but when calculating the objective function, we still need to compute the dot product <span class="math inline">\(\phi(x_i) \cdot \phi(x_j^T)\)</span>. When using kernel tricks, if <span class="math inline">\(x\)</span> is mapped to a high-dimensional space, computing the dot product can still be very time-consuming.</p>
<p>Now, let us assume that a function satisfies the following property: suppose <span class="math inline">\(x_i,x_j\)</span> are two data points in the original space, and <span class="math inline">\(\phi(x)\)</span> can map data points in the original space to a high-dimensional space <span class="math inline">\(z\)</span>. If there exists a function <span class="math inline">\(K\)</span> such that:</p>
<p><span class="math display">\[
K(x_i,x_j) = \phi(x_i)\phi(x_j)^T
\]</span></p>
<p>So we call the function <span class="math inline">\(K(x_i,x_j)\)</span> a <strong>positive definite kernel function</strong>. Usually, the kernel function is what we use in practical applications, so we simply refer to it as the kernel function. The kernel function can transform the calculation of the vector inner product <span class="math inline">\(\phi(x_i) \cdot \phi(x_j^T)\)</span> in the dual problem objective function into a function calculation <span class="math inline">\(K(x_i,x_j)\)</span>. This greatly simplifies the computational complexity of dual problem optimization, making it no longer dependent on the dimension of <span class="math inline">\(\phi(x)\)</span>, but rather dependent on the number of samples. We know that the inner product of two vectors represents the angle between the two vectors in space, that is, the similarity of the two vectors. Therefore, the kernel function actually represents the similarity of two samples in some high-dimensional space.</p>
<p>To determine whether a function is a positive definite kernel function, we mainly use the two properties of positive definite kernel functions.</p>
<ul>
<li>Symmetry：<span class="math inline">\(K(x_ i, x_j) = K(x_j,x_i)\)</span></li>
<li>Positivity：for any <span class="math inline">\(n\)</span> points <span class="math inline">\((x_1,…x_n)\)</span>, the gram matrix is a positive semi-definite matrix</li>
</ul>
<p><span class="math display">\[
G = \begin{bmatrix}
K(x_1,x_1) &amp; ... &amp;K(x_1,x_n)\\
... &amp; K(x_i,x_j) &amp;...\\
K(x_n,x_1) &amp;... &amp;K(x_n,x_n)
\end{bmatrix}
\]</span></p>
<p>Common kernel function includes:</p>
<ul>
<li>Linear Kernel: <span class="math inline">\(K(x_i,x_j) = x_i \cdot x_j\)</span></li>
<li>Polynomial Kernel: <span class="math inline">\(K(x_i,x_j) = (ax_i \cdot x_j + b)^d\)</span></li>
<li>Gaussian RBF Kernel: <span class="math inline">\(K(x_i,x_j) = \exp(-\frac{||x_i-x_j||^2}{2\sigma^2}) = \exp(-\gamma||x_i-x_j||^2)\)</span>,</li>
<li>Sigmoid Kernel: <span class="math inline">\(K(x_i,x_j) = tanh(\gamma x_i\cdot x_j+r)\)</span></li>
</ul>
<p>where <span class="math inline">\(\gamma\)</span> represents an adjustable hyper-parameter.</p>
<p>For SVMs that use kernel tricks, the final decision boundary is:</p>
<p><span class="math display">\[
f(x) = sign(\sum_i^n\lambda_i y_i K(x,x_i)+b )
\]</span></p>
<h2 id="support-vector-regression">5. Support Vector Regression</h2>
<p>SVM can also be used for regression analysis, called Support Vector Regression (SVR). SVR is particularly useful for datasets with a large number of features and complex nonlinear relationships between the features and the target variable, as it can apply kernel method as well.</p>
<p>Suppose we want to regress a linear hyperplane <span class="math inline">\(y = wx+ b\)</span> . Now we decide we can tolerate some error if the distance between the true data point <span class="math inline">\(y_i\)</span> and the prediction <span class="math inline">\(\hat{y_i }\)</span> if they are close enough. Then the modified MAE loss function is:</p>
<p><span class="math display">\[
\xi_i = loss(x_i) = \left \{ \begin{aligned}
&amp;0  \ &amp;|y_i-\hat{y_i}| \le \epsilon\\
&amp;|y_i-\hat{y_i}|  \ &amp;|y_i-\hat{y_i}| &gt; \epsilon\\
\end{aligned}\right.
\]</span></p>
<p><img src="/Users/kenny_yang/Downloads/4.png" srcset="/img/loading.gif" lazyload></p>
<p>We call the region within <span class="math inline">\(\epsilon\)</span> distance from the hyperplane the <strong><span class="math inline">\(\epsilon\)</span>-insensitivity zone.</strong> Ideally, we want all data points to fall within this zone (constraint). During training, we can treat the points outside of this region as penalty terms in the optimization process (slack variables). We add a regularization term <span class="math inline">\(\frac{1}{2}||\omega||^2\)</span> in the loss function, the training of the regression model can then be written as:</p>
<p><span class="math display">\[
\begin{aligned}
min\ &amp;\frac{1}{2}||w||^2 + C\sum_i\xi_i\\
s.t. \ &amp;|y_ i - (wx_i+ b)| \le \epsilon +\xi_ i \qquad   \forall i
\end{aligned}
\]</span></p>
<p>We found that the optimization problem is very similar to the form of Soft Margin SVM, and we can use the same techniques, kernel functions, dual problem solving methods, etc. to solve this problem. We call such a model a <strong>Support Vector Regressor（SVR）</strong>, the data points lie out of the zone is the support vectors.</p>
<p><img src="/Users/kenny_yang/Downloads/5.png" srcset="/img/loading.gif" lazyload></p>
<p>By discussing different cases, removing absolute value signs, we can transform the above problem into a convex problem. For each data point <span class="math inline">\(x_i\)</span>, we introduce two slack variables <span class="math inline">\(\xi_i^+\)</span> and <span class="math inline">\(\xi_i^-\)</span>, which represent the distance from the point to the upper boundary of the <span class="math inline">\(\epsilon\)</span>-insensitive zone and the distance from the point to the lower boundary of the <span class="math inline">\(\epsilon\)</span>-insensitive zone, respectively. If a data point is not within the <span class="math inline">\(\epsilon\)</span>-insensitive zone, then it can only be either above or below the zone, and thus at most one of <span class="math inline">\(\xi_i^+\)</span> and <span class="math inline">\(\xi_i^-\)</span> can be non-zero for the same data point. We can then formulate the optimization problem as follows:</p>
<p><span class="math display">\[
\begin{aligned}
min\ \frac{1}{2}||w||^2 + C\sum_i（\xi_i）\\
s.t. \ y_ i - (wx_i+ b) &amp;\le \epsilon +\xi_i^+ \qquad   \forall i\\
\ (wx_i+ b)-y_ i &amp;\le \epsilon +\xi_i^- \qquad   \forall i\\
\xi_ i^+ &gt;0, \xi_i^- &gt;0
\end{aligned}
\]</span></p>
<p>We can also derive the dual form of SVR using the Lagrange multiplier method, which is not listed here. Through the dual form, the optimal hyperplane can be obtained as:</p>
$$
<span class="math display">\[\begin{aligned}
f(x) &amp;= \sum_i(\lambda_i^+-\lambda_i^-)x\cdot x_i^T +b^*\\

\end{aligned}\]</span>
<p>$$</p>
<p><span class="math display">\[
b^* = \left\{\begin{aligned}
-\sum_i(\lambda_i^+-\lambda_i^-)x_i \cdot x_k^T+\epsilon+y_k, \ 0\le\lambda_k^+\le C\\
-\sum_i(\lambda_i^+-\lambda_i^-)x_i \cdot x_k^T-\epsilon+y_k, \ 0\le\lambda_k^-\le C\\
\end{aligned}\right.
\]</span></p>
<p>where <span class="math inline">\((x_ k, y_ k )\)</span> is the closest support vector.</p>
<p>When applying kernel method, the optimal hyperplane is:</p>
$$
<span class="math display">\[\begin{aligned}
f(x) &amp;= \sum_i(\lambda_i^+-\lambda_i^-)K(x,x_i) +b^*\\

\end{aligned}\]</span>
<p>$$</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Machine-Learning/" class="category-chain-item">Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/SVM/">#SVM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Support Vector Machine(SVM)</div>
      <div>http://example.com/2023/04/30/svm/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Zhengyuan Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>April 30, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/05/11/PSM/" title="Propensity Score Matching">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Propensity Score Matching</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/04/23/markov-chain/" title="Basics of Markov Chain">
                        <span class="hidden-mobile">Basics of Markov Chain</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <i class="iconfont icon-bug"></i> <a href="http://zhengyuanyang.com/report/" target="_blank" rel="nofollow noopener"><span>Report Bug</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
