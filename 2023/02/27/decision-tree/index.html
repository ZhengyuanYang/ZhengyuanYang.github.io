

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhengyuan Yang">
  <meta name="keywords" content="">
  
    <meta name="description" content="This article discuss about Linear Regression and Generalized Linear Model">
<meta property="og:type" content="article">
<meta property="og:title" content="Decision Tree and Ensemble Models">
<meta property="og:url" content="http://example.com/2023/02/27/decision-tree/index.html">
<meta property="og:site_name" content="Data Shore">
<meta property="og:description" content="This article discuss about Linear Regression and Generalized Linear Model">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/02/27/decision-tree/1.png">
<meta property="og:image" content="http://example.com/2023/02/27/decision-tree/2.png">
<meta property="og:image" content="http://example.com/2023/02/27/decision-tree/3.png">
<meta property="og:image" content="http://example.com/2023/02/27/decision-tree/4.png">
<meta property="og:image" content="http://example.com/2023/02/27/decision-tree/5.png">
<meta property="og:image" content="http://example.com/2023/02/27/decision-tree/6.png">
<meta property="og:image" content="http://example.com/2023/02/27/decision-tree/7.png">
<meta property="article:published_time" content="2023-02-27T20:48:59.000Z">
<meta property="article:modified_time" content="2023-03-03T11:49:16.450Z">
<meta property="article:author" content="Zhengyuan Yang">
<meta property="article:tag" content="Decisions Tree">
<meta property="article:tag" content="Ensemble Model">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2023/02/27/decision-tree/1.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Decision Tree and Ensemble Models - Data Shore</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Data Shore</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Toolbox/">
                <i class="iconfont icon-briefcase"></i>
                Toolbox
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner.gif') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Decision Tree and Ensemble Models"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-02-27 12:48" pubdate>
          February 27, 2023 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          26k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          219 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Decision Tree and Ensemble Models</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="decision-tree-and-ensemble-models">Decision Tree and Ensemble Models</h1>
<h2 id="decision-tree">1. Decision Tree</h2>
<h3 id="about-decision-tree">1.1 About Decision Tree</h3>
<p>Decision Tree is a data structure that can split data into subsets. In Machine Learning, we can use such a structure as our hypothesis space to segment our datasets into units and perform prediction.</p>
<p><img src="/2023/02/27/decision-tree/1.png" srcset="/img/loading.gif" lazyload></p>
<p>A decision tree consists of decision nodes and leaf nodes. Each decision node represent a split of sample based on some conditions a input feature <span class="math inline">\(X_i\)</span>. A directed route of the decision nodes represent a series of <strong>conditioning</strong> on the sample sapce. Each leaf node represents a subsample space obtained from a unique route, and we can estimate the conditional probability distribution <span class="math inline">\(P(Y|X=x)\)</span> from the leaf node. The output variable Y can be either continuous(regression) or discrete(classification).</p>
<p>The decision tree is a non-parameter model. It can be infinitely extended without a constraint on max tree depth.</p>
<h3 id="training-process">1.2 Training Process</h3>
<p>The training process of decision tree is a recursive process. Let D denote the original sample set, X denote the feature spaces</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">def splitting(tree):<br>		while impurity metrics is not reached:<br>				x+i = select a feature x_i in feature space X(tree)<br>				sub_tree_left,sub_tree_right = find the best split based on x_i(tree, x_i)<br>				tree.left = splitting(sub_tree_left)<br>				tree.right = splitting(sub_tree_right)<br>	  return tree<br><br>splitting(D)<br><br>def label_and_predict(tree, sample):<br>		for leaf in tree:<br>				if a classification_tree:<br>						# use the mode of Y of training samples belong to this leaf node as the prediction<br>						leaf.pred = leaf.Y.mode()<br>				else:<br>						# use the mean of Y of training samples belong to this leaf node as the prediction<br>						leaf.pred = leaf.Y.mean()<br>	<br>		leaf_node_for_sample = find the leaf node the sample belong to according to sample&#x27;s X<br>		sample_pred = leaf_node.pred<br><br></code></pre></td></tr></table></figure>
<p><img src="/2023/02/27/decision-tree/2.png" srcset="/img/loading.gif" lazyload></p>
<p>The implementation of Decision Tree includes ID3, C4.5 and CART. The differences of these implementations are the strategy they select layer(feature) and best split.</p>
<h2 id="implementation-of-decision-tree">2. Implementation of Decision Tree</h2>
<h3 id="id3">2.1 ID3</h3>
<p>ID3 is a decision tree that can deal only with <strong>finite discrete inputs</strong>(usually <strong>categorical features</strong>). Suppose we have a sample set D and a feature space X, where <span class="math inline">\(X_i\)</span> is a finite discrete feature like sex, country. If <span class="math inline">\(X_i\)</span> has infinite values or is continues, then the ID3 requires the feature to be binned into finite discrete sections.</p>
<h4 id="feature-selection-and-splitting">2.1.1 Feature Selection and Splitting</h4>
<p>The strategy of selecting feature to split is based on Information Gain. For the explanation of IG, refer to <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">this article</a>.</p>
<p>For each feature <span class="math inline">\(X_i\)</span>: <span class="math display">\[
\begin{aligned}
IG(X_i) &amp;= H(D) - H(D|X_i) \\
&amp;= -\sum_m^k p(y=c_m)log(p(y=c_m)) - (-\sum_n\sum_m p(y=c_m|X_i = x_n)p(X=x_n)log(p(y=c_m|X_i = x_n)))
\end{aligned}
\]</span> According to the definition, the feature with the largest imformation can reduce the uncertainty of Y to the greatest extent, thus would be selected as the feature to split. There is no specific techniques in finding best split, since all features are categorical, just split the tree into n nodes, where n is the number of categories of feature <span class="math inline">\(X_i\)</span></p>
<h4 id="properties">2.1.2 Properties</h4>
<ul>
<li>ID3 does not have any pruning strategy</li>
<li>ID3 perfer features with large number of category</li>
<li>ID3 can only deal with finite discrete inputs if no binning is implemented.</li>
<li>ID3 cannot deal with missing values</li>
<li>Since or feature is catrgorical, once a featrure is selected a layer, it would not appear in the following subtrees any more.</li>
</ul>
<h3 id="c4.5">2.2 C4.5</h3>
<p>C4.5 is a improved version of ID3. One drawback of ID3 is that it would perfer features with large number of category. For example, if there is a unique identity number for each sample, then that feature's IG would be 1, but this provide no clues for future's prediction.</p>
<h4 id="feature-selection-and-splitting-1">2.2.1 Feature selection and splitting</h4>
<p>To solve this, the C4.5 use Information Gain Rate to select feature: <span class="math display">\[
IGR(X_i) = \frac{IG(X_i)}{H(D)}
\]</span> this would penalize those features with to many categories, as these features usually have larger entropy. However, this could bring the algorithm to the opposite that it would prefer feature with less number of categories. This means the absolute values of IG can be low while the IGR is high. To fix this, the C4.5 would first do a feature selection based on IG:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">for X_i in X:<br>		if IG(X_i) &lt; IG(X).avg()<br>		X.drop(X_i)<br></code></pre></td></tr></table></figure>
<p>In addition, the C4.5 introduce different techniques to deal with selection on continuous input feature, for a continuous feature <span class="math inline">\(X_i\)</span>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">for X_i in X<br>		if X_i is continuous:<br>      Sort all sample by X_i in a ascending order<br>      for each adjacent samples<br>          m_j = the medium of X_i_j,X_i_(j+1) #use the medium as a threshold<br>          #Z is a label flagged whether X_i of a sample is above the threshold<br>          Z = Z_right if x_i_j &lt; m_j else Z_right <br>          IGR_j = IGR(D, Z) #A binary split based on Z<br>      # The split with the highest IGR is the best split of feature X_i<br>      IGR_best_i = max(IGR_j)<br>    else:<br>    		IGR_best_i = calculate IGR(X_i) as a categorical variable like ID3<br>    <br># Compare all<br>the feature to select = argmax_Xi(IGR_best_i)<br>		<br></code></pre></td></tr></table></figure>
<p>The process put feature selection and feature splitting together. It first use a enlightening method to filter some feature with low IG. then calculate the IGR for the rest of the features. For a continuous feature, it use the split with highest IGR as the best split. For a categorical feature, it still generate a child node for each category.</p>
<h4 id="pruning-strategy">2.2.2 Pruning Strategy</h4>
<p>One drawback ID3 is that it does not have a pruning strategy, thus can be easy to overfit. In C4.5, post-pruning is applied to control the complexity of the model. The post-pruning means the algorithm would first make the split and then decide whether to prune the subtrees based on the error rate on the leaf node. Specifically, the post-pruning strategy is a method called Pessimistic Error Pruning.</p>
<p>For a decision node, suppose the algorithm first split <span class="math inline">\(n_L\)</span> child nodes based on a feature. Then:</p>
<ul>
<li>Let <span class="math inline">\(E_i\)</span> denote the number of false classification in the <span class="math inline">\(i^{th}\)</span> node</li>
<li>Let <span class="math inline">\(N_i\)</span> denote the number of samples in the <span class="math inline">\(i^{th}\)</span> node</li>
<li>Let <span class="math inline">\(\gamma\)</span> be a penalty parameters for complexity of model</li>
</ul>
The error rate of splitting and treat the node as a subtree: <span class="math display">\[
e_{tree} = \frac{ \sum_iE_i + \gamma*n_L}{\sum_iN_i}
\]</span> Consider a random variable <span class="math inline">\(M_{tree}\)</span> denoting the number of misclassification. For example, if the output variable Y is binary, we can consider <span class="math inline">\(M_{tree} \sim Bino(e,\sum_i N_i)\)</span> $$
<span class="math display">\[\begin{aligned}
E[M_{tree}] &amp;= \sum_iN_i *e_{tree}\\
Std[M_{tree}] &amp;= \sqrt{\sum_i N_i*e_{tree}*(1-e_{tree})}

\end{aligned}\]</span>
<p><span class="math display">\[
 Now apply pruning and replace the subtree with a leaf node. Let E, N denote the number of misclassified samples and all samples in the leaf node, calculate the error rate if not splitting and treat the node as a leaf node:
\]</span> e_{leaf} =  <span class="math display">\[
Accordingly:
\]</span> E[M_{leaf}] = N*e_{leaf} <span class="math display">\[
If $M_{leaf}$ is significant greater than $M_{tree}$, we can make the judgement that the splitting is necessary. When N is big, the binomial distribution is approximate to a Normal distribution. Thus, through a test,the condition of accepting a splitting is:
\]</span> p = T_test(E[M_{leaf}] - E[M_{tree}])  $$ Otherwise, the subtree should be pruned.</p>
<h4 id="missing-value-processing">2.2.3 Missing Value Processing</h4>
<p>When there exists missing values in features, two problem needs to be solved by the decision tree:</p>
<ul>
<li>How to calculate the IG/IGR of <span class="math inline">\(X_i\)</span> when there are missing values in <span class="math inline">\(X_i\)</span>?</li>
<li>When training, how to decide which sub-node the sample containing missing value belong?</li>
<li>When predicting, how to label the sample when it meet a node that based on its missing values?</li>
</ul>
<p>For the first question, the solution by C4.5 is to calculate the IGR on samples without missing values and then add a weight to the IGR. Let the weight be: <span class="math display">\[
\rho  = \frac{n_{\widetilde{D}}}{n_D}
\]</span> where <span class="math inline">\(D, \widetilde{D}\)</span> is the orginal sample set and a subset that samples with missing values of <span class="math inline">\(X_i\)</span> are dropped. <span class="math inline">\(n\)</span> denote the number of samples. The revised IGR would then be: <span class="math display">\[
IGR(X_i)&#39;/IG(X_i)&#39; = \rho * IGR(X_i)/IG(X_i)
\]</span></p>
<p>For the second question, let <span class="math inline">\(w_k\)</span> denote the fraction of samples that belong to the <span class="math inline">\(k^{th}\)</span> category(or value for continuous variable): <span class="math display">\[
\omega_k = \frac{n_k}{n_ {\widetilde{D}}}
\]</span> If a sample's value on <span class="math inline">\(X_i\)</span> is missing when trying to split the samples based on <span class="math inline">\(X_i\)</span>, then those missing value samples would <strong>enter every child node with a weight <span class="math inline">\(\omega_k\)</span></strong>. This means, when further splitting the nodes, the <span class="math inline">\(P(y = c_m)\)</span> in the information gain now calculated as: <span class="math display">\[
P(y = c_m) = \frac{\sum \omega_{k, y= c_m}}{\sum \omega_k}
\]</span> and the weight of information gain would also be: <span class="math display">\[
\rho = \frac{\sum\omega_\widetilde{D}}{\sum \omega_D}
\]</span> For example, suppose we have the following dataset</p>
<p><img src="/2023/02/27/decision-tree/3.png" srcset="/img/loading.gif" lazyload></p>
<p>In this first split, <span class="math inline">\(\widetilde{D} = \{1,2,3,4,5,7,8,,9,10,11,12,13,14\}\)</span>, <span class="math inline">\(\rho = \frac{13}{14}\)</span>, then <span class="math display">\[
\begin{aligned}
H(\tilde{D}) &amp;= [-\frac{5}{13}*log(\frac{5}{13})-\frac{3}{13}*log(\frac{3}{13})-\frac{5}{13}*log(\frac{5}{13})]\\
IGR &amp;= \frac{H(\tilde{D}) - H(\tilde{D}|outlook)}{H(\tilde{D})}\\
IGR&#39; &amp;= \frac{13}{14}*IGR
\end{aligned}
\]</span> Suppose outlook is the feature with max IGR'. Let split the sample based on outlook. Since there is a sample 6 that missing value "outlook", let will enter each node with a weight. <span class="math inline">\(\omega_s = \frac{5}{13}, \omega_o = \frac{3}{13}, \omega_r = \frac{5}{13}\)</span></p>
<p>Now move the subtree that "outlook = sunny"</p>
<p><img src="/2023/02/27/decision-tree/4.png" srcset="/img/loading.gif" lazyload></p>
<p>suppose we want to further split the tree and we want to calculate the IGR of "Windy", now: <span class="math display">\[
\begin{aligned}
\rho &amp;= \frac{5+\frac{5}{13}}{5+\frac{5}{13}} = 1\\
H(\tilde{D_s}) &amp;= [-\frac{2+\frac{5}{13}}{5+\frac{5}{13}}*log(\frac{2+\frac{5}{13}}{5+\frac{5}{13}})-\frac{3}{5+\frac{5}{13}}*log(\frac{3}{5+\frac{5}{13}})]\\
IGR &amp;= \frac{H(\tilde{D_s}) - H(\tilde{D_s}|Windy)}{H(\tilde{D_s})}\\
IGR&#39; &amp;= 1*IGR 
\end{aligned}
\]</span></p>
<p>Finally, for the third question, when we try to make the prediction. Suppose we have a decision tree like this:</p>
<p><img src="/2023/02/27/decision-tree/5.png" srcset="/img/loading.gif" lazyload></p>
<p>Now we have a sample that "outlook = sunny" and "humidity = null". To decide the label of this sample, we just calculate the conditional probability distribution of Y when the sample meet the node it is missing value of.</p>
<p>For the humidity node: <span class="math display">\[
\begin{aligned}
P(Class = play|sunny) =&amp; P(Class = Play|Humidity \le 75|sunny) P(Humidity \le 75|sunny) \\
&amp;+ P(Class = Play|Humidity &gt; 75|sunny) P(Humidity &gt; 75|sunny)\\
=&amp;\frac{2}{5.4}*1+\frac{3.4}{5.4}*\frac{3}{3.4} = 44\%
\end{aligned}
\]</span></p>
<p><span class="math inline">\(P(Class = Play) &lt; P(Class = not \ play)\)</span>, thus the prediction is "play"</p>
<h4 id="properties-1">2.2.4 Properties</h4>
<ul>
<li>C4.5 is a classification algorithm. Although it deal with continuous features, the output features still needs to be categorical</li>
<li>For continuous feature, C4.5 sort the samples and split the samples into 2 parts. The continuous feature can be applied again in the subtrees.</li>
<li>For categorical feature, C4.5 still split the samples into n parts. The categorical feature won't be applied again in the subtrees.</li>
<li>C4.5 is still a Multi-tree. It has lower efficiency than CART.</li>
</ul>
<h3 id="classification-and-regression-treecart">2.3 Classification and Regression Tree(CART)</h3>
<p>The C4.5 algorithm can only deal with classification tasks, and the efficiency of it is low. CART is a improved algorithm for C4.5.</p>
<h4 id="feature-selection-and-splitting-2">2.3.1 Feature selection and splitting</h4>
<p>CART has similar processing like C4.5, which sort the samples in an ascending order and find a best split to segment the samples in 2 parts. The difference is CART applies such method on all feature, not just continuous features, and it provides a different metrics to measure a split.</p>
<p>In addition, CART can do both regression and classification. When the output variable is continuous, it use a combined MSE to measure the error of a split: <span class="math display">\[
MSE = [\sum_{i \in D_{left}}(y_i-\bar{y}) + \sum_{i \in D_{right}}(y_i-\bar{y})]
\]</span> When doing classification, CART use the <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2022/09/22/loss-function/#gini-impurity">Gini Index</a> as a metric: <span class="math display">\[
G = 1- \sum_i^mP(y = c_m)
\]</span></p>
<p><span class="math display">\[
Combined \ Gini = G_{left} + G_{right}
\]</span></p>
<p>Unlike information gain, both metrics are better when lower.</p>
<p>The feature selection process would then be:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">for X_i in X<br>      Sort all sample by X_i in a ascending order<br>      for each adjacent samples<br>          m_j = the medium of X_i_j,X_i_(j+1) #use the medium as a threshold<br>          #Z is a label flagged whether X_i of a sample is above the threshold<br>          Z = Z_right if x_i_j &lt; m_j else Z_right<br>          #A binary split based on Z<br>          if a regression tree:<br>          		I_j = combine_MSE(D, Z)<br>          else:<br>          		I_j = combined_Gini(D,Z)<br>      # The split with the lowest MSE or Gini is the best split of feature X_i<br>      I_best_i = min(I_j)<br><br>    <br># Compare all<br>the feature to select = argmax_Xi(I_best_i)<br></code></pre></td></tr></table></figure>
<h4 id="pruning-strategy-1">2.3.2 Pruning Strategy</h4>
<p>The pruning strategy applied in CART is called Cost Complexity Pruning. It is also a kind of post-pruning. The idea of this strategy is similar like Pessimistic Error Pruning in C4.5, but it works different.</p>
<p>Suppose we have a parent node t. Let <span class="math inline">\(T\)</span>be tree below node t(<span class="math inline">\(T\)</span> use t as a root node). Let C be error function that can evaluate the performance of <span class="math inline">\(T\)</span>. For regression, C can be MSE, for classification, C can be misclassification rate. Construct a regularization term <span class="math inline">\(C_\alpha\)</span>: <span class="math display">\[
C_\alpha(T) = C(T) + \alpha|T|
\]</span> where |T| is the number of leaf nodes in T, <span class="math inline">\(\alpha\)</span> is a hyperparameter that adjust the penalty of complexity.</p>
<p>Similarly like the C4.5, we calculate the difference on <span class="math inline">\(C_\alpha\)</span> between use the tree T and replace T with a single leaf node. Define the cost of doing so as: <span class="math display">\[
g(t) = \frac{C_\alpha(T_{leaf}) - C_\alpha(T_{tree})}{|T|-1}
\]</span> <span class="math inline">\(g(t)\)</span> represent the cost on performance if replace the non-leaf node t with a leaf node.</p>
<p>The pruning strategy is then:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">non-leaf nodes sorted = sort the non-leaf nodes in original DT from the bottom to the top<br>for node in non-leaf nodes sorted:<br>		# calculate the cost of each node<br>		cost_nodesa.append(g(node))<br># fine the node with least cost and prune that node<br>node_to_prune = argmin(cost_node_i)<br>DT_temp = replace node_to_prune with a leaf node<br># once a node is pruned, record the updated tree<br>DTs.append(DT.temp)<br><br># validate each tree in the record and select the best<br>for DT_j in DTs:<br>	validate the performance of DT_j<br>select the DT_j with best performance<br><br></code></pre></td></tr></table></figure>
<h4 id="missing-value-processing-1">2.3.3 Missing Value Processing</h4>
<p>Back to the three questions:</p>
<ul>
<li>How to calculate the MSE/Gini of <span class="math inline">\(X_i\)</span> when there are missing values in <span class="math inline">\(X_i\)</span>?</li>
<li>When training, how to decide which sub-node the sample containing missing value belong?</li>
<li>When predicting, how to label the sample when it meet a node that based on its missing values?</li>
</ul>
<p>For the first question, the solution by CART is same as C4.5, which is give the MSE or Gini with a weight: <span class="math display">\[
\rho = \frac{n_{\widetilde{D}}}{n_D}
\]</span></p>
<p>where <span class="math inline">\(D,\tilde{D}\)</span> is the orginal sample set and a subset that samples with missing values of <span class="math inline">\(X_i\)</span> are dropped. n denote the number of samples.</p>
<p>The Gini/MSE would then be: <span class="math display">\[
Gini&#39;\ or \ MSE&#39; = \frac{ 1}{\rho} * (Gini\ or \ MSE)
\]</span></p>
<p>For the scond and third question, the CART use a method call <strong>surrogate splitter</strong>. Suppose we select <span class="math inline">\(X_i\)</span> from X as our best splitting feature. If a sample reach the splitter with missing value on <span class="math inline">\(X_i\)</span>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">IG = Gini or MSE of the Decision node on X_i<br><br>thresh = a threshold measure the difference of IG on two feature<br><br>for X_j in the X except for X_i:<br>		IG_j = calculate Gini or MSE of X_j on current decision node<br>		# record X_j as a surrogate splitter<br>		SS.append(j, IG_j)<br># rank the surrogate splitters according to there information gain<br><br>SS = SS.rank by S_j<br>for j,IG_j in enumerate(SS):<br>		if sample.X_j is missing: drop X_j from SS<br>		if IG - IG_j &gt; thresh: drop X_j from SS<br>		if there are surrogate splitter left in SS:<br>				decide the sample based on X_SS[0] #decide the sample by the best surrogate splitter<br>		else:<br>				let the sample enter the leaf node with most samples in it<br>		<br></code></pre></td></tr></table></figure>
<p>In this method, for a best spliter node <span class="math inline">\(X_i\)</span>, if a sample happen to miss value <span class="math inline">\(X_i\)</span>, we will calculate the MSE or Gini of the rest features based on the samples in this node. We rank these features by Gini or MSE and apply them as a surrogate splitter.</p>
<p>Given a sample, a legal surrogate splitter should guarante:</p>
<ul>
<li>The sample should not miss its value on the feature this splitter based on</li>
<li>The surrogate splitter should not be too different too the original splitter, that is, the difference between there Gini or MSE should be within a threshold</li>
</ul>
<p>In the legal surrogate splitters, select the best(with lowest Gini or MSE) surrogate splitter to decide which child node the sample should go to. If there are no legal surrogate splitters, just put the sample into the node that have the most samples.</p>
<p>This method is very time-consuming, as it demands metrics calculation on all features in every split.</p>
<h4 id="properties-of-cart">2.3.4 Properties of CART</h4>
<ul>
<li>CART can deal with both Regression and Classification</li>
<li>Features in CART would be applied as splitters repeatedly. We can use the frequency they emerge as a feature importance measure</li>
<li>CART is a strict binary tree, and it does not involve log calculations, thus is faster than C4.5</li>
<li>CART has different pruning and missing value processing strategy comparing to C4.5</li>
<li>CART suit big sample, the variance can be high if applied on small sample</li>
</ul>
<h2 id="ensemble-model">3. Ensemble Model</h2>
<p>As single model can have problem with bias or variance. An ideal is to combine multiple basic learner(single model) into a strong learner(ensemble model). The way to combine the basic learners includes:</p>
<p><strong>Bagging</strong></p>
<p>Bootstrap aggregating is a method to deal with high variance of model.It randomly take samples with replacement from the original dataset. Each subset is sent to a weak learner and used to estimate the probability <span class="math inline">\(P(Y|X)\)</span>. All weak learner work parallelly. When a new sample sent to the model for prediction, it would go through all weak learner and get a set of partial prediction. The weak learners would than make a vote to decide the final decision by the strong learner.</p>
<p><strong>Boosting</strong></p>
<p>Boosting is a method to deal with high bias of model. The bias is reflected by the residual between <span class="math inline">\(y_{true}\)</span> and <span class="math inline">\(y_{pred}\)</span>. We can thus model on the residual again to get more accurate model. Boosting is a kind od serial method.</p>
<p>As a typical non-parameter learner, Decision is often applied as a basic learner in ensemble model.</p>
<h3 id="random-forest">3.1 Random Forest</h3>
<h4 id="training-process-1">3.1.1 Training Process</h4>
<p>A random forest is a <strong>bagging method</strong> in which every learner is a decision tree. The basic learner is a CART, but in most applications, the basic learner support Information Gain as a splitting criterea when doing classification.</p>
<p><img src="/2023/02/27/decision-tree/6.png" srcset="/img/loading.gif" lazyload></p>
<p>Specifically, the train process of a RF should be like:</p>
<ol type="1">
<li>Randomly take n sample set from the dataset with replacement, where n is the number of weak learner. Each sample set contain m samples</li>
<li>For each subset, conduct a column sampling, which take a subset k from the original feature space X</li>
<li>Train each weak learner with a sub training set with a size of <span class="math inline">\(m * k\)</span>.</li>
<li>For a test sample, run the test sample through each tree, then average each tree’s prediction.</li>
</ol>
<h4 id="pruning-strategy-2"><strong>3.1.2 Pruning</strong> Strategy</h4>
<p>Since the bagging method already realize a overfitting control, usually we won't apply pruning on an RF. We can still demand pruning, if we do want so, based on the pruning strategy of a single DT though.</p>
<h4 id="missing-value-processing-2"><strong>3.1.3 Missing Value Processing</strong></h4>
<p>Although weak learner already provides missing value processing strategy, it could be very time-consuming when applied to ensemble model with larger scale of estimators. Thus, most ensemble model frameworks do not provide missing value processing function in ensemble modeling.</p>
<p>However, if we do want to equip the RF model with a missing value imputation method, we can do the following stuff:</p>
<ol type="1">
<li>impute all numerical missing value with sample average, all categorical missing value with sample mode</li>
<li>use the imputed sample set to calculate a similarity matrix among all samples, where the similarity between the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> samples is <span class="math inline">\(W_{i, j}\)</span></li>
<li>Back to the original sample set, use the similarity as a weight, fill a missing value <span class="math inline">\(X_{k,j}\)</span>, with the weighted average of all other sample: <span class="math inline">\(X_{k,j} = \frac{1}{n} \sum_{i\ne k}^n W_{i,k}X_{i,j}\)</span>. If <span class="math inline">\(X\)</span> is a categorical variable, use a weighted vote.</li>
</ol>
<p>This process is basically a data filling based on <strong>collaborative filtering</strong></p>
<h4 id="feature-importance"><strong>3.1.4 Feature Importance</strong></h4>
<p>Ensemble models usually have some embedded feature importance evaluation method. For the RF, there commonly are two methods:</p>
<p><strong>Mean Impurity Decrease / Error Decrease/ Information Gain</strong></p>
<p>For a feature <span class="math inline">\(X_i\)</span>, its importance on a non-leaf node m is: <span class="math display">\[
FI(X_i,m) = GI_m -(GI_{left}+GI_{right})
\]</span> Where GI is the gini index of the node, left and right represent the left and right child node of m.</p>
<p>Suppose in a DT, M is the collection of a node that select <span class="math inline">\(X_i\)</span> as a spliter, then the feature importance of <span class="math inline">\(X_i\)</span> on this tree is: <span class="math display">\[
FI(X_i,DT) = \sum_{m \in M} FI(X_i,m)
\]</span> Suppose there are n trees in the forest, the feature importance of <span class="math inline">\(X_i\)</span> would then be: <span class="math display">\[
FI(X_i) = \sum_j^nFI(X_i,DT_j)
\]</span> We can normalize the feature importance so its scale is 1: <span class="math display">\[
\hat{FI(X_i)} = \frac{FI(X_i)}{\sum_i FI(X_i)}
\]</span> Samely, we can replace the criterion with MSE or Imformation Gain and compute according feature importance</p>
<p><strong>OOB Error(Permutation)</strong></p>
<p>IN a RF, for a DT, there would exist some samples that never been selected by the bootstrap process. We call these samples the Out-of-bag data. We can use these data to evaluate the trained DT. Let <span class="math inline">\(E_j\)</span> denote the error rate (1-accuracy) of the <span class="math inline">\(j^th\)</span> DT using OOB data.</p>
<p>Now add a random noise to feaure <span class="math inline">\(X_i\)</span>, and use the OOB data with noise to evaluate the DT again. Denote the error as <span class="math inline">\(E_j&#39;\)</span></p>
<p>If a noise added to <span class="math inline">\(X_i\)</span> would lead to a great descent of accuracy of the DT, then <span class="math inline">\(X_i\)</span> is very important to that DT. Thus we define the feature importance of <span class="math inline">\(X_i\)</span> on the forest with n trees as: <span class="math display">\[
FI(X_i) = \sum_j FI(X_i,DT_j) = \sum_j (E_j - E&#39;_j)
\]</span></p>
<h3 id="gradient-boosting-decision-treegbdt">3.2 Gradient Boosting Decision Tree(GBDT)</h3>
<h4 id="training-process-2">3.2.1 Training Process</h4>
<p>In the GDBT, we define the residual as: <span class="math display">\[
r = \frac{\partial L(y_{ture},M(X)) }{\partial M(X)}
\]</span> where L is a loss function, M(X) is the output of a tree</p>
<p>Let <span class="math inline">\(X_0,Y_0\)</span> be the original inputs and outputs. According to the idea of boosting, the traning process of a GBDT is:</p>
<ol type="1">
<li>Fit a model <span class="math inline">\(M0\)</span> to predict <span class="math inline">\(\hat{Y_0} = M_0(X)\)</span></li>
<li>Calculate the residual <span class="math inline">\(r_0 = \frac{\partial L(\hat{Y_0},Y_0)}{\partial\hat{Y_0}}\)</span></li>
<li>Fit a new model <span class="math inline">\(M1\)</span> using X as inputs and <span class="math inline">\(r_0\)</span> as outputs, get the prediction of residual <span class="math inline">\(\hat{r_0} = M_1(X )\)</span></li>
<li>update the prediction <span class="math inline">\(\hat{Y_1} = \hat{Y_0} + \alpha \hat{r_0}\)</span>, where <span class="math inline">\(\alpha\)</span> is the learning rate</li>
<li>use <span class="math inline">\(\hat{Y_1}, Y_0\)</span> to calculate <span class="math inline">\(r_1\)</span>, this time <span class="math inline">\(r_1 = \frac{\partial L(\hat{Y_1},Y_0)}{\partial\hat{r_0}}\)</span></li>
<li>Repeat process 3-5 until the residual is small enough</li>
</ol>
<p><strong>GDBT for Classification</strong></p>
<p>GDBT requires the residual to be continuous, this means the basic learner of GBDT are all Regression Tree. But GBDT can still do classification. The idea is the same as Generalized Linear Model like Logistic Regression. For the weak learner to predict <span class="math inline">\(\hat{r_i}\)</span>, the weak learner first estimates a canonical parameter <span class="math inline">\(\theta\)</span> for a Nature Exponential Family Distribution. Let <span class="math inline">\(\eta = M(X)\)</span> denote the estimation of <span class="math inline">\(\theta\)</span> given by the DT: <span class="math display">\[
\hat{r_i} = AF(M_{i+1}(X)) = AF(\eta_{i+1})
\]</span> where AF is the activation function. <span class="math display">\[
\hat{Y_{i+1}} = \hat{Y_{i}}+\alpha\hat{r_i} = \hat{Y_{i}}+\alpha AF(\eta_{i+1})
\]</span> Note thar the GDBT always <span class="math display">\[
r_{i+1} = \frac{\partial L(\hat{Y_{i+1}},Y_0)}{\partial \eta_{i+1}}
\]</span> The loss function in classification scenarios are usually <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2022/09/22/loss-function/#log-losscross-entropy-loss">Log Loss</a>.</p>
<p>For details of GLM, refer to <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2023/02/23/linear-regression/#generalized-linear-model">this article</a></p>
<p><strong>Subsampling</strong></p>
<p>In many framework, GDBT support sampling. Suppose we have 100 feature and the subsampling rate is 0.9, the every time a new weak learner is created, 90 sample is drawed randomly from the sample set. This could lead to a situation that for model <span class="math inline">\(M_{i+1}\)</span>, the output variable for some samples are <span class="math inline">\(r_i\)</span>, while other samples's output is <span class="math inline">\(r_{j &lt; i}\)</span>. Such a process would increase the bias but can reduce the variance of the model.</p>
<h4 id="pruning-strategy-3">3.2.2 Pruning Strategy</h4>
<p>Some GDBT support <strong>pre-pruning</strong>, that is, the impurity decrease or information gain must be less or greater than a threshold for the split the be generated. Samely, we can applied the post-pruning strategy the weak learner has originally.</p>
<p>However, in real application of boosting, we usually control overfitting by</p>
<ul>
<li>constrain the max depth of a single tree</li>
<li>set a shrinkage parameter to <span class="math inline">\(\alpha\)</span> to control the step size of learning</li>
<li>Apply subsampling</li>
</ul>
<h4 id="missing-value-processing-3">3.2.3 Missing Value Processing</h4>
<p>In GDBT, for time-saving reasons, the strategy is to:</p>
<ol type="1">
<li>When training, calculate impurity or information gain without missing value sample</li>
<li>Try to put <strong>all the missing value samples</strong> respectively into the left and right split and calculate which would bring less loss(based on the loss function selected). Put the sample into that split and record the direction(left or right)</li>
<li>When predicting, suppose a sample is missing value <span class="math inline">\(X_i\)</span> and meet a node split on that feature. If there exists value missing on that node, then find the direction that missing values samples goes during the training process. If there are no missing value samples in the training, then the new sample goes to a default direction decided by the instance.</li>
</ol>
<h4 id="feature-importance-1">3.2.4 Feature Importance</h4>
<p>The GDBT can use same feature importance evaluation method like the RF. Since GDBT's basic estimator is Regression Tree, we usually just calculate the decrease of <strong>MSE</strong>. However, when doing classification, we can also consider directly use the real label Y to calculate the <strong>impurtiy</strong> of a node.</p>
<h3 id="xgboosting">3.3 XGBoosting</h3>
<h4 id="training-process-3">3.3.1 Training Process</h4>
<p><strong>Objective Function</strong></p>
<p>The XGboosting is a improved version of GDBT. The biggest difference is the way it define the residual. XGB use the second derivative Taylor Series to define the residual, that: <span class="math display">\[
\begin{aligned}
r_{i+ 1} &amp;= [L(y_0,\hat{y}_{i})+g_{i}\hat{r}_i+\frac{1}{2}h_{i}\hat{r}_i^2] + \Omega(T)\\
g_{i} &amp;= -\frac{\partial L(y_0,\hat{y}_{i})}{\partial M_{i+1}(X)} \\
h_{i} &amp;= -\frac{\partial^2 L(y_0,\hat{y}_{i})}{\partial M_{i+1}(X)^2 } \\
\end{aligned}
\]</span> where in regression <span class="math inline">\(\hat{r_i} = M_{i+1}(X)\)</span> and in classification <span class="math inline">\(\eta_ {i+ 1} = M_ {i+1}(X)\)</span>, and <span class="math inline">\(\Omega(T)\)</span> is a regularization term: <span class="math display">\[
\Omega(T) = \gamma J + \frac{1}{2}\lambda \sum_{j=1}^J b_j^2
\]</span> where J is number of leaf nodes under node T, <span class="math inline">\(b_j\)</span> is values on those leaf nodes, <span class="math inline">\(\gamma, \lambda\)</span> are hyper parameters control the degree of penalty.</p>
<p><strong>Splitting Strategy</strong></p>
<p>Unlike traditional CART, the XGB use a generalized gain deduced from the objective function.</p>
<p>Let <span class="math inline">\(R_ j\)</span> represent the collection of samples belong to the current node t. In equation 32, we know that the prediction of residual on a node t <span class="math inline">\(\hat{ r}_t\)</span> is actually decided by the samples, which is the average of samples beneath the t's subtree, which is the leaf values <span class="math inline">\(b_j\)</span>. If we express <span class="math inline">\(r_i\)</span> in <span class="math inline">\(\sum_j^ Jbj\)</span>: <span class="math display">\[
r_t = [L(y_0,\hat{y}_{i})+G_{j}b_j+\frac{1}{2}(H_{j}+\lambda )b_j^2] + \gamma J
\]</span> where <span class="math inline">\(G_j = \sum_{k \in R_j}g_k\)</span>, representing the sum of first derivatives of samples belong to this node. Same with <span class="math inline">\(H_j\)</span>.</p>
<p>When the samples belongs to the node <span class="math inline">\(R_ j\)</span> is given, in other word, a split has been made, the only unknown variable in this function is <span class="math inline">\(b_j\)</span>. Now we calculate the optimal value of <span class="math inline">\(b_j\)</span> using <span class="math inline">\(\frac{d r_{i+ 1}}{db_j} = 0\)</span>, and we obtain: <span class="math display">\[
b^* = -\frac{G_j}{H_j+\lambda}
\]</span> and bring the optimal back to objective function to get the best error function given a split: <span class="math display">\[
obj_{R_j} =-\frac{1}{2}[\frac{ G_ j^ 2}{H_ j + \lambda}] + \gamma J]
\]</span> In a splitting process, XGB need to compare the error of using a node as a leaf node or splitting it into 2 child nodes. In such scenario, <span class="math inline">\(J, J_L, J_R\)</span> all equal to 1. Thus we define the Gain of a split as: <span class="math display">\[
\begin{aligned}
Gain &amp; = Obj_{L+R} - [Obj_L + Obj_R ] \\
&amp; = [-\frac{1}{2} \cdot \frac{ (G_L + G_R)^ 2}{H_L+H_R + \lambda} + \gamma] - [-\frac{1}{2} \cdot (\frac{G_L^ 2}{H_L + \lambda} + \frac{G_R^ 2}{H_R + \lambda})+2\gamma]  \\
&amp; = \frac{1}{2}[\frac{G_L^ 2}{H_L + \lambda} + \frac{G_R^ 2}{H_R + \lambda} - \frac{ (G_L + G_R)^ 2}{H_L+H_R + \lambda}] + \gamma
\end{aligned}
\]</span></p>
<p>Gain is naturally an error decrease measure, it is quiet similar to information gain <span class="math inline">\(IG = H( D) - [H( D| left) + H(D|right) ]\)</span> . The only difference is it replace the entropy with a best error transformed from the objective function.</p>
<p>According to the definition, higher gain bring more error decrease. Thus, we can use gain as a criterion to decide the best split. The specific splitting strategy is just like the MSE in Regression Tree, the only difference is the the criterion.</p>
<p><img src="/2023/02/27/decision-tree/7.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Other Improvement</strong></p>
<ol type="1">
<li>XGB can do both row and column subsampling, which means it can drop some feature just like RF</li>
<li>XGB support linear regression model as basic estimator. (Dart is also supported)</li>
<li>XGB support customized loss function, as long as it is second derivative</li>
<li>XGB sort the samples respectively by each feature and save the ranking as blocks. This makes in best-split-find process can be done parallelly across different feature.</li>
</ol>
<h4 id="pruning-strategy-4">3.3.2 Pruning Strategy</h4>
<p>XGB support both pre-pruning and post-pruning. For pre-pruning, XGB can set a threshold for gain when splitting. For post-pruning, XGB can set a threshold, that if a subtree contains no child node that has a greater gain than this threshold, it will be pruned.</p>
<p>Like GDBT, more common overfitting control method in XGB is:</p>
<ul>
<li>Raw and column subsampling rate</li>
<li>shrinkage parameter $$</li>
<li>Structural hyper parameters like max_depth, min_leaf_sample_size</li>
</ul>
<h4 id="missing-value-processing-4">3.3.3 Missing Value Processing</h4>
<p>As mentioned above, XGBoosting support linear estimator. When the booster is gblinear, XGB can not deal with missing value. It would fill all missing value with 0.</p>
<p>When the booster is gbtree or dart, the XGB use same missing value processing strategy like GDBT.</p>
<h4 id="feature-importance-2">3.3.4 Feature Importance</h4>
<p><strong>Gain</strong></p>
<p>The Gain method is similar to the mean impurity decrease method in Random Forest. The different is, instead of impurity, the gain method use the gain to calculate the decrease.</p>
<p>Let T denote a collections of spliter using <span class="math inline">\(X_ i\)</span> in the <span class="math inline">\(j+1^{th}\)</span> tree model: <span class="math display">\[
Gain(X_i, DT_{j+1}) = \sum_{t \in T} Gain(t)
\]</span> suppose there are n DTs: <span class="math display">\[
Gain(X_i) = \frac{ 1}{n} \sum _j^n Gain(X_i,DT_j)
\]</span> We can than use this gain as a feature importance. We can also normalize the feature importance so its scale is 1: <span class="math display">\[
\hat{Gain(X_i)} = \frac{Gain(X_i)}{\sum_i Gain(X_i)}
\]</span></p>
<p><strong>Weight</strong></p>
<p>The frequency of <span class="math inline">\(X_i\)</span> being used as a spliter in the whole ensemble model. Note that when we make column subsampling, we should consider carefully whether to use weight as a feature importance, as sample important feature may just happened to be sampled only a few times.</p>
<p><strong>Cover</strong></p>
<p>For a feature <span class="math inline">\(X_i\)</span>, let <span class="math inline">\(R_t\)</span> denote the number of sample under a node t that split using <span class="math inline">\(X_i\)</span>. Let T denote a collection of all nodes in the ensemble models that uses <span class="math inline">\(X_i\)</span> as spliter, <span class="math inline">\(n_T\)</span> denote the length of T(how many times the feature is used). <span class="math display">\[
cover = \frac{\sum_{t \in T}R_t}{n_T}
\]</span> <strong>Difference of these 3 methods:</strong></p>
<ul>
<li>weight gives higher FI to numerical features.</li>
<li>Gain give higer FI to unique features. If we want to applied, drop features that is unique or close to unique like personal ID</li>
<li>cover gives higher FI to categorical features.</li>
</ul>
<h3 id="other-ensemble-models">3.4 Other Ensemble Models</h3>
<p><a href>ongoing</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Machine-Learning/" class="category-chain-item">Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Decisions-Tree/">#Decisions Tree</a>
      
        <a href="/tags/Ensemble-Model/">#Ensemble Model</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Decision Tree and Ensemble Models</div>
      <div>http://example.com/2023/02/27/decision-tree/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Zhengyuan Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>February 27, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/03/03/EM/" title="Expectation Maximization Algorithm(EM)">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Expectation Maximization Algorithm(EM)</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/02/23/linear-regression/" title="Linear Regression and Generalized Linear Model">
                        <span class="hidden-mobile">Linear Regression and Generalized Linear Model</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <i class="iconfont icon-bug"></i> <a href="http://zhengyuanyang.com/report/" target="_blank" rel="nofollow noopener"><span>Report Bug</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
