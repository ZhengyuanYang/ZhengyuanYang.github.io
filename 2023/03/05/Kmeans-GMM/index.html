

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhengyuan Yang">
  <meta name="keywords" content="">
  
    <meta name="description" content="This article discuss some distance-based clustering methods.">
<meta property="og:type" content="article">
<meta property="og:title" content="K-Means and GMM">
<meta property="og:url" content="http://example.com/2023/03/05/Kmeans-GMM/index.html">
<meta property="og:site_name" content="Data Shore">
<meta property="og:description" content="This article discuss some distance-based clustering methods.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/03/05/Kmeans-GMM/2.png">
<meta property="og:image" content="http://example.com/2023/03/05/Kmeans-GMM/1.png">
<meta property="og:image" content="http://example.com/2023/03/05/Kmeans-GMM/3.png">
<meta property="og:image" content="http://example.com/2023/03/05/Kmeans-GMM/4.png">
<meta property="og:image" content="http://example.com/2023/03/05/Kmeans-GMM/5.png">
<meta property="og:image" content="http://example.com/2023/03/05/Kmeans-GMM/6.png">
<meta property="article:published_time" content="2023-03-05T19:17:44.000Z">
<meta property="article:modified_time" content="2023-04-30T11:05:36.261Z">
<meta property="article:author" content="Zhengyuan Yang">
<meta property="article:tag" content="K-Means">
<meta property="article:tag" content="GMM">
<meta property="article:tag" content="Clustering">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2023/03/05/Kmeans-GMM/2.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>K-Means and GMM - Data Shore</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Data Shore</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner.gif') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="K-Means and GMM"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-03-05 11:17" pubdate>
          March 5, 2023 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          15k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          127 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">K-Means and GMM</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="k-means-and-gmm">K-Means and GMM</h1>
<h2 id="k-means">1. K-Means</h2>
<h3 id="about-k-means">1.1 About K-Means</h3>
<p>K-Means is a typical unsupervised learning algorithm for <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2022/09/11/basic_knowledgefor_ML/#clustering">clustering</a>. It is a distance-based clustering algorithm.K-Means is a very interpretable algorithm.</p>
<p>We can understand K-Means from a EM algorithm perspective. The basic idea of K-means is to set a set of centers, and use the distance between a data point and all centers to decide which cluster the data point belongs.</p>
<p>Let a series of variable <span class="math inline">\(\mu = (\mu_1,\mu_2,...\mu_K)\)</span> denote the locations of the centers.<span class="math inline">\(\mu\)</span> represent a split plan of the clustering algorithm. For each data point <span class="math inline">\(x_n\)</span>, let <span class="math inline">\(r_n = (r_{n,1},r_{n,2},...r_{n,K})\)</span> denote a group of variable indicating which cluster the data point belong to, where <span class="math inline">\(r_{n,k}\)</span> is a binary variable(0 or 1). The objective of the K-means is to minimize the sum of distance between data points and center with a center: <span class="math display">\[
J  = \sum_n \sum_k r_{n,k} ||x_n-\mu_k||^2
\]</span> This function is the objective function of a K-means algorithm, it is also called the <strong>distortion</strong>.</p>
<p>Using a <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2023/03/03/EM/">EM algorithm</a>:</p>
<ul>
<li>Initialization: Randomly Pick some data point</li>
<li>Expectation: Given <span class="math inline">\(\mu\)</span>, optimize J about $r_n $ (find a best split)</li>
<li>Maximization: Given <span class="math inline">\(r_n\)</span>, optimize J about <span class="math inline">\(\mu\)</span> (update the center)</li>
</ul>
<p><strong>Training Process</strong></p>
<p>Base on this idea, the training process of the K-Means is:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode"># Initialize Centers<br>centers = randomly select k data points as centers<br><br># the terminate condition include:<br>#    E converge: The points in the clusters does not change(clusters_i+1 = clusters_i)<br>#    M converge: The location of centers does not change (mu_i+1 -mu &lt; epsilon) <br>while terminate condition is not reached<br>  # find a best split<br>  for each x in X:<br>      for each center in centers:<br>          dis_k = distance(x,center) #usually euclidean distance<br>          dis.append(dis_k)<br>      # the closet center is the cluster x belong<br>      x.cluster = minidx(dis)<br>  # update centers using the data points in to it<br>  for each cluster:<br>      cluster.center = cluster.x.mean()<br>		<br></code></pre></td></tr></table></figure>
<p><strong>Assumption &amp; Properties</strong>:</p>
<ol type="1">
<li>The k must be a given number. This could be difficult in real application(<strong>K selection</strong>)</li>
<li>K-Means is a distance-based method, it require scaling of feature(<strong>Scaling</strong>)</li>
<li>K-Means is sensitive to the initial centers selection. Bad choice on initial centroids could trap the algorithm in local optimal. So K-Means could be unstable when the initial centroid change.(<strong>Initial centroid selection</strong>)</li>
<li>K- Means assume each cluster is a convex space(a sphere). Consider kernel functions when this assumption is not satisfied. (<strong>Kernel function</strong>)</li>
<li>K-Means assume a cluster is uniformly. This means K_Means can be unstable when there are outlier in the dataset or when some variable are not normally distributed. Consider outlier detection and normality transformation before using K-Means. (<strong>Normality</strong>)</li>
<li>K-Means assume the feature are not correlated. (<strong>Correlation Analysis</strong>)</li>
<li>K-Means require encoding on categorical variable, this could give binary variable very high variance, and overestimate the effect of them. (<strong>Categorical Variable Processing</strong>)</li>
<li>The traditional K-Means use euclidean distance, this assume different features(directions) have same importance(weights) to similarity. In real application, this assumption could not always been fulfilled. <strong>(Feature Weighting)</strong></li>
<li>When a cluster actually exists in the space but fails to be recognized by the algorithm, we call it a hidden cluster. This can usually be caused by insufficient samples of that cluster(hidden clusters). K_Means cannot deal with imbalanced data hidden cluster.<strong>(Hierarchical Clustering)</strong></li>
</ol>
<p>Although K-Means has so many drawbacks, it still has wide application in industries. The core reason is that it is simple and interpretable, and there is only one hyper parameter to adjust. The unstable results is decided by the inner mechanism of K-Means. If we really cannot find a stable clustering results through K-Means, we should consider other algorithm.</p>
<p><img src="/2023/03/05/Kmeans-GMM/2.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="improvements-on-k-means">2. Improvements on K-means</h2>
<h3 id="k-selection">2.1 K-Selection</h3>
<p><strong>Score-K Plot(Elbow Method)</strong></p>
<p>The Elbow method depict the relationship of the number of K and the performance of clustering. Let S denotes a metrics that can evaluate the performance of the clustering model. The elbow graph shows the S'value(on the y-axis) corresponding to the different values of K(on the x-axis). When we see an <strong>elbow shape in the graph, we pick the K-value where the elbow gets created</strong>. We can call this point the <strong>Elbow point</strong>. Beyond the Elbow point, increasing the value of ‘K’ does not lead to a significant reduction (or increase) in S.</p>
<p><img src="/2023/03/05/Kmeans-GMM/1.png" srcset="/img/loading.gif" lazyload></p>
<p>As shown in the picture, when K increase, the value of evaluating metric goes better while the cost of time being greater. Select the K with closest metrics value and time consuming as the elbow. The evaluating metric for the algorithm includes distortion, Silhouette Coefficient and Calinski Index. For details, refer to <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2023/03/06/evaluation-cluster/">this article</a>. Note that choosing K on a single metric can be verty biased, since some metrics prefer smaller K while other prefer greater K. Select the optimal K with multiple metrics taken into account.</p>
<p><strong>ISODATA</strong></p>
<p>The ISODATA is a improved kmenas model with embedded K selection method. Comparing to traditional Kmeans, it requires 4 parameter:</p>
<ul>
<li><span class="math inline">\(K_0\)</span>: An expected number of K. The final K selection would be in <span class="math inline">\([{K_ 0 \over 2}, 2K_ 0]\)</span></li>
<li><span class="math inline">\(N_{min}\)</span>: the minimum number of data points in a cluster</li>
<li><span class="math inline">\(\sigma_{max}\)</span>: The max allowed <span class="math inline">\(\sigma\)</span>, where <span class="math inline">\(\sigma\)</span> is a metric to measure the within distance in a cluster</li>
<li><span class="math inline">\(d_{min}\)</span>: The min allowed d, where d is a metric to measure the distance between two clusters</li>
</ul>
<p>The training Process of the ISODATA is given as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">Initialize Centers<br><br>while terminate condition is not reached<br>  find a best split based on distance<br>  <br>  <br>  for each cluster in the K_0 clusters:<br>  		if the number of points in the cluster &lt; N_min:<br>  				drop the cluster and label or data points in it with the nearest cluster<br>  				K = K-1<br>  				<br>	update the centroids of all clusters<br><br>  if K &lt; K_0 / 2:<br>  # spliter some clusters into 2 clusters<br>  		For each cluster:<br>  				# calulate the sigma on each dimension(feature), and select the maximum<br>  				s_max = max(sigma_x1,sigma_x2,...)<br>  				if s_max &gt; sigma_max and n &gt; 2*N_min:<br>  						execute a kmeans on this cluster with K=2 to split it into two cluster<br>  				else:<br>  						forward<br>  if K &gt; 2K_0:<br>  # Calculate the distance between each pair of clusters:<br>  		D = distance_matrix(Clusters):<br>  		while any D(i,j) &lt; d_min:<br>  				merge cluster i and cluster j into one cluster<br>  				#update the centroid of the new cluster<br>  				c_temp = 1/(ni+nj)(ni*ci+nj*cj)<br>  				update D<br>  		<br>  # update centers using the data points in to it<br>  for each cluster:<br>      cluster.center = cluster.x.mean()<br><br></code></pre></td></tr></table></figure>
<p>Such a algorithm can adjust the number of K in the dynamic training process.</p>
<h3 id="k-means-1">2.2 K-Means++</h3>
<p>The K-Means++ is an improved version of K-Means aimed at optimizing the initial centroids selection. Basically, it believe the more separated the initial centroids are, the more stable the clustering results will be. For the initial centroids choosing:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode"># randomly pick a data point<br>c1 = random(dataset)<br>centers.append(c1)<br><br>while len(centers) &lt; k:<br>		for each sample s:<br>        # calculate the minimum of the distance between <br>        D_s = min(dis(s, c1), dis(s,c2),....)<br>        D_s = D_s ** 2<br>        D.append(D_s)<br>     # Use the ratio of D[s] and sum of D as a probability of s being selected as a centroids<br>     Prob = D[s] / sum(D[s]) for s in all sample<br>		# decide the next centroid using a Roulette method<br>		r = rand(0,1)<br>		if  Prob[s-1] &lt; r &lt; Prob[s]:<br>			 # select s as a centroid<br>			 centers.append(s)<br><br></code></pre></td></tr></table></figure>
<h3 id="kernel-k-means">2.3 Kernel K-Means</h3>
<p>Kernel K-means is an unsupervised clustering algorithm that extends the traditional K-means algorithm by incorporating the kernel trick. The kernel trick allows for clustering of non-linearly separable data by implicitly mapping the data points to a higher-dimensional feature space. By using kernel functions, we can compute similarity between data points in the higher-dimensional space without explicitly calculating their high-dimensional representations.</p>
<p>The specific difference is, when calculating the distance between two data points, we know use a kernel distance(kernel trick) to evaluate the similarity of 2 data point in a high dimension.</p>
<p>Choose a kernel function: Select an appropriate kernel function, such as the Gaussian Radial Basis Function (RBF), polynomial kernel, or Sigmoid kernel. The kernel function is used to compute similarities between data points in the higher-dimensional space.</p>
<p>For details of kernel function, refer to <a href>ongoing</a></p>
<h3 id="k-medoids">2.4 K-Medoids</h3>
<p>We can conduct outlier detection to elimate their influences. However, some points that are on the boundary of clusters would "pull away" the centroid we found. Consider a cluster <span class="math inline">\((1,2),(2,1),(1,1),(5,10)\)</span>, the last point would distinctly move the centroid in updating.</p>
<p>To fix such problems, we can introduce K-Medoids, which apply different centroid updating strategy. In K-Medoids, the new centroid is not the mean of all data points in the cluster, Instead, for each point <span class="math inline">\(x\)</span> in the cluster, we need to compute the Manhattan Distance between <span class="math inline">\(x\)</span> and every other data point. The sum of these distances is the error of point <span class="math inline">\(x\)</span> : <span class="math display">\[
\begin{aligned}
Error(x_i) &amp;= \sum_{j \ne i } ManhattanD(x_i,x_j)\\
centroid &amp;= argmin_{x_i} \ Error(x_i) 
\end{aligned}
\]</span></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode"># Initialize Centers<br>centers = randomly select k data points as centers<br><br>while terminate condition is not reached<br>  # find a best split<br>  for each x in X:<br>      for each center in centers:<br>          dis_k = distance(x,center) #usually euclidean distance<br>          dis.append(dis_k)<br>      # the closet center is the cluster x belong<br>      x.cluster = minidx(dis)<br>  # update centers using the data points in to it<br>  for each cluster_k:<br>      for each x_i in cluster:<br>      		#calculate the sum of mahatten distance between point x_i and the rest point<br>      		for each x_j in rest points in the cluster:<br>      				error_ij = Mah_Dist(x_i,x_j)<br>      		error_i = sum(error_ij)<br>      		Error.append(error_i)<br>      # update the cent <br>      center_k = argmin_x(Error)<br></code></pre></td></tr></table></figure>
<p>Although K-Medoids can undermine the effect of points on boundary, the time complexity of updating centroids is <span class="math inline">\(O(n^2)\)</span>, which makes the whole algorithm time-consuming.</p>
<h3 id="k-means-on-categorical-variable">2.5 K-Means on Categorical Variable</h3>
<p>K-means algorithm has issues when dealing with categorical variables. This is mainly because K-means relies on Euclidean distance to compute similarity between data points, which is meaningful for continuous variables but not applicable to categorical variables. Categorical variables are typically non-numeric data that cannot be directly used to calculate Euclidean distances. Even if you transform categorical variables into numeric values through some feature engineering method(such as one-hot encoding), it still doesn't capture their actual relationships.</p>
<p>To address this issue, we can adopt the following approaches to improve the K-means algorithm for handling categorical variables:</p>
<p><strong>Alternative distance metrics:</strong></p>
<p>For categorical data, you can use other distance metrics such as Hamming distance or Jaccard similarity. These metrics can better capture the relationships between categorical variables. When implementing K-means, you can replace Euclidean distance with one of these metrics. For details of Distance, refer to <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2022/10/08/distance/#jaccard-distance">this article</a></p>
<p>An implementation of this method is the <strong>K-Modes</strong> algorithm. K-Modes is a clustering algorithm specifically designed for categorical variables. It works by replacing the Euclidean distance with the <strong>hamming distance</strong> and updating the cluster centroids using the mode instead of the mean. The matching distance is the number of mismatching attributes between two data points, for example a data point X = [1,2,5,3,0], and the centroid C = [1,2,4,2,3], then <span class="math inline">\(d_{match} = 3\)</span></p>
<p><strong>K-Prototypes algorithm:</strong></p>
<p>K-Prototypes is a hybrid of K-means and K-Modes algorithms. It is suitable for datasets containing both continuous and categorical variables. The K-Prototypes algorithm calculates a mixed distance based on the attribute type, combining the Euclidean distance for continuous variables with the matching distance for categorical variables. Similarly, updating the cluster centroids also requires considering both continuous and categorical variables separately.</p>
<h3 id="weighted-k-means">2.5 Weighted K-Means</h3>
<p>In the original K-Means algorithm, we eliminated the dimensional differences of each feature through scaling, and at this point, the importance of each feature for similarity calculation is equal. However, in real applications, this may not be true. If a feature contains very little information and a lot of noise, but has the same importance as other dimensions in distance calculation, then this feature will reduce the accuracy of clustering. Therefore, an improved algorithm, Weighted K-Means, was proposed to solve this problem. In WKMeans, we assign a weight to each feature, and the optimization objective function of clustering becomes: <span class="math display">\[
J(C) = J  = \sum_n \sum_k r_{n,k}\sum_i w_i^\beta(x_{n,i}-\mu_{k,i})^2
\]</span> where <span class="math inline">\(\omega_i\)</span> is the weight of the <span class="math inline">\(i^{th}\)</span> dimension, <span class="math inline">\(\sum_i\omega_i = 1\)</span>. <span class="math inline">\(\beta\)</span> is a parameter controls the extent of weighting, usually among <span class="math inline">\([0,1]\)</span>. When <span class="math inline">\(\beta = 0\)</span>, the WKMeans degenerated to the original K-Means algorithm.</p>
<p>In training procedures of the WK-Means, we need to update the weight vector after we update the centroids of all clusters in each iteration. With the Lagrange multiplier method, we can obtain the updating equation of <span class="math inline">\(\omega_i\)</span> : <span class="math display">\[
\omega_i = \frac{1}{\sum_j\frac{D_i}{D_j}^{\frac{1}{\beta-1}}}
\]</span> where <span class="math inline">\(D_j\)</span> is the sum of unweighted distance of all samples to their corresponding centroid on the <span class="math inline">\(j^{th}\)</span> dimension, <span class="math inline">\(D_j = \sum_n\sum_kr_{n,k}(x_{n,j}-\mu_{k,j})^2\)</span>.</p>
<h3 id="hierarchical-clustering">2.6 Hierarchical Clustering</h3>
<p>Hierarchical clustering is a type of clustering algorithm that aims to build a hierarchy of clusters. It can be classified in Agglomerative Clustering and Divisive Clustering.</p>
<p>The Agglomerative Clustering starts by considering all data points as individual clusters and then repeatedly merges the two closest clusters until there is only one cluster left, while in divisive clustering, the algorithm starts with all the data points in a single cluster and recursively splits them into smaller and smaller clusters, until the minimum number of samples in a cluster is reached.</p>
<p>The result is a tree-like structure, called a dendrogram, that shows the hierarchical relationships among the data points.</p>
<figure>
<img src="/2023/03/05/Kmeans-GMM/3.png" srcset="/img/loading.gif" lazyload alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>The Divisive Clustering is easy to implement, we can simply repeat K-Means recursively on each hierarchy. The Agglomerative Clustering involves computing the similarly of two clusters when merging them, there are three metrics:</p>
<ul>
<li>Single Linkage: the distance between the two closest data points in the two clusters</li>
</ul>
<figure>
<img src="/2023/03/05/Kmeans-GMM/4.png" srcset="/img/loading.gif" lazyload alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li>Complete Linkage: the distance between the two farthest data points in the two clusters</li>
</ul>
<figure>
<img src="/2023/03/05/Kmeans-GMM/5.png" srcset="/img/loading.gif" lazyload alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<ul>
<li>Compute the distance between each pair of data points in the two clusters。Calculate the mean of all distances as the distance between two clusters.</li>
</ul>
<figure>
<img src="/2023/03/05/Kmeans-GMM/6.png" srcset="/img/loading.gif" lazyload alt="img"><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="gaussian-mixture-model">3. Gaussian Mixture Model</h2>
<h3 id="mixture-model">3.1 Mixture Model</h3>
<p>A mixture model is a statistical model that represents the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs.</p>
<p><img src></p>
<p>If we believe the distribution of each sub-population is a Gaussian distribution, then we call the whole model a gaussian mixture model: <span class="math display">\[
P(x) = \sum_i^k \alpha_kN(x|\mu_k,\sigma_k)
\]</span> where <span class="math inline">\(\sum_ i^ k \alpha_ k = 1\)</span>, representing a weight that allows <span class="math inline">\(\int P(X = x) = 1\)</span>. In a clustering problem, we can use the probability that a random data point will belong to cluster k as the weight <span class="math inline">\(\alpha_k = P(z=C_k)\)</span>, in other word, the distribution of the hidden variable z, denoted as <span class="math inline">\(P(z) = (\pi_1,\pi_2,...\pi_k)\)</span>. Since we usually have multiple feature in clustering, the gaussian distribution is usually a <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2022/11/04/distribution/#multivariate-normal-distribution">mutilvariate normal distribution</a>, replace <span class="math inline">\(\mu_k,\sigma_k\)</span> with matrix of mean and covariance <span class="math inline">\(M, \Sigma\)</span>: <span class="math display">\[
P(x) = \sum_i^{k}P(x|z=C_i)P(z=C_i) = \sum_i^{k}P(x|M_k,\Sigma ^2_k)\pi_k
\]</span> Thus, we find that the GMM model transform the clustering to a parameter estimating problem on <span class="math inline">\((M_1,...M_k),(\Sigma_1,...\Sigma_k),(\pi_1,...\pi_k)\)</span>.</p>
<h3 id="em-algorithm-to-solve-gmm">3.2 EM Algorithm to Solve GMM</h3>
<p>We can applied the <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2023/03/03/EM/">EM algorithm</a> to optimize the estimation of the three groups of parameters.</p>
<p>We know from the EM algorithm that for each point <span class="math inline">\(x_i\)</span>, the lower boundary of the likelihood function is: <span class="math display">\[
Q_i(z_j) = P(z_j|x_i,\theta_j)=\frac{P(z_j)P(x_i|z_j,\theta_j)}{\sum_jP(z_j)P(x_i|z_j,\theta_j)}= \frac{\pi_jN(x_i|M_j,\Sigma_j)}{\sum_j^k\pi_jN(x_i|M_j,\Sigma_j)}
\]</span> The Likelihood function would be: <span class="math display">\[
\begin{aligned}
L(M,\Sigma,\pi) &amp;= \sum_i^n\sum_j Q_i(z_j)log(P(x_i,Z_i = z_j|\theta_{j}))\\
&amp; =\sum_i\sum_j \frac{\pi_jN(x_i|M_j,\Sigma_j)}{\sum_j^k\pi_jN(x_i|M_j,\Sigma_j)}log[\pi_jN(x_i|M_j,\Sigma_j)]
\end{aligned}
\]</span> Through deriving, we can find the update equation for the parameters. Let <span class="math inline">\(\mu^{(t)}_{j}\)</span> denote the estimation for mean of the <span class="math inline">\(j^{th}\)</span> cluster in the <span class="math inline">\(t^{th}\)</span> iteration: <span class="math display">\[
\begin{aligned}
M_{j}^{(t+1)}  &amp;=  \frac{\sum_i^nQ_i(z_j) x_i}{\sum_iQ_i(z_j)}\\
\Sigma_j^{(t+1)} &amp;= \frac{\sum_i^n (x_i - M_j^{(t+1)})^T\cdot (x_i - M_j^{(t+1)})Q_i(z_j)}{\sum_i^nQ_i(z_j)}\\
\pi^{(t+1)}_j &amp;= \frac{\sum_i^nQ_i(z_j)}{n}
\end{aligned}
\]</span> Thus the traning algorithm for GMM is:</p>
<ol type="1">
<li>Randomly initial <span class="math inline">\(M,\Sigma,\pi\)</span></li>
<li>For each sample, compute posterior probability <span class="math inline">\(Q_i(z_j)\)</span></li>
<li>Update <span class="math inline">\(M,\Sigma,\pi\)</span> through the equations above</li>
<li>Repeat step 2-3, until termination condition is reached</li>
</ol>
<h3 id="about-gmm">3.3 About GMM</h3>
<ol type="1">
<li>GMM is a probability model, the prediction yields the probabilities a data points belongs to each cluster</li>
<li>To predict a data point, fix <span class="math inline">\(M, \Sigma\)</span>, compute the probability it belongs to each cluster</li>
<li>Unlike K-Means, GMM is soft-clustering, K-Means can be saw as a hard clustering variant of GMM.</li>
<li>GMM directly model on <span class="math inline">\(P(X, Z)\)</span>, thus it is a generative model.</li>
</ol>
<p>​</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Machine-Learning/" class="category-chain-item">Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/K-Means/">#K-Means</a>
      
        <a href="/tags/GMM/">#GMM</a>
      
        <a href="/tags/Clustering/">#Clustering</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>K-Means and GMM</div>
      <div>http://example.com/2023/03/05/Kmeans-GMM/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Zhengyuan Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>March 5, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/03/06/evaluation-cluster/" title="Evaluation for Clustering Model">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Evaluation for Clustering Model</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/03/03/EM/" title="Expectation Maximization Algorithm(EM)">
                        <span class="hidden-mobile">Expectation Maximization Algorithm(EM)</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <i class="iconfont icon-bug"></i> <a href="http://zhengyuanyang.com/report/" target="_blank" rel="nofollow noopener"><span>Report Bug</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
