

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zhengyuan Yang">
  <meta name="keywords" content="">
  
    <meta name="description" content="This article introduce basic and improved sampling methods frequently used in machine learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Sampling Methods for Machine Learning">
<meta property="og:url" content="http://example.com/2022/12/05/sampling/index.html">
<meta property="og:site_name" content="Data Shore">
<meta property="og:description" content="This article introduce basic and improved sampling methods frequently used in machine learning">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/12/05/sampling/1.png">
<meta property="og:image" content="http://example.com/2022/12/05/sampling/2.png">
<meta property="og:image" content="http://example.com/2022/12/05/sampling/3.png">
<meta property="og:image" content="http://example.com/2022/12/05/sampling/4.png">
<meta property="og:image" content="http://example.com/2022/12/05/sampling/5.png">
<meta property="og:image" content="http://example.com/2022/12/05/sampling/6.png">
<meta property="article:published_time" content="2022-12-05T04:53:54.000Z">
<meta property="article:modified_time" content="2023-10-02T18:13:45.514Z">
<meta property="article:author" content="Zhengyuan Yang">
<meta property="article:tag" content="Sampling">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2022/12/05/sampling/1.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Sampling Methods for Machine Learning - Data Shore</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Data Shore</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/Toolbox/">
                <i class="iconfont icon-briefcase"></i>
                Toolbox
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner.gif') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Sampling Methods for Machine Learning"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-12-05 12:53" pubdate>
          December 5, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          14k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          119 mins
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Sampling Methods for Machine Learning</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="sampling-and-simulation-for-machine-learning">Sampling and Simulation for Machine Learning</h1>
<h2 id="about-sampling-and-simulation"><strong>1. About Sampling and Simulation</strong></h2>
<p>The term "Sampling" can be used in different contexts in machine learning (ML). From a statistical perspective, sampling refers to the process of collecting a subset of units from a real sample population. The objective of sampling is to describe the characteristics of the sample through statistical analysis.</p>
<p>From a computational perspective, however, the term "Sampling" means "generating scenarios to represent a probability distribution" or in other words, "Simulation". In a computer context, "Sampling" actually refers to "Simulation". To clarify the distinction, in this article, sampling in a statistical context will be referred to as "Sampling", while sampling in a computational context will be referred to as "Simulation".</p>
<p><strong>For Sampling</strong></p>
<p>There are various sampling method that have each different application. Sampling methods can be categorized as:</p>
<ol type="1">
<li><strong>Probability Sampling</strong>: Every Single unit in the population has a chance to be selected as a sample point</li>
<li><strong>Non-Probability Sampling</strong>: Some units in the population does not have a chance to be selected as a sample point</li>
</ol>
<p>From another dimension sampling methods can be categorized as:</p>
<ol type="1">
<li><strong>Without Replacement</strong> : when a unit is selected, it would be taken out from the sample population</li>
<li><strong>With Replacement</strong>: when a unit is selected, it would not be taken out from the sample population</li>
</ol>
<p>This article focuses on sampling methods that has application in machine learning.</p>
<h2 id="sampling-method"><strong>2. Sampling Method</strong></h2>
<h3 id="simple-random-sampling"><strong>2.1 Simple Random Sampling</strong></h3>
<p>In a random sampling, each unit in the sample population has a equal probability to be selected.</p>
<p><strong>Pros</strong>:</p>
<ul>
<li>Easy to apply</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>If the selection strategy is by any way associated with statistics through a confounder, a <strong>simpson paradox</strong> would be created</li>
<li>If the researcher has any interests on the group differences among partition of the sample frame, SRS is not accommodate to it</li>
</ul>
<h3 id="systematic-sampling">2.2 Systematic Sampling</h3>
<p>Systematic Sampling arranges all units in the population into some order, and then selecting elements at regular intervals through that ordered list.</p>
<p><strong>Pros</strong>:</p>
<ul>
<li>It help avoid selection bias. The sample point are uniformly distributed on the section sorted by a particular order</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>If there exists periodicity, and inner period of the order list is a multiple or factor of the interval we selected, the sample would be unrepresentative</li>
</ul>
<h3 id="stratified-sampling">2.3 Stratified Sampling</h3>
<p>Stratified Sampling separates units into some "Strata" according to some categorical characteristics. The percentage of these strata is called a <strong>sampling fraction</strong>. A subsection of the sample is then assigned to each strata so that the proportion of each category of sample points is still the sample as sampling fraction. In each strata, sample are selected through SRS or systematic sampling.</p>
<p><strong>Pros</strong>:</p>
<ul>
<li>It help avoid selection bias. The proportion of the category remains the same after sampling</li>
<li>Researcher can apply different sampling method on different subgroup. This allow them to choose most suitable method for a subgroup</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Time-consuming and hard to design when a sample point has multiple characteristics</li>
</ul>
<p>Stratified Sampling is the basic idea of many over-sampling and under sampling methods.</p>
<h2 id="monte-carlo-simulation"><strong>3. Monte Carlo Simulation</strong></h2>
<h3 id="inverse-transformation-sampling">3.1 Inverse Transformation Sampling</h3>
<p>We know that if a randam variable u = CDF(X), then u U(0,1) (refer to this <a target="_blank" rel="noopener" href="http://zhengyuanyang.com/2022/11/04/distribution/#distribution-transformation">article</a>). The Inverse Transformation Sampling generate such a uniformly distributed random variab u U(0,1). Then the sample value X for each u would be</p>
<p><span class="math display">\[X = CDF^{-1}(u )\]</span></p>
<p><strong>Pros:</strong> easy to apply</p>
<p><strong>Cons</strong>: given P(X), CDF^{-1}(X) cannot always been easily obtained</p>
<h3 id="mcmc-sampling"><strong>3.2 MCMC Sampling</strong></h3>
<p>Markov Chain Monte Carlo (MCMC) sampling is a simulation technique used to approximate complex probability distributions. It is particularly useful when direct sampling from the distribution is difficult or infeasible.</p>
<p>MCMC sampling works by constructing a Markov chain that has the desired distribution as its stationary distribution. The chain explores the distribution by iteratively sampling from the current state and moving to a new state based on a set of transition probabilities. Through a large number of iterations, the chain converges to the desired distribution, allowing for efficient sampling.</p>
<p>MCMC sampling has various applications in machine learning, including Bayesian inference, parameter estimation, and model fitting. It is especially valuable when dealing with high-dimensional spaces or complex models.</p>
<p><strong>Pros</strong>:</p>
<ul>
<li>Can sample from complex probability distributions</li>
<li>Useful for Bayesian inference and model fitting</li>
</ul>
<p><strong>Cons</strong>:</p>
<ul>
<li>Can be computationally intensive</li>
<li>Requires careful tuning of the Markov chain parameters</li>
</ul>
<p>MCMC sampling is a powerful tool that enables researchers to explore and approximate complex probability distributions, making it an essential technique in machine learning and statistical modeling.</p>
<h2 id="resampling-methods-for-imbalanced-data">4. Resampling Methods for Imbalanced Data</h2>
<p>An imbalanced dataset is one where the distribution of classes or categories is not equal or nearly equal. This can occur in various datasets, such as classification problems with multiple classes.</p>
<p>Imbalanced datasets pose challenges for machine learning algorithms as they can bias models towards the majority class, leading to poor performance in predicting the minority class. Addressing this issue requires specialized Resampling techniques.</p>
<p>A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).</p>
<p><img src="/2022/12/05/sampling/1.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="tomek-links">4.1 Tomek Links</h3>
<p>A Tomek Link exists between two samples if there are no other samples in their immediate vicinity.</p>
<p>Let <span class="math inline">\(x_i, x_j\)</span> denote two sample points, <span class="math inline">\(d(x_i,x_j)\)</span> denote the distance between these two points bu some measure, if:</p>
<ol type="1">
<li><span class="math inline">\(x_i.x_j\)</span> belong to different categories, and</li>
<li>There are no points <span class="math inline">\(x_k\)</span> that satisfy <span class="math inline">\(d(x_k,x_i)&lt;d(x_ i,x_j) \ or d(x_k,x_j)&lt;d(x_ i,x_j)\)</span></li>
</ol>
<p>then we call <span class="math inline">\((x_i,x_j)\)</span> a tomek link. In other words, a Tomek Link represents a pair of samples that are the closest neighbors of each other, but belong to different classes. For a tomek link, there exists two possiblities:</p>
<ol type="1">
<li>at least one of <span class="math inline">\((x_i,x_j)\)</span> is noisy</li>
<li><span class="math inline">\((x_i,x_j)\)</span> lies on the decision boundary</li>
</ol>
<p><img src="/2022/12/05/sampling/2.png" srcset="/img/loading.gif" lazyload></p>
<p>In either case, the classification model's performance would be undermined by the presence of tomek links. Therefore, we aim to eliminate the majority class instances involved in these links. This approach, known as under-sampling, is commonly used in imbalanced classification tasks. By removing the samples that form Tomek Links, we can better define the decision boundary between the majority and minority classes, which has the potential to improve the classifier's performance.</p>
<h3 id="edited-nearest-neighbors">4.2 Edited Nearest Neighbors</h3>
<p>Edited Nearest Neighbors (ENN) is an under-sampling technique used to reduce the number of samples in the majority class by removing instances that are misclassified by their nearest neighbors.</p>
<p>The ENN algorithm works as follows:</p>
<ol type="1">
<li>For each sample belongs to the majority class, find its k nearest neighbors based on a chosen distance metric.</li>
<li>If its class label differs from most of its neighbors, remove it from the dataset.</li>
</ol>
<p>By iteratively applying this process, ENN aims to identify and remove noisy or misclassified instances from the majority class, thereby balancing the class distribution and potentially improving the performance of machine learning models.</p>
<p><img src="/2022/12/05/sampling/3.png" srcset="/img/loading.gif" lazyload></p>
<p>ENN is a relatively simple and effective method for under-sampling in imbalanced datasets. However, it is important to note that ENN may discard informative instances if they happen to be misclassified by their nearest neighbors. Thus, the choice of the number of nearest neighbors (k) and the distance metric should be carefully considered to achieve the desired balance between class distribution and information preservation. Besides, as the number of minority points is relatively fewer, the points we can discard by ENN is limited.</p>
<h3 id="nearmiss">4.3 NearMiss</h3>
<p>NearMiss is an under-sampling technique commonly used for imbalanced datasets. It aims to select a subset of samples from the majority class that are "near" to the minority class.</p>
<p>There are three variations of the NearMiss algorithm:</p>
<ol type="1">
<li><strong>NearMiss-1</strong>: Reserve samples from the majority class that have the smallest average distance to the k nearest neighbors in the minority class.</li>
<li><strong>NearMiss-2</strong>: Reserve samples from the majority class that have the farthest average distance to the k nearest neighbors in the minority class.</li>
<li><strong>NearMiss-3</strong>: Selects samples from the minority class and reserve the k nearest neighbors of it in the majority class</li>
</ol>
<p><img src="/2022/12/05/sampling/4.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/2022/12/05/sampling/5.png" srcset="/img/loading.gif" lazyload></p>
<p>The NearMiss algorithm helps to address class imbalance by reducing the dominance of the majority class. By selecting samples that are "near" to the minority class, it aims to improve the classification performance on the minority class.</p>
<p>It is important to note that the choice of the NearMiss variant depends on the specific dataset and problem at hand.</p>
<p>An advantage of NearMiss is that the number(or the ration of number) of samples in majority class is controllable</p>
<h3 id="ensemble-methods">4.4 Ensemble Methods</h3>
<p>Except for specific algorithms, there are also some approaches based on ensemble learning that can be used to perform undersampling.</p>
<p><strong>EasyEnsemble</strong></p>
<p>EasyEnsemble is an ensemble learning technique specifically designed for imbalanced classification tasks. It works by creating multiple subsets of the majority class and combining them with the entire minority class. Each subset is created by <strong>randomly under-sampling (with replacement)</strong> the majority class to balance its distribution, or we can say, number, with the minority class. The resulting subsets are then used to train individual models.</p>
<p>During prediction, each model in the ensemble is used to make predictions, and the final prediction is determined by aggregating the predictions of all models. This approach helps to improve the classification performance on the minority class by giving more weight to its predictions.</p>
<p>It is worth noting that the number of subsets created in EasyEnsemble can be adjusted based on the severity of class imbalance and the available computational resources. The more subsets created, the more emphasis is placed on balancing the class distribution.</p>
<p><strong>BalanceCascade</strong></p>
<p>BalanceCascade is an ensemble learning technique commonly used for imbalanced classification tasks. It is an extension of the EasyEnsemble method and aims to further improve the classification performance on the minority class.</p>
<p>BalanceCascade works by iteratively training and combining multiple models. We train the model with the current dataset and obtain the classification results. Then, the misclassified instances from the majority class in the current dataset are identified and removed. This process is repeated for a predefined number of iterations or until a stopping criterion is met.</p>
<p>By iteratively focusing on the misclassified instances of the majority class, BalanceCascade aims to improve the classification performance by gradually reducing the impact of the majority class on the final prediction. This iterative process helps to refine the decision boundary between the classes and enhance the model's ability to correctly classify the minority class.</p>
<h3 id="synthetic-minority-oversampling-techniquesmote">4.5 Synthetic Minority Oversampling Technique(SMOTE)</h3>
<p><strong>SMOTE</strong></p>
<p>The Synthetic Minority Oversampling Technique (SMOTE) is a popular algorithm used for oversampling in imbalanced classification tasks. SMOTE works by creating synthetic examples of the minority class to balance the class distribution.</p>
<p>The SMOTE algorithm can be summarized in the following steps:</p>
<ol type="1">
<li><p>For a sample <span class="math inline">\(x_i\)</span> in the minority class, find its k nearest neighbors, denoted as <span class="math inline">\(NN\)</span></p></li>
<li><p>Randomly select(with replacement) <span class="math inline">\(x_{nn} \in NN\)</span>, make an interpolation by <span class="math display">\[
x_s = x_i + (x_{nn} - x_i) * rand(0, 1)
\]</span> where <span class="math inline">\(rand(0,1)\)</span> is a random number between 0 and 1, add <span class="math inline">\(x_s\)</span> to the minority class dataset.</p></li>
<li><p>Repeat steps 1 and 2 until the desired level of oversampling is achieved.</p></li>
</ol>
<p>By generating synthetic examples, SMOTE effectively increases the representation of the minority class in the dataset.</p>
<p>It is important to note that the choice of the number of nearest neighbors <span class="math inline">\(k\)</span> in SMOTE and the distance metric can affect the quality of the synthetic examples and the performance of the resulting models.</p>
<p>Additionally, there’s a trade-off for SMOTE. On one hand, if the selected minority class samples are surrounded by majority class samples, this minority sample points can be an invading noisy point or a point lies near the decision boundary. As a result, the newly synthesized samples will overlap significantly with the surrounding majority class samples, which is called an <strong>invasion</strong>, leading to classification difficulties. On the other hand, if the selected minority class samples are surrounded by other minority class samples, the newly synthesized samples will not provide much useful information, as it is far from the decision boundary(which is also a noisy point to some degree).</p>
<p>The former situation can be solved by append under-sampling method, such as Tomek links, right after the SMOTE algorithm. For the later situation, we can introduce Borderline-SMOTE</p>
<p><strong>Borderline-SMOTE</strong></p>
<p>Borderline-SMOTE is an enhanced version of SMOTE that focuses on generating synthetic examples near the decision boundary between the minority and majority classes.</p>
<p>Let <span class="math inline">\(S\)</span> denote the collection of minority sample points, <span class="math inline">\(L\)</span> denote the collection of majority sample points, <span class="math inline">\(T\)</span> denote the collection of all sample points. The Borderline-SMOTE algorithm can be summarized in the following steps:</p>
<ol type="1">
<li>For <span class="math inline">\(s\)</span> in <span class="math inline">\(S\)</span> , find the k nearest neighbors, denoted as <span class="math inline">\(NN\)</span> , let <span class="math inline">\(m\)</span> denote the number of points in <span class="math inline">\(NN\)</span> that belongs to <span class="math inline">\(L\)</span> : <span class="math inline">\(m = |NN \cap L|\)</span></li>
<li>Classify <span class="math inline">\(s\)</span> according to <span class="math inline">\(m\)</span>
<ol type="1">
<li>If <span class="math inline">\(m = k\)</span>, then <span class="math inline">\(s\)</span> is <strong>noise(invasion)</strong></li>
<li>if <span class="math inline">\(\frac{k}{2}\le m &lt;k\)</span>, then <span class="math inline">\(s\)</span> is <strong>danger(near decision boundary)</strong></li>
<li>if <span class="math inline">\(0\le m &lt;\frac{k}{2}\)</span>, then <span class="math inline">\(s\)</span> is <strong>safe(lack of information)</strong></li>
</ol></li>
<li>Only when <span class="math inline">\(s\)</span> is <strong>danger</strong>, implement SMOTE on <span class="math inline">\(s\)</span> and generate synthetic examples</li>
<li>Repeat step 1-3 for next point in <span class="math inline">\(S\)</span></li>
</ol>
<p>By generating synthetic examples near the decision boundary, Borderline-SMOTE aims to improve the classification performance on the minority class while avoiding the creation of invasion samples or noisy samples in regions far from the decision boundary.</p>
<p><img src="/2022/12/05/sampling/6.png" srcset="/img/loading.gif" lazyload></p>
<p>The algorithm above is in some cases called Borderline-SMOTE 1, as there is another version called Borderline-SMOTE 2. The difference between these two versions is in step 3. For Borderline-SMOTE 2, the step 3 is:</p>
<ol type="1">
<li>Only when <span class="math inline">\(s\)</span> is danger:
<ol type="1">
<li>respectively find its k nearest neighbors in <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span> , denoted as <span class="math inline">\(NN_L\)</span> and <span class="math inline">\(NN_S\)</span> , let <span class="math inline">\(n\)</span> denote the total number of synthetic samples we want to generate</li>
<li>Generate <span class="math inline">\(\alpha n\)</span> points with SMOTE on samples in <span class="math inline">\(NN_S\)</span></li>
<li>Generate <span class="math inline">\((1-\alpha) n\)</span> points with SMOTE on samples in <span class="math inline">\(NN_L\)</span></li>
</ol></li>
</ol>
<p>where <span class="math inline">\(\alpha\)</span> is a parameter controlling the ratio of synthetic samples we generate along the two sides of the decision boundary. This can control the invasions to some extent.</p>
<h3 id="adasyn">4.6 ADASYN</h3>
<p>ADASYN (Adaptive Synthetic Sampling) is an oversampling technique designed to address the class imbalance problem in imbalanced datasets. It focuses on generating synthetic examples according the distribution of the minority class data points.</p>
<p>The ADASYN algorithm can be summarized in the following steps:</p>
<ol type="1">
<li>Calculate the class imbalance ratio, which is the ratio of the number of majority class samples to the number of minority class samples: <span class="math inline">\(d = \frac{n_S}{n_L}\)</span></li>
<li>If <span class="math inline">\(d\)</span> is smaller than a preset threshold, than the dataset is imbalanced enough, otherwise, end the algorithm</li>
<li>Calculate the number of synthetic examples to generate for <span class="math inline">\(G\)</span>:</li>
</ol>
<p><span class="math display">\[
G = (n_L - n_S)\beta
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a controlling parameter . When <span class="math inline">\(\beta = 1\)</span>, the number of samples from the minority class will be supplemented to match the number of samples from the majority class.</p>
<ol start="4" type="1">
<li>For each point <span class="math inline">\(x_i\)</span> in <span class="math inline">\(S\)</span> , identify the k nearest neighbors of the <span class="math inline">\(s\)</span> , denoted as <span class="math inline">\(NN\)</span>, let <span class="math inline">\(r_i\)</span> denote the ratio of points in <span class="math inline">\(NN\)</span> that belongs to the majority class:</li>
</ol>
<p><span class="math display">\[
r_i = \frac{|NN \cap L |}{|NN|}
\]</span></p>
<ol start="5" type="1">
<li><p>For each point <span class="math inline">\(x_i\)</span> in <span class="math inline">\(S\)</span>, compute the density of it: <span class="math display">\[
p_i = \frac{r_i}{\sum_ i^{n_s}r_i}
\]</span></p></li>
<li><p>For each point <span class="math inline">\(x_i\)</span> in <span class="math inline">\(S\)</span>, generate <span class="math inline">\(p_i *G\)</span> synthetic samples with SMOTE</p></li>
</ol>
<p>ADASYN adapts the synthetic generation process based on the local density distribution of the minority class. It generates more synthetic examples for samples that are in regions with a lower density of minority class samples, making the synthetic examples more informative and challenging for classification.</p>
<p>A problem of ADASYN is that it may cause invasion, as it can generate more samples near invading noisy point. We can introduce other methods, such as borderline SMOTE or Tomek links to solve this problem.</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Machine-Learning/" class="category-chain-item">Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Sampling/">#Sampling</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Sampling Methods for Machine Learning</div>
      <div>http://example.com/2022/12/05/sampling/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Zhengyuan Yang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>December 5, 2022</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/12/13/bayesian-model/" title="Bayesian Model in Machine Learning">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Bayesian Model in Machine Learning</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/12/02/Basic-prob/" title="Basic of Probability Theory for Data Science">
                        <span class="hidden-mobile">Basic of Probability Theory for Data Science</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <i class="iconfont icon-bug"></i> <a href="http://zhengyuanyang.com/report/" target="_blank" rel="nofollow noopener"><span>Report Bug</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
