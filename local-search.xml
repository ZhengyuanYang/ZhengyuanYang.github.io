<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Evaluation for Clustering Model</title>
    <link href="/2023/03/06/evaluation-cluster/"/>
    <url>/2023/03/06/evaluation-cluster/</url>
    
    <content type="html"><![CDATA[<h1 id="evaluation-for-clustering-model">Evaluation for Clustering Model</h1><h2 id="evaluation-for-clustering-tendency">1. Evaluation for Clustering Tendency</h2><p>Before conducting clustering, we usually would evaluate whether there exists natural clustering tendency in the dataset. The most common measure is the Hopkins Statistics.</p><p>Let S denote a sample sapce <span class="math inline">\(S = (s_ 1,s_2,...s_n)\)</span>, X denote a feature space <span class="math inline">\(X = (x_1,x_2,...x_n)\)</span>.</p><p>Randomly pick a subsample p with k points from the sample space S <span class="math inline">\(p = (p_1,p_2,...p_k)\)</span>. For each <span class="math inline">\(p_i\)</span>, let <span class="math inline">\(a_i\)</span> denote the distance between $p_i $ and its nearest neighbor in the original sample S: <span class="math display">\[a_i = \min_{s_j \in S}\{ dist(p_i,s_j)\}\]</span> Then we create k artificial points, where each point's <span class="math inline">\(m^{th}\)</span> feature <span class="math inline">\(x_m\)</span> is generated uniformly from a distribution <span class="math inline">\(U(min[S_{x_m}],max[S_{x_m}])\)</span>. We call this collection q. For each <span class="math inline">\(q_i\)</span>, let <span class="math inline">\(b_i\)</span> denote the distance between $q_i $ and its nearest neighbor in the original sample S(the distance must be greater than 0): <span class="math display">\[b_i = \min_{s_j \in S,s_j \ne q_i}\{ dist(q_i,s_j)\}\]</span> Define the Hopkins statistics as: <span class="math display">\[HS = \frac{\sum_i^k b_i}{\sum_i^ka_i + \sum_i^k b_i}\]</span> If the data points are randomly distributed in the sample space, the HS should be approximately 0.5. The closer HS is to 1, the stronger clustering tendency there exists. Usually, we consider conduct a clustering if <span class="math inline">\(HS &gt; 0.75\)</span></p><h2 id="evaluation-for-clustering-goodness">2. Evaluation for Clustering Goodness</h2><p>The evaluation for Clustering Goodness can be categorized as internal evaluation and external evaluation. In <strong>external evaluation</strong>, we suppose we do have <strong>groud truth</strong>(true label) in the training process, but we still will not have that label in real application when the model is online. In such scenarios, we can simply use loss in classification and metrics in classification evaluation, such as entropy, impurity, accuracy and F-1 score.</p><p>However, the more common scenario is that we do not have a label to evaluate the quality of the clustering results even in training. In this case, we would need <strong>internal evaluation</strong>. Usually, we would consider a clustering results to be better if the points are compact inside a cluster while the clusters are sparse in the whole space. Thus, we often construct measure based on the internal distance of a cluster <span class="math inline">\(C_i\)</span> and the external distance between two cluster <span class="math inline">\(C_i,C_j\)</span>. Notice evaluating the model on a single metrics can be vert biased, as different metrics prefer different number of clusters. In most application, we would integrate multiple metrics to evaluate a clustering model.]</p><p><strong>Distortion</strong></p><p>The overall distortion is defined as: <span class="math display">\[D = \sum_k^K\sum_{x_i \in C_k} ||x_i - \mu_{C_k}||^2_2\]</span> Where <span class="math inline">\(\mu_{C_i}\)</span> is the centroid of <span class="math inline">\(C_i\)</span>,</p><p>The smaller distortion is, the better the clustering results are. We can replace the distance calculating method to <span class="math inline">\(||x-\mu||^p_p\)</span></p><p><strong>Cohesion, Separation and Silhouette Coefficient</strong></p><p>These three metrics are all on a sample point level.</p><p>The Cohension is the averge distance between a sample and all other samples in the same cluster: <span class="math display">\[a(i) = \frac{\sum_{j \ne i, j \in C_k} dist(x_i,x_j)}{N_k-1}\]</span> where <span class="math inline">\(N_k\)</span> is number of samples in Cluster k.</p><p>The Separation is the averge distance between a sample and all other samples in the nearest neighbor cluster: <span class="math display">\[b(i) = \frac{\sum_{j \in C_{nn}} dist(x_i,x_j)}{N_{nn}-1}\]</span></p><p>Good clustering performance should yield small a(i) and great b(i). Thus, we define the silhouette coefficient of a data point as: <span class="math display">\[s(i) \ \frac{b(i) - a(i)}{max\{a(i),b(i)\} }\]</span> we can find that when s(i) &lt;0, b(i) &lt; a(i), which means the clustering algorithm is invalid on this sample. When b(i) &gt; a(i), s(i) is in [0,1]. The greater s(i) is, the better clustering performance is.</p><p>We can then calculate the overall s(i) by average:</p><p><img src="/2023/03/06/evaluation-cluster/1.png"></p><p><strong>Calinski-Harabasz Index</strong></p><p>The Calinski-Harabasz Index is the ratio of the within-cluster- distance and between-cluster-distance. The within-distance is defined samely as distortion: <span class="math display">\[W = \sum_k^K\sum_{x_i \in C_k} ||x_i - \mu_{C_k}||^2_2\]</span> while the between-distance is defined as the weighted sum of the distance between centroid of cluster k and centroid of all samples: <span class="math display">\[B = \sum_k n_k \ ||c_ k-c||^2_2\]</span> Define the CHI as: <span class="math display">\[CHI =  \frac{B}{K-1}/\frac{W}{n-K}\]</span> where n is the number of all samples. The greater s is, the better clustering performance is. The theoratical range of CHI is <span class="math inline">\((0,+\infty)\)</span>. Note that CHI often prefer smaller number of cluster, while small K gives high CHI, it could also give high distortion. Again, we should evaluate the performance based on multiple metrics as a integer.</p><h2 id="evaluation-for-clustering-stability">3. Evaluation for Clustering Stability</h2><p>Clustering stability: an overview</p><p>luxberg</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
      <tag>Clustering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K-Means and GMM</title>
    <link href="/2023/03/05/Kmeans-GMM/"/>
    <url>/2023/03/05/Kmeans-GMM/</url>
    
    <content type="html"><![CDATA[<h1 id="k-means-and-gmm">K-Means and GMM</h1><h2 id="k-means">1. K-Means</h2><h3 id="about-k-means">1.1 About K-Means</h3><p>K-Means is a typical unsupervised learning algorithm for <a href="http://zhengyuanyang.com/2022/09/11/basic_knowledgefor_ML/#clustering">clustering</a>. It is a distance-based clustering algorithm.K-Means is a very interpretable algorithm.</p><p>We can understand K-Means from a EM algorithm perspective. The basic idea of K-means is to set a set of centers, and use the distance between a data point and all centers to decide which cluster the data point belongs.</p><p>Let a series of variable <span class="math inline">\(\mu = (\mu_1,\mu_2,...\mu_K)\)</span> denote the locations of the centers.<span class="math inline">\(\mu\)</span> represent a split plan of the clustering algorithm. For each data point <span class="math inline">\(x_n\)</span>, let <span class="math inline">\(r_n = (r_{n,1},r_{n,2},...r_{n,K})\)</span> denote a group of variable indicating which cluster the data point belong to, where <span class="math inline">\(r_{n,k}\)</span> is a binary variable(0 or 1). The objective of the K-means is to minimize the sum of distance between data points and center with a center: <span class="math display">\[J  = \sum_n \sum_k r_{n,k} ||x_n-\mu_k||^2\]</span> This function is the objective function of a K-means algorithm, it is also called the <strong>distortion</strong>.</p><p>Using a <a href="http://zhengyuanyang.com/2023/03/03/EM/">EM algorithm</a>:</p><ul><li>Initialization: Randomly Pick some data point</li><li>Expectation: Given <span class="math inline">\(\mu\)</span>, optimize J about $r_n $ (find a best split)</li><li>Maximization: Given <span class="math inline">\(r_n\)</span>, optimize J about <span class="math inline">\(\mu\)</span> (update the center)</li></ul><p><strong>Training Process</strong></p><p>Base on this idea, the training process of the K-Means is:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode"># Initialize Centers<br>centers = randomly select k data points as centers<br><br># the terminate condition include:<br>#    E converge: The points in the clusters does not change(clusters_i+1 = clusters_i)<br>#    M converge: The location of centers does not change (mu_i+1 -mu &lt; epsilon) <br>while terminate condition is not reached<br>  # find a best split<br>  for each x in X:<br>      for each center in centers:<br>          dis_k = distance(x,center) #usually euclidean distance<br>          dis.append(dis_k)<br>      # the closet center is the cluster x belong<br>      x.cluster = minidx(dis)<br>  # update centers using the data points in to it<br>  for each cluster:<br>      cluster.center = cluster.x.mean()<br><br></code></pre></td></tr></table></figure><p><strong>Assumption &amp; Properties</strong>:</p><ol type="1"><li>The k must be a given number. This could be difficult in real application(<strong>K selection</strong>)</li><li>K-Means is a distance-based method, it require scaling of feature(<strong>Scaling</strong>)</li><li>K-Means is sensitive to the initial centers selection. Bad choice on initial centroids could trap the algorithm in local optimal. So K-Means could be unstable when the initial centroid change.(<strong>Initial centroid selection</strong>)</li><li>K- Means assume each cluster is a convex space(a sphere). Consider kernel functions when this assumption is not satisfied. (<strong>Kernel function</strong>)</li><li>K-Means assume a cluster is uniformly. This means K_Means can be unstable when there are outlier in the dataset or when some variable are not normally distributed. Consider outlier detection and normality transformation before using K-Means. (<strong>Normality</strong>)</li><li>K-Means assume the feature are not correlated. (<strong>Correlation Analysis</strong>)</li><li>K-Means require encoding on categorical variable, this could give binary variable very high variance, and overestimate the effect of them. (<strong>Categorical Variable Processing</strong>)</li><li>The traditional K-Means use euclidean distance, this assume different features(directions) have same importance(weights) to similarity. In real application, this assumption could not always been fulfilled. <strong>(Feature Weighting)</strong></li><li>When a cluster actually exists in the space but failsto be recognize by the algorithm, we call it a hidden cluster. This can usually be caused by insufficient samples of that cluster(hidden clusters). K_Means cannot deal with imbalanced data hidden cluster.<strong>(Hierarchical Clustering)</strong></li></ol><p>Although K-Means has so many drawbacks, it still has wide application in industries. The core reason is that it is simple and interpretable, and there is only one hyper parameter to adjust. The unstable results is decided by the inner mechanism of K-Means. If we really cannot find a stable clustering results through K-Means, we should consider other algorithm.</p><p><img src="/2023/03/05/Kmeans-GMM/2.png"></p><h2 id="improvements-on-k-means">2. Improvements on K-means</h2><h3 id="k-selection">2.1 K-Selection</h3><p><strong>Score-K Plot(Elbow Method)</strong></p><p>The Elbow method depict the relationship of the number of K and the performance of clustering. Let S denotes a metrics that can evaluate the performance of the clustering model. The elbow graph shows the S'value(on the y-axis) corresponding to the different values of K(on the x-axis). When we see an <strong>elbow shape in the graph, we pick the K-value where the elbow gets created</strong>. We can call this point the <strong>Elbow point</strong>. Beyond the Elbow point, increasing the value of ‘K’ does not lead to a significant reduction (or increase) in S.</p><p><img src="/2023/03/05/Kmeans-GMM/1.png"></p><p>As shown in the picture, when K increase, the value of evaluating metric goes better while the cost of time being greater. Select the K with closest metrics value and time consuming as the elbow. The evaluating metric for the algorithm includes distortion, Silhouette Coefficient and Calinski Index. For details, refer to <a href="http://zhengyuanyang.com/2023/03/06/evaluation-cluster/">this article</a>. Note that choosing K on a single metric can be verty biased, since some metrics prefer smaller K while other prefer greater K. Select the optimal K with multiple metrics taken into account.</p><p><strong>ISODATA</strong></p><p>The ISODATA is a improved kmenas model with embedded K selection method. Comparing to traditional Kmeans, it requires 4 parameter:</p><ul><li><span class="math inline">\(K_0\)</span>: An expected number of K. The final K selection would be in <span class="math inline">\([{K_ 0 \over 2}, 2K_ 0]\)</span></li><li><span class="math inline">\(N_{min}\)</span>: the minimum number of data points in a cluster</li><li><span class="math inline">\(\sigma_{max}\)</span>: The max allowed <span class="math inline">\(\sigma\)</span>, where <span class="math inline">\(\sigma\)</span> is a metric to measure the within distance in a cluster</li><li><span class="math inline">\(d_{min}\)</span>: The min allowed d, where d is a metric to measure the distance between two clusters</li></ul><p>The training Process of the ISODATA is given as follows:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">Initialize Centers<br><br>while terminate condition is not reached<br>  find a best split based on distance<br>  <br>  <br>  for each cluster in the K_0 clusters:<br>  if the number of points in the cluster &lt; N_min:<br>  drop the cluster and label or data points in it with the nearest cluster<br>  K = K-1<br>  <br>update the centroids of all clusters<br><br>  if K &lt; K_0 / 2:<br>  # spliter some clusters into 2 clusters<br>  For each cluster:<br>  # calulate the sigma on each dimension(feature), and select the maximum<br>  s_max = max(sigma_x1,sigma_x2,...)<br>  if s_max &gt; sigma_max and n &gt; 2*N_min:<br>  execute a kmeans on this cluster with K=2 to split it into two cluster<br>  else:<br>  forward<br>  if K &gt; 2K_0:<br>  # Calculate the distance between each pair of clusters:<br>  D = distance_matrix(Clusters):<br>  while any D(i,j) &lt; d_min:<br>  merge cluster i and cluster j into one cluster<br>  #update the centroid of the new cluster<br>  c_temp = 1/(ni+nj)(ni*ci+nj*cj)<br>  update D<br>  <br>  # update centers using the data points in to it<br>  for each cluster:<br>      cluster.center = cluster.x.mean()<br><br></code></pre></td></tr></table></figure><p>Such a algorithm can adjust the number of K in the dynamic training process.</p><h3 id="k-means-1">2.2 K-Means++</h3><p>The K-Means++ is an improved version of K-Means aimed at optimizing the initial centroids selection. Basically, it believe the more separated the initial centroids are, the more stable the clustering results will be. For the initial centroids choosing:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode"># randomly pick a data point<br>c1 = random(dataset)<br>centers.append(c1)<br><br>while len(centers) &lt; k:<br>for each sample s:<br>        # calculate the minimum of the distance between <br>        D_s = min(dis(s, c1), dis(s,c2),....)<br>        D_s = D_s ** 2<br>        D.append(D_s)<br>     # Use the ratio of D[s] and sum of D as a probability of s being selected as a centroids<br>     Prob = D[s] / sum(D[s]) for s in all sample<br># decide the next centroid using a Roulette method<br>r = rand(0,1)<br>if  Prob[s-1] &lt; r &lt; Prob[s]:<br> # select s as a centroid<br> centers.append(s)<br><br></code></pre></td></tr></table></figure><h3 id="kernel-k-means">2.3 Kernel K-Means</h3><p>Kernel K-means is an unsupervised clustering algorithm that extends the traditional K-means algorithm by incorporating the kernel trick. The kernel trick allows for clustering of non-linearly separable data by implicitly mapping the data points to a higher-dimensional feature space. By using kernel functions, we can compute similarity between data points in the higher-dimensional space without explicitly calculating their high-dimensional representations.</p><p>The specific difference is, when calculating the distance between two data points, we know use a kernel distance(kernel trick) to evaluate the similarity of 2 data point in a high dimension.</p><p>Choose a kernel function: Select an appropriate kernel function, such as the Gaussian Radial Basis Function (RBF), polynomial kernel, or Sigmoid kernel. The kernel function is used to compute similarities between data points in the higher-dimensional space.</p><p>for details of kernel function, refer to <a href>ongoing</a></p><h3 id="k-means-on-categorical-variable">2.4 K-Means on Categorical Variable</h3><p>K-means algorithm has issues when dealing with categorical variables. This is mainly because K-means relies on Euclidean distance to compute similarity between data points, which is meaningful for continuous variables but not applicable to categorical variables. Categorical variables are typically non-numeric data that cannot be directly used to calculate Euclidean distances. Even if you transform categorical variables into numeric values through some feature engineering method(such as one-hot encoding), it still doesn't capture their actual relationships.</p><p>To address this issue, we can adopt the following approaches to improve the K-means algorithm for handling categorical variables:</p><p><strong>Alternative distance metrics:</strong></p><p>For categorical data, you can use other distance metrics such as Hamming distance or Jaccard similarity. These metrics can better capture the relationships between categorical variables. When implementing K-means, you can replace Euclidean distance with one of these metrics. For details of Distance, refer to <a href="http://zhengyuanyang.com/2022/10/08/distance/#jaccard-distance">this article</a></p><p>An implementation of this method is the <strong>K-Modes</strong> algorithm. K-Modes is a clustering algorithm specifically designed for categorical variables. It works by replacing the Euclidean distance with the <strong>matching distance</strong> and updating the cluster centroids using the mode instead of the mean. The matching distance is the number of mismatching attributes between two data points, for example a data point X = [1,2,5,3,0], and the centroid C = [1,2,4,3,2], then <span class="math inline">\(d_{match} = 3\)</span></p><p><strong>K-Prototypes algorithm:</strong></p><p>K-Prototypes is a hybrid of K-means and K-Modes algorithms. It is suitable for datasets containing both continuous and categorical variables. The K-Prototypes algorithm calculates a mixed distance based on the attribute type, combining the Euclidean distance for continuous variables with the matching distance for categorical variables. Similarly, updating the cluster centroids also requires considering both continuous and categorical variables separately.</p><p><strong>K-Medoids algorithm for binary data</strong>: The K-Medoids algorithm is a variant of the K-means algorithm that uses pairwise distances between data points instead of raw features to compute similarity. For categorical data, you can transform them into binary representation (e.g., one-hot encoding) and then use Hamming distance or other distance metrics suitable for binary data. The K-Medoids algorithm selects actual data points as cluster centers (i.e., medoids) instead of computing the centroid positions.</p><p>Remember that these methods are specifically designed to handle categorical variables in clustering algorithms. It is essential to select the most suitable method based on the characteristics of your data and the problem at hand.</p><h3 id="weighted-k-means">2.5 Weighted K-Means</h3><h3 id="hierarchical-clustering">2.6 Hierarchical Clustering</h3><p>​</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>K-Means</tag>
      
      <tag>GMM</tag>
      
      <tag>Clustering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Expectation Maximization Algorithm(EM)</title>
    <link href="/2023/03/03/EM/"/>
    <url>/2023/03/03/EM/</url>
    
    <content type="html"><![CDATA[<h1 id="expectation-maximization-algorithmem">Expectation Maximization Algorithm(EM)</h1><h2 id="baisc-idea-of-expectation-maximization-algorithm">1. Baisc Idea of Expectation Maximization Algorithm</h2><p>The Expectation Maximization Algorithm(EM) is a type of algorithm sharing the same basic idea. It's not a specific model but a way to solve problems like parameter estimation and clustering.</p><p>First, recall the formula of the <a href="http://zhengyuanyang.com/2023/02/13/prob-dense-estimation/#maximum-likelihood-estimation"><strong>Maximum Likelihood Estimation</strong></a>. Given a sample set, we suppose the variable follows a distribution <span class="math inline">\(P(X|\theta)\)</span>. According to MLE, we can estimate the parameter <span class="math inline">\(\theta\)</span> by: <span class="math display">\[\theta = argmax  \ \  ln[\prod_n P(X_n = x|\theta)] = argmax  \ \  \sum_n ln[P(X_n = x|\theta)]\]</span> Suppose we now have a hidden variable <span class="math inline">\(Z\)</span> that has j possible values. We cannot directly read Z from the dataset. Under each possible value <span class="math inline">\(z_i\)</span>, X follows a unique distribution <span class="math inline">\(X \sim D(\theta = \theta_i)\)</span>. The Likelihood estimation function should now be: <span class="math display">\[(\theta, Z) =argmax  \ \  \sum_n ln[\sum_i P(X_n = x,Z_n = z_i)|\theta_i)]\]</span> For example, suppose we want to estimate the average height of a group of student. As we all known, the average height of male and female are different. <span class="math inline">\(H_m \sim N(\mu_1,\sigma^2_1),H_f \sim N(\mu_2,\sigma^2_2)\)</span>. Suppose we do not have the variable sex in our data set, but we do know there are two distribution, and we want to estimate both of them. In this case, when given a sample, there are actually two thing to do:</p><ol type="1"><li>What's the gender(hidden variabl) of a sample?</li><li>Given the gender, update according distribution's average height(parameter)</li></ol><p><strong>We can see when we need a EM algorithm: There exists multiple distributions. We don't know which distribution a sample is drawn from, but we do know this process is decided by a hidden variable not collected in the data.</strong></p><p>Now back to the equation of above, we know that finding out the derivatives of this function is difficult, as the log of the margin distribution <span class="math inline">\(log(\sum_z P( x, z))\)</span>'s derivative is very hard to solve. In this context, we can transform the format: <span class="math display">\[\sum_n ln[\sum_i P(X_n = x,Z_n = z_i)|\theta_i)] =  \sum_n ln[\sum_i Q(z_i)\frac{P(x,z_i|\theta_i)}{Q(z_i)}]\]</span> Where we demand <span class="math inline">\(Q(z)\)</span> is a new distribution such that: <span class="math inline">\(\sum Q(z_i)\)</span></p><p>according to the Jensen inequality, <span class="math inline">\(log(\sum_i\lambda_iy_i) \ge \sum_i\lambda_i[log(y_i)]\)</span> , the equality is satisfied when <span class="math inline">\(y_i = c\)</span>.</p>Thus: <span class="math display">\[\iota(\theta,z|X) = \sum_n ln[\sum_i Q(z_i)\frac{P(x,z_i|\theta_i)}{Q(z_i)}] \ge \sum_n\sum_iQ(z_i)log\frac{P(x,z_i|\theta_i)}{Q(z_i)}\]</span> According the condition of the equality fulfillment of Jensen inequality: $$<span class="math display">\[\begin{aligned}\frac{P(x,z_i|\theta_i)}{Q(z_i)} &amp;= c \\P(x,z_i|\theta_i)&amp;= c Q(z_i)\\\sum_i P(x,z_i|\theta_i)&amp;= c \sum_i Q(z_i) = c\\Q(z_i) &amp;= \frac{P(x,z_i|\theta)}{c} = \frac{P(x,z_i|\theta_i)}{\sum_iP(x,z_i|\theta_i)}\\&amp;= \frac{P(x,z_i|\theta_i)}{P(x,|\theta_i)} = P(z_i|x,\theta_i)\end{aligned}\]</span><p><span class="math display">\[We now know that when we select $Q(z_i) = P(z_i|x,\theta_i)$, what we get is **a low boundary** of the log likelihood function. Also: \]</span> P(z_i|x,_i) =  =  $$ Suppose the value of the hidden variable is <span class="math inline">\(z_i\)</span>, and the parameter of the distribution when <span class="math inline">\(Z = z_i\)</span> is <span class="math inline">\(\theta_i\)</span>, the basic assumption of the problem can be solved by EM is, when <span class="math inline">\(\theta_i\)</span> is fixed for a value, and the sample results x are given, we should be able to calculate the specific value of <span class="math inline">\(P(z_i,x|\theta = \theta_i)\)</span></p><p>Such a fact means, we can directly calculate <span class="math inline">\(Q(Z)\)</span> from the observation on data. <strong>This save us from calculating the <span class="math inline">\(Z^*\)</span> by <span class="math inline">\(\frac{\partial L}{\partial Z} = 0\)</span></strong>, where <span class="math inline">\(Z^*\)</span> denotes the optimal value of <span class="math inline">\(Z\)</span> about L when <span class="math inline">\(\theta_i\)</span> is fixed.</p><p>According to such assumption, the EM algorithm can be conclude as:</p><ol type="1"><li><p>Randomly select a value for each <span class="math inline">\(\theta_i\)</span> in <span class="math inline">\(\theta_i = \theta_{i,0}\)</span></p></li><li><p><strong>Expectation</strong>: calculate the conditional probability<span class="math inline">\(Q(z_j) = P(Z_n = z_i,X_n = x|\theta_i = \theta_{i,0})\)</span>, <span class="math inline">\(Q(z_j)\)</span> should yield a constant for each <span class="math inline">\(z_i\)</span></p></li><li><p><strong>Maximization</strong>: bring <span class="math inline">\(Q(z)\)</span> back the likelihood function, since <span class="math inline">\(Q(z_i)\)</span> is now a constant and do not have an influence to derivative, we can reformat the likelihood function as: <span class="math display">\[\theta_{i,1} = argmax \ \ \sum_n\sum_i Q(z_i)log(P(X_n = x,Z_n = z_i|\theta_{i,0}))\]</span></p></li><li><p>Let <span class="math inline">\(\theta_i = \theta_{i,1}\)</span>, repeat step 2-4, until <span class="math inline">\(\theta\)</span> converge</p></li></ol><h2 id="example-of-em">2. Example of EM</h2><p>Suppose we have 2 coin A and B, each with a different probability <span class="math inline">\(p_1,p_2\)</span> of flipped as a head. We randomly select a coin and flip it 10 times and call it a trail. We repeat 5 trail and obtian 5 samples. Let:</p><ul><li><span class="math inline">\(X_n\)</span> denote the number of heads in the <span class="math inline">\(n^{th}\)</span> trail</li><li><span class="math inline">\(Z_n\)</span> denote the selection of coin in the <span class="math inline">\(n^{th}\)</span> trail</li><li><span class="math inline">\(\theta_{A,k}, \theta_ {B,k}\)</span> denote the estimation of <span class="math inline">\(p_1,p_2\)</span> in the <span class="math inline">\(k^{th}\)</span> iteration</li></ul><p>suppose we have the following data(unlike shown in the picture, we do not know which coin is selected)</p><p><img src="/2023/03/03/EM/1.png"></p><p><strong>Step 1</strong></p><p>Initialize <span class="math inline">\(\theta_A = 0.6, \ \theta_ B = 0.5\)</span></p><p><strong>Step 2</strong></p>when <span class="math inline">\(Z = A, \theta = \theta_A\)</span>, now looking are the data of the first trail $$<span class="math display">\[\begin{aligned}P(Z_1 = A|X_1,\theta_{A,1})  &amp;= \frac{P(Z_1 = A, X_1=5|\theta_{A,1} = 0.6)}{P(Z_1 = A, X_1=5|\theta_{A,1} = 0.6) + P(Z_1 = B, X_1=5|\theta_{B,1} = 0.5)} \\&amp; =\frac{0.6^5*(1-0.6)^5}{0.6^5*(1-0.6)^5 + 0.5^5*(1-0.5)^5} \\&amp; = 0.45\end{aligned}\]</span><p>$$ Samely, <span class="math inline">\(P(Z_1 = B|X_1,\theta_{B,1})\)</span> is 0.55. Repeat the calculation for each of the <span class="math inline">\(n\)</span>trails. Let <span class="math inline">\(\mu_{A, n} = P(Z_n = A|X_n,\theta_{A,1}), \quad \mu_{B, n} = P(Z_n = B|X_n,\theta_{B,1})\)</span></p><p><strong>Step 3</strong></p><p>Format the likelihood function: <span class="math display">\[\begin{aligned}L(\theta_{A,1},\theta_{B,1}) &amp;= \sum_n \sum_{z_i}P(Z_n = z_i|X_n,\theta_{z_i,1})\ logP(X_n, Z_n|\theta_{z_i,1})\\&amp; =\sum_{n=1}^5 \mu_{A,n} log(\theta_{A,1}^{X_n}(1-\theta_{A,1})^{10-X_n} ) + \mu_{B,n}log(\theta_{B,1}^{X_n}(1-\theta_{B,1})^{10-X_n} )\end{aligned}\]</span> <strong>Step 4</strong></p><p>Now find the partial derivatives of the Likelihood function and let them euqals to 0, updated the parameters by the optimal value. <span class="math display">\[\theta_{A, 2} = \theta_{A,1}^* = solve (\frac{\partial L_1 }{\partial \theta_{A,1}} = 0)\]</span></p><p><span class="math display">\[\theta_{B, 2} = \theta_{B,1}^* = solve (\frac{\partial L_1 }{\partial \theta_{B,1}} = 0)\]</span></p><h2 id="application-of-em-algorithm">3. Application of EM Algorithm</h2><p>The application of the thought of EM algorithm includes:</p><ul><li>K-Means and GMM</li><li>Sequential Minimal Optimization in SVM</li><li>HMM</li></ul>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>EM Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Decision Tree and Ensemble Models</title>
    <link href="/2023/02/27/decision-tree/"/>
    <url>/2023/02/27/decision-tree/</url>
    
    <content type="html"><![CDATA[<h1 id="decision-tree-and-ensemble-models">Decision Tree and Ensemble Models</h1><h2 id="decision-tree">1. Decision Tree</h2><h3 id="about-decision-tree">1.1 About Decision Tree</h3><p>Decision Tree is a data structure that can split data into subsets. In Machine Learning, we can use such a structure as our hypothesis space to segment our datasets into units and perform prediction.</p><p><img src="/2023/02/27/decision-tree/1.png"></p><p>A decision tree consists of decision nodes and leaf nodes. Each decision node represent a split of sample based on some conditions a input feature <span class="math inline">\(X_i\)</span>. A directed route of the decision nodes represent a series of <strong>conditioning</strong> on the sample sapce. Each leaf node represents a subsample space obtained from a unique route, and we can estimate the conditional probability distribution <span class="math inline">\(P(Y|X=x)\)</span> from the leaf node. The output variable Y can be either continuous(regression) or discrete(classification).</p><p>The decision tree is a non-parameter model. It can be infinitely extended without a constraint on max tree depth.</p><h3 id="training-process">1.2 Training Process</h3><p>The training process of decision tree is a recursive process. Let D denote the original sample set, X denote the feature spaces</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">def splitting(tree):<br>while impurity metrics is not reached:<br>x+i = select a feature x_i in feature space X(tree)<br>sub_tree_left,sub_tree_right = find the best split based on x_i(tree, x_i)<br>tree.left = splitting(sub_tree_left)<br>tree.right = splitting(sub_tree_right)<br>  return tree<br><br>splitting(D)<br><br>def label_and_predict(tree, sample):<br>for leaf in tree:<br>if a classification_tree:<br># use the mode of Y of training samples belong to this leaf node as the prediction<br>leaf.pred = leaf.Y.mode()<br>else:<br># use the mean of Y of training samples belong to this leaf node as the prediction<br>leaf.pred = leaf.Y.mean()<br><br>leaf_node_for_sample = find the leaf node the sample belong to according to sample&#x27;s X<br>sample_pred = leaf_node.pred<br><br></code></pre></td></tr></table></figure><p><img src="/2023/02/27/decision-tree/2.png"></p><p>The implementation of Decision Tree includes ID3, C4.5 and CART. The differences of these implementations are the strategy they select layer(feature) and best split.</p><h2 id="implementation-of-decision-tree">2. Implementation of Decision Tree</h2><h3 id="id3">2.1 ID3</h3><p>ID3 is a decision tree that can deal only with <strong>finite discrete inputs</strong>(usually <strong>categorical features</strong>). Suppose we have a sample set D and a feature space X, where <span class="math inline">\(X_i\)</span> is a finite discrete feature like sex, country. If <span class="math inline">\(X_i\)</span> has infinite values or is continues, then the ID3 requires the feature to be binned into finite discrete sections.</p><h4 id="feature-selection-and-splitting">2.1.1 Feature Selection and Splitting</h4><p>The strategy of selecting feature to split is based on Information Gain. For the explanation of IG, refer to <a href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">this article</a>.</p><p>For each feature <span class="math inline">\(X_i\)</span>: <span class="math display">\[\begin{aligned}IG(X_i) &amp;= H(D) - H(D|X_i) \\&amp;= -\sum_m^k p(y=c_m)log(p(y=c_m)) - (-\sum_n\sum_m p(y=c_m|X_i = x_n)p(X=x_n)log(p(y=c_m|X_i = x_n)))\end{aligned}\]</span> According to the definition, the feature with the largest imformation can reduce the uncertainty of Y to the greatest extent, thus would be selected as the feature to split. There is no specific techniques in finding best split, since all features are categorical, just split the tree into n nodes, where n is the number of categories of feature <span class="math inline">\(X_i\)</span></p><h4 id="properties">2.1.2 Properties</h4><ul><li>ID3 does not have any pruning strategy</li><li>ID3 perfer features with large number of category</li><li>ID3 can only deal with finite discrete inputs if no binning is implemented.</li><li>ID3 cannot deal with missing values</li><li>Since or feature is catrgorical, once a featrure is selected a layer, it would not appear in the following subtrees any more.</li></ul><h3 id="c4.5">2.2 C4.5</h3><p>C4.5 is a improved version of ID3. One drawback of ID3 is that it would perfer features with large number of category. For example, if there is a unique identity number for each sample, then that feature's IG would be 1, but this provide no clues for future's prediction.</p><h4 id="feature-selection-and-splitting-1">2.2.1 Feature selection and splitting</h4><p>To solve this, the C4.5 use Information Gain Rate to select feature: <span class="math display">\[IGR(X_i) = \frac{IG(X_i)}{H(D)}\]</span> this would penalize those features with to many categories, as these features usually have larger entropy. However, this could bring the algorithm to the opposite that it would prefer feature with less number of categories. This means the absolute values of IG can be low while the IGR is high. To fix this, the C4.5 would first do a feature selection based on IG:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">for X_i in X:<br>if IG(X_i) &lt; IG(X).avg()<br>X.drop(X_i)<br></code></pre></td></tr></table></figure><p>In addition, the C4.5 introduce different techniques to deal with selection on continuous input feature, for a continuous feature <span class="math inline">\(X_i\)</span>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">for X_i in X<br>if X_i is continuous:<br>      Sort all sample by X_i in a ascending order<br>      for each adjacent samples<br>          m_j = the medium of X_i_j,X_i_(j+1) #use the medium as a threshold<br>          #Z is a label flagged whether X_i of a sample is above the threshold<br>          Z = Z_right if x_i_j &lt; m_j else Z_right <br>          IGR_j = IGR(D, Z) #A binary split based on Z<br>      # The split with the highest IGR is the best split of feature X_i<br>      IGR_best_i = max(IGR_j)<br>    else:<br>    IGR_best_i = calculate IGR(X_i) as a categorical variable like ID3<br>    <br># Compare all<br>the feature to select = argmax_Xi(IGR_best_i)<br><br></code></pre></td></tr></table></figure><p>The process put feature selection and feature splitting together. It first use a enlightening method to filter some feature with low IG. then calculate the IGR for the rest of the features. For a continuous feature, it use the split with highest IGR as the best split. For a categorical feature, it still generate a child node for each category.</p><h4 id="pruning-strategy">2.2.2 Pruning Strategy</h4><p>One drawback ID3 is that it does not have a pruning strategy, thus can be easy to overfit. In C4.5, post-pruning is applied to control the complexity of the model. The post-pruning means the algorithm would first make the split and then decide whether to prune the subtrees based on the error rate on the leaf node. Specifically, the post-pruning strategy is a method called Pessimistic Error Pruning.</p><p>For a decision node, suppose the algorithm first split <span class="math inline">\(n_L\)</span> child nodes based on a feature. Then:</p><ul><li>Let <span class="math inline">\(E_i\)</span> denote the number of false classification in the <span class="math inline">\(i^{th}\)</span> node</li><li>Let <span class="math inline">\(N_i\)</span> denote the number of samples in the <span class="math inline">\(i^{th}\)</span> node</li><li>Let <span class="math inline">\(\gamma\)</span> be a penalty parameters for complexity of model</li></ul>The error rate of splitting and treat the node as a subtree: <span class="math display">\[e_{tree} = \frac{ \sum_iE_i + \gamma*n_L}{\sum_iN_i}\]</span> Consider a random variable <span class="math inline">\(M_{tree}\)</span> denoting the number of misclassification. For example, if the output variable Y is binary, we can consider <span class="math inline">\(M_{tree} \sim Bino(e,\sum_i N_i)\)</span> $$<span class="math display">\[\begin{aligned}E[M_{tree}] &amp;= \sum_iN_i *e_{tree}\\Std[M_{tree}] &amp;= \sqrt{\sum_i N_i*e_{tree}*(1-e_{tree})}\end{aligned}\]</span><p><span class="math display">\[ Now apply pruning and replace the subtree with a leaf node. Let E, N denote the number of misclassified samples and all samples in the leaf node, calculate the error rate if not splitting and treat the node as a leaf node:\]</span> e_{leaf} =  <span class="math display">\[Accordingly:\]</span> E[M_{leaf}] = N*e_{leaf} <span class="math display">\[If $M_{leaf}$ is significant greater than $M_{tree}$, we can make the judgement that the splitting is necessary. When N is big, the binomial distribution is approximate to a Normal distribution. Thus, through a test,the condition of accepting a splitting is:\]</span> p = T_test(E[M_{leaf}] - E[M_{tree}])  $$ Otherwise, the subtree should be pruned.</p><h4 id="missing-value-processing">2.2.3 Missing Value Processing</h4><p>When there exists missing values in features, two problem needs to be solved by the decision tree:</p><ul><li>How to calculate the IG/IGR of <span class="math inline">\(X_i\)</span> when there are missing values in <span class="math inline">\(X_i\)</span>?</li><li>When training, how to decide which sub-node the sample containing missing value belong?</li><li>When predicting, how to label the sample when it meet a node that based on its missing values?</li></ul><p>For the first question, the solution by C4.5 is to calculate the IGR on samples without missing values and then add a weight to the IGR. Let the weight be: <span class="math display">\[\rho  = \frac{n_{\widetilde{D}}}{n_D}\]</span> where <span class="math inline">\(D, \widetilde{D}\)</span> is the orginal sample set and a subset that samples with missing values of <span class="math inline">\(X_i\)</span> are dropped. <span class="math inline">\(n\)</span> denote the number of samples. The revised IGR would then be: <span class="math display">\[IGR(X_i)&#39;/IG(X_i)&#39; = \rho * IGR(X_i)/IG(X_i)\]</span></p><p>For the second question, let <span class="math inline">\(w_k\)</span> denote the fraction of samples that belong to the <span class="math inline">\(k^{th}\)</span> category(or value for continuous variable): <span class="math display">\[\omega_k = \frac{n_k}{n_ {\widetilde{D}}}\]</span> If a sample's value on <span class="math inline">\(X_i\)</span> is missing when trying to split the samples based on <span class="math inline">\(X_i\)</span>, then those missing value samples would <strong>enter every child node with a weight <span class="math inline">\(\omega_k\)</span></strong>. This means, when further splitting the nodes, the <span class="math inline">\(P(y = c_m)\)</span> in the information gain now calculated as: <span class="math display">\[P(y = c_m) = \frac{\sum \omega_{k, y= c_m}}{\sum \omega_k}\]</span> and the weight of information gain would also be: <span class="math display">\[\rho = \frac{\sum\omega_\widetilde{D}}{\sum \omega_D}\]</span> For example, suppose we have the following dataset</p><p><img src="/2023/02/27/decision-tree/3.png"></p><p>In this first split, <span class="math inline">\(\widetilde{D} = \{1,2,3,4,5,7,8,,9,10,11,12,13,14\}\)</span>, <span class="math inline">\(\rho = \frac{13}{14}\)</span>, then <span class="math display">\[\begin{aligned}H(\tilde{D}) &amp;= [-\frac{5}{13}*log(\frac{5}{13})-\frac{3}{13}*log(\frac{3}{13})-\frac{5}{13}*log(\frac{5}{13})]\\IGR &amp;= \frac{H(\tilde{D}) - H(\tilde{D}|outlook)}{H(\tilde{D})}\\IGR&#39; &amp;= \frac{13}{14}*IGR\end{aligned}\]</span> Suppose outlook is the feature with max IGR'. Let split the sample based on outlook. Since there is a sample 6 that missing value "outlook", let will enter each node with a weight. <span class="math inline">\(\omega_s = \frac{5}{13}, \omega_o = \frac{3}{13}, \omega_r = \frac{5}{13}\)</span></p><p>Now move the subtree that "outlook = sunny"</p><p><img src="/2023/02/27/decision-tree/4.png"></p><p>suppose we want to further split the tree and we want to calculate the IGR of "Windy", now: <span class="math display">\[\begin{aligned}\rho &amp;= \frac{5+\frac{5}{13}}{5+\frac{5}{13}} = 1\\H(\tilde{D_s}) &amp;= [-\frac{2+\frac{5}{13}}{5+\frac{5}{13}}*log(\frac{2+\frac{5}{13}}{5+\frac{5}{13}})-\frac{3}{5+\frac{5}{13}}*log(\frac{3}{5+\frac{5}{13}})]\\IGR &amp;= \frac{H(\tilde{D_s}) - H(\tilde{D_s}|Windy)}{H(\tilde{D_s})}\\IGR&#39; &amp;= 1*IGR \end{aligned}\]</span></p><p>Finally, for the third question, when we try to make the prediction. Suppose we have a decision tree like this:</p><p><img src="/2023/02/27/decision-tree/5.png"></p><p>Now we have a sample that "outlook = sunny" and "humidity = null". To decide the label of this sample, we just calculate the conditional probability distribution of Y when the sample meet the node it is missing value of.</p><p>For the humidity node: <span class="math display">\[\begin{aligned}P(Class = play|sunny) =&amp; P(Class = Play|Humidity \le 75|sunny) P(Humidity \le 75|sunny) \\&amp;+ P(Class = Play|Humidity &gt; 75|sunny) P(Humidity &gt; 75|sunny)\\=&amp;\frac{2}{5.4}*1+\frac{3.4}{5.4}*\frac{3}{3.4} = 44\%\end{aligned}\]</span></p><p><span class="math inline">\(P(Class = Play) &lt; P(Class = not \ play)\)</span>, thus the prediction is "play"</p><h4 id="properties-1">2.2.4 Properties</h4><ul><li>C4.5 is a classification algorithm. Although it deal with continuous features, the output features still needs to be categorical</li><li>For continuous feature, C4.5 sort the samples and split the samples into 2 parts. The continuous feature can be applied again in the subtrees.</li><li>For categorical feature, C4.5 still split the samples into n parts. The categorical feature won't be applied again in the subtrees.</li><li>C4.5 is still a Multi-tree. It has lower efficiency than CART.</li></ul><h3 id="classification-and-regression-treecart">2.3 Classification and Regression Tree(CART)</h3><p>The C4.5 algorithm can only deal with classification tasks, and the efficiency of it is low. CART is a improved algorithm for C4.5.</p><h4 id="feature-selection-and-splitting-2">2.3.1 Feature selection and splitting</h4><p>CART has similar processing like C4.5, which sort the samples in an ascending order and find a best split to segment the samples in 2 parts. The difference is CART applies such method on all feature, not just continuous features, and it provides a different metrics to measure a split.</p><p>In addition, CART can do both regression and classification. When the output variable is continuous, it use a combined MSE to measure the error of a split: <span class="math display">\[MSE = [\sum_{i \in D_{left}}(y_i-\bar{y}) + \sum_{i \in D_{right}}(y_i-\bar{y})]\]</span> When doing classification, CART use the <a href="http://zhengyuanyang.com/2022/09/22/loss-function/#gini-impurity">Gini Index</a> as a metric: <span class="math display">\[G = 1- \sum_i^mP(y = c_m)\]</span></p><p><span class="math display">\[Combined \ Gini = G_{left} + G_{right}\]</span></p><p>Unlike information gain, both metrics are better when lower.</p><p>The feature selection process would then be:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">for X_i in X<br>      Sort all sample by X_i in a ascending order<br>      for each adjacent samples<br>          m_j = the medium of X_i_j,X_i_(j+1) #use the medium as a threshold<br>          #Z is a label flagged whether X_i of a sample is above the threshold<br>          Z = Z_right if x_i_j &lt; m_j else Z_right<br>          #A binary split based on Z<br>          if a regression tree:<br>          I_j = combine_MSE(D, Z)<br>          else:<br>          I_j = combined_Gini(D,Z)<br>      # The split with the lowest MSE or Gini is the best split of feature X_i<br>      I_best_i = min(I_j)<br><br>    <br># Compare all<br>the feature to select = argmax_Xi(I_best_i)<br></code></pre></td></tr></table></figure><h4 id="pruning-strategy-1">2.3.2 Pruning Strategy</h4><p>The pruning strategy applied in CART is called Cost Complexity Pruning. It is also a kind of post-pruning. The idea of this strategy is similar like Pessimistic Error Pruning in C4.5, but it works different.</p><p>Suppose we have a parent node t. Let <span class="math inline">\(T\)</span>be tree below node t(<span class="math inline">\(T\)</span> use t as a root node). Let C be error function that can evaluate the performance of <span class="math inline">\(T\)</span>. For regression, C can be MSE, for classification, C can be misclassification rate. Construct a regularization term <span class="math inline">\(C_\alpha\)</span>: <span class="math display">\[C_\alpha(T) = C(T) + \alpha|T|\]</span> where |T| is the number of leaf nodes in T, <span class="math inline">\(\alpha\)</span> is a hyperparameter that adjust the penalty of complexity.</p><p>Similarly like the C4.5, we calculate the difference on <span class="math inline">\(C_\alpha\)</span> between use the tree T and replace T with a single leaf node. Define the cost of doing so as: <span class="math display">\[g(t) = \frac{C_\alpha(T_{leaf}) - C_\alpha(T_{tree})}{|T|-1}\]</span> <span class="math inline">\(g(t)\)</span> represent the cost on performance if replace the non-leaf node t with a leaf node.</p><p>The pruning strategy is then:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">non-leaf nodes sorted = sort the non-leaf nodes in original DT from the bottom to the top<br>for node in non-leaf nodes sorted:<br># calculate the cost of each node<br>cost_nodesa.append(g(node))<br># fine the node with least cost and prune that node<br>node_to_prune = argmin(cost_node_i)<br>DT_temp = replace node_to_prune with a leaf node<br># once a node is pruned, record the updated tree<br>DTs.append(DT.temp)<br><br># validate each tree in the record and select the best<br>for DT_j in DTs:<br>validate the performance of DT_j<br>select the DT_j with best performance<br><br></code></pre></td></tr></table></figure><h4 id="missing-value-processing-1">2.3.3 Missing Value Processing</h4><p>Back to the three questions:</p><ul><li>How to calculate the MSE/Gini of <span class="math inline">\(X_i\)</span> when there are missing values in <span class="math inline">\(X_i\)</span>?</li><li>When training, how to decide which sub-node the sample containing missing value belong?</li><li>When predicting, how to label the sample when it meet a node that based on its missing values?</li></ul><p>For the first question, the solution by CART is same as C4.5, which is give the MSE or Gini with a weight: <span class="math display">\[\rho = \frac{n_{\widetilde{D}}}{n_D}\]</span></p><p>where <span class="math inline">\(D,\tilde{D}\)</span> is the orginal sample set and a subset that samples with missing values of <span class="math inline">\(X_i\)</span> are dropped. n denote the number of samples.</p><p>The Gini/MSE would then be: <span class="math display">\[Gini&#39;\ or \ MSE&#39; = \frac{ 1}{\rho} * (Gini\ or \ MSE)\]</span></p><p>For the scond and third question, the CART use a method call <strong>surrogate splitter</strong>. Suppose we select <span class="math inline">\(X_i\)</span> from X as our best splitting feature. If a sample reach the splitter with missing value on <span class="math inline">\(X_i\)</span>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">IG = Gini or MSE of the Decision node on X_i<br><br>thresh = a threshold measure the difference of IG on two feature<br><br>for X_j in the X except for X_i:<br>IG_j = calculate Gini or MSE of X_j on current decision node<br># record X_j as a surrogate splitter<br>SS.append(j, IG_j)<br># rank the surrogate splitters according to there information gain<br><br>SS = SS.rank by S_j<br>for j,IG_j in enumerate(SS):<br>if sample.X_j is missing: drop X_j from SS<br>if IG - IG_j &gt; thresh: drop X_j from SS<br>if there are surrogate splitter left in SS:<br>decide the sample based on X_SS[0] #decide the sample by the best surrogate splitter<br>else:<br>let the sample enter the leaf node with most samples in it<br><br></code></pre></td></tr></table></figure><p>In this method, for a best spliter node <span class="math inline">\(X_i\)</span>, if a sample happen to miss value <span class="math inline">\(X_i\)</span>, we will calculate the MSE or Gini of the rest features based on the samples in this node. We rank these features by Gini or MSE and apply them as a surrogate splitter.</p><p>Given a sample, a legal surrogate splitter should guarante:</p><ul><li>The sample should not miss its value on the feature this splitter based on</li><li>The surrogate splitter should not be too different too the original splitter, that is, the difference between there Gini or MSE should be within a threshold</li></ul><p>In the legal surrogate splitters, select the best(with lowest Gini or MSE) surrogate splitter to decide which child node the sample should go to. If there are no legal surrogate splitters, just put the sample into the node that have the most samples.</p><p>This method is very time-consuming, as it demands metrics calculation on all features in every split.</p><h4 id="properties-of-cart">2.3.4 Properties of CART</h4><ul><li>CART can deal with both Regression and Classification</li><li>Features in CART would be applied as splitters repeatedly. We can use the frequency they emerge as a feature importance measure</li><li>CART is a strict binary tree, and it does not involve log calculations, thus is faster than C4.5</li><li>CART has different pruning and missing value processing strategy comparing to C4.5</li><li>CART suit big sample, the variance can be high if applied on small sample</li></ul><h2 id="ensemble-model">3. Ensemble Model</h2><p>As single model can have problem with bias or variance. An ideal is to combine multiple basic learner(single model) into a strong learner(ensemble model). The way to combine the basic learners includes:</p><p><strong>Bagging</strong></p><p>Bootstrap aggregating is a method to deal with high variance of model.It randomly take samples with replacement from the original dataset. Each subset is sent to a weak learner and used to estimate the probability <span class="math inline">\(P(Y|X)\)</span>. All weak learner work parallelly. When a new sample sent to the model for prediction, it would go through all weak learner and get a set of partial prediction. The weak learners would than make a vote to decide the final decision by the strong learner.</p><p><strong>Boosting</strong></p><p>Boosting is a method to deal with high bias of model. The bias is reflected by the residual between <span class="math inline">\(y_{true}\)</span> and <span class="math inline">\(y_{pred}\)</span>. We can thus model on the residual again to get more accurate model. Boosting is a kind od serial method.</p><p>As a typical non-parameter learner, Decision is often applied as a basic learner in ensemble model.</p><h3 id="random-forest">3.1 Random Forest</h3><h4 id="training-process-1">3.1.1 Training Process</h4><p>A random forest is a <strong>bagging method</strong> in which every learner is a decision tree. The basic learner is a CART, but in most applications, the basic learner support Information Gain as a splitting criterea when doing classification.</p><p><img src="/2023/02/27/decision-tree/6.png"></p><p>Specifically, the train process of a RF should be like:</p><ol type="1"><li>Randomly take n sample set from the dataset with replacement, where n is the number of weak learner. Each sample set contain m samples</li><li>For each subset, conduct a column sampling, which take a subset k from the original feature space X</li><li>Train each weak learner with a sub training set with a size of <span class="math inline">\(m * k\)</span>.</li><li>For a test sample, run the test sample through each tree, then average each tree’s prediction.</li></ol><h4 id="pruning-strategy-2"><strong>3.1.2 Pruning</strong> Strategy</h4><p>Since the bagging method already realize a overfitting control, usually we won't apply pruning on an RF. We can still demand pruning, if we do want so, based on the pruning strategy of a single DT though.</p><h4 id="missing-value-processing-2"><strong>3.1.3 Missing Value Processing</strong></h4><p>Although weak learner already provides missing value processing strategy, it could be very time-consuming when applied to ensemble model with larger scale of estimators. Thus, most ensemble model frameworks do not provide missing value processing function in ensemble modeling.</p><p>However, if we do want to equip the RF model with a missing value imputation method, we can do the following stuff:</p><ol type="1"><li>impute all numerical missing value with sample average, all categorical missing value with sample mode</li><li>use the imputed sample set to calculate a similarity matrix among all samples, where the similarity between the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> samples is <span class="math inline">\(W_{i, j}\)</span></li><li>Back to the original sample set, use the similarity as a weight, fill a missing value <span class="math inline">\(X_{k,j}\)</span>, with the weighted average of all other sample: <span class="math inline">\(X_{k,j} = \frac{1}{n} \sum_{i\ne k}^n W_{i,k}X_{i,j}\)</span>. If <span class="math inline">\(X\)</span> is a categorical variable, use a weighted vote.</li></ol><p>This process is basically a data filling based on <strong>collaborative filtering</strong></p><h4 id="feature-importance"><strong>3.1.4 Feature Importance</strong></h4><p>Ensemble models usually have some embedded feature importance evaluation method. For the RF, there commonly are two methods:</p><p><strong>Mean Impurity Decrease / Error Decrease/ Information Gain</strong></p><p>For a feature <span class="math inline">\(X_i\)</span>, its importance on a non-leaf node m is: <span class="math display">\[FI(X_i,m) = GI_m -(GI_{left}+GI_{right})\]</span> Where GI is the gini index of the node, left and right represent the left and right child node of m.</p><p>Suppose in a DT, M is the collection of a node that select <span class="math inline">\(X_i\)</span> as a spliter, then the feature importance of <span class="math inline">\(X_i\)</span> on this tree is: <span class="math display">\[FI(X_i,DT) = \sum_{m \in M} FI(X_i,m)\]</span> Suppose there are n trees in the forest, the feature importance of <span class="math inline">\(X_i\)</span> would then be: <span class="math display">\[FI(X_i) = \sum_j^nFI(X_i,DT_j)\]</span> We can normalize the feature importance so its scale is 1: <span class="math display">\[\hat{FI(X_i)} = \frac{FI(X_i)}{\sum_i FI(X_i)}\]</span> Samely, we can replace the criterion with MSE or Imformation Gain and compute according feature importance</p><p><strong>OOB Error(Permutation)</strong></p><p>IN a RF, for a DT, there would exist some samples that never been selected by the bootstrap process. We call these samples the Out-of-bag data. We can use these data to evaluate the trained DT. Let <span class="math inline">\(E_j\)</span> denote the error rate (1-accuracy) of the <span class="math inline">\(j^th\)</span> DT using OOB data.</p><p>Now add a random noise to feaure <span class="math inline">\(X_i\)</span>, and use the OOB data with noise to evaluate the DT again. Denote the error as <span class="math inline">\(E_j&#39;\)</span></p><p>If a noise added to <span class="math inline">\(X_i\)</span> would lead to a great descent of accuracy of the DT, then <span class="math inline">\(X_i\)</span> is very important to that DT. Thus we define the feature importance of <span class="math inline">\(X_i\)</span> on the forest with n trees as: <span class="math display">\[FI(X_i) = \sum_j FI(X_i,DT_j) = \sum_j (E_j - E&#39;_j)\]</span></p><h3 id="gradient-boosting-decision-treegbdt">3.2 Gradient Boosting Decision Tree(GBDT)</h3><h4 id="training-process-2">3.2.1 Training Process</h4><p>In the GDBT, we define the residual as: <span class="math display">\[r = \frac{\partial L(y_{ture},M(X)) }{\partial M(X)}\]</span> where L is a loss function, M(X) is the output of a tree</p><p>Let <span class="math inline">\(X_0,Y_0\)</span> be the original inputs and outputs. According to the idea of boosting, the traning process of a GBDT is:</p><ol type="1"><li>Fit a model <span class="math inline">\(M0\)</span> to predict <span class="math inline">\(\hat{Y_0} = M_0(X)\)</span></li><li>Calculate the residual <span class="math inline">\(r_0 = \frac{\partial L(\hat{Y_0},Y_0)}{\partial\hat{Y_0}}\)</span></li><li>Fit a new model <span class="math inline">\(M1\)</span> using X as inputs and <span class="math inline">\(r_0\)</span> as outputs, get the prediction of residual <span class="math inline">\(\hat{r_0} = M_1(X )\)</span></li><li>update the prediction <span class="math inline">\(\hat{Y_1} = \hat{Y_0} + \alpha \hat{r_0}\)</span>, where <span class="math inline">\(\alpha\)</span> is the learning rate</li><li>use <span class="math inline">\(\hat{Y_1}, Y_0\)</span> to calculate <span class="math inline">\(r_1\)</span>, this time <span class="math inline">\(r_1 = \frac{\partial L(\hat{Y_1},Y_0)}{\partial\hat{r_0}}\)</span></li><li>Repeat process 3-5 until the residual is small enough</li></ol><p><strong>GDBT for Classification</strong></p><p>GDBT requires the residual to be continuous, this means the basic learner of GBDT are all Regression Tree. But GBDT can still do classification. The idea is the same as Generalized Linear Model like Logistic Regression. For the weak learner to predict <span class="math inline">\(\hat{r_i}\)</span>, the weak learner first estimates a canonical parameter <span class="math inline">\(\theta\)</span> for a Nature Exponential Family Distribution. Let <span class="math inline">\(\eta = M(X)\)</span> denote the estimation of <span class="math inline">\(\theta\)</span> given by the DT: <span class="math display">\[\hat{r_i} = AF(M_{i+1}(X)) = AF(\eta_{i+1})\]</span> where AF is the activation function. <span class="math display">\[\hat{Y_{i+1}} = \hat{Y_{i}}+\alpha\hat{r_i} = \hat{Y_{i}}+\alpha AF(\eta_{i+1})\]</span> Note thar the GDBT always <span class="math display">\[r_{i+1} = \frac{\partial L(\hat{Y_{i+1}},Y_0)}{\partial \eta_{i+1}}\]</span> The loss function in classification scenarios are usually <a href="http://zhengyuanyang.com/2022/09/22/loss-function/#log-losscross-entropy-loss">Log Loss</a>.</p><p>For details of GLM, refer to <a href="http://zhengyuanyang.com/2023/02/23/linear-regression/#generalized-linear-model">this article</a></p><p><strong>Subsampling</strong></p><p>In many framework, GDBT support sampling. Suppose we have 100 feature and the subsampling rate is 0.9, the every time a new weak learner is created, 90 sample is drawed randomly from the sample set. This could lead to a situation that for model <span class="math inline">\(M_{i+1}\)</span>, the output variable for some samples are <span class="math inline">\(r_i\)</span>, while other samples's output is <span class="math inline">\(r_{j &lt; i}\)</span>. Such a process would increase the bias but can reduce the variance of the model.</p><h4 id="pruning-strategy-3">3.2.2 Pruning Strategy</h4><p>Some GDBT support <strong>pre-pruning</strong>, that is, the impurity decrease or information gain must be less or greater than a threshold for the split the be generated. Samely, we can applied the post-pruning strategy the weak learner has originally.</p><p>However, in real application of boosting, we usually control overfitting by</p><ul><li>constrain the max depth of a single tree</li><li>set a shrinkage parameter to <span class="math inline">\(\alpha\)</span> to control the step size of learning</li><li>Apply subsampling</li></ul><h4 id="missing-value-processing-3">3.2.3 Missing Value Processing</h4><p>In GDBT, for time-saving reasons, the strategy is to:</p><ol type="1"><li>When training, calculate impurity or information gain without missing value sample</li><li>Try to put <strong>all the missing value samples</strong> respectively into the left and right split and calculate which would bring less loss(based on the loss function selected). Put the sample into that split and record the direction(left or right)</li><li>When predicting, suppose a sample is missing value <span class="math inline">\(X_i\)</span> and meet a node split on that feature. If there exists value missing on that node, then find the direction that missing values samples goes during the training process. If there are no missing value samples in the training, then the new sample goes to a default direction decided by the instance.</li></ol><h4 id="feature-importance-1">3.2.4 Feature Importance</h4><p>The GDBT can use same feature importance evaluation method like the RF. Since GDBT's basic estimator is Regression Tree, we usually just calculate the decrease of <strong>MSE</strong>. However, when doing classification, we can also consider directly use the real label Y to calculate the <strong>impurtiy</strong> of a node.</p><h3 id="xgboosting">3.3 XGBoosting</h3><h4 id="training-process-3">3.3.1 Training Process</h4><p><strong>Objective Function</strong></p><p>The XGboosting is a improved version of GDBT. The biggest difference is the way it define the residual. XGB use the second derivative Taylor Series to define the residual, that: <span class="math display">\[\begin{aligned}r_{i+ 1} &amp;= [L(y_0,\hat{y}_{i})+g_{i}\hat{r}_i+\frac{1}{2}h_{i}\hat{r}_i^2] + \Omega(T)\\g_{i} &amp;= -\frac{\partial L(y_0,\hat{y}_{i})}{\partial M_{i+1}(X)} \\h_{i} &amp;= -\frac{\partial^2 L(y_0,\hat{y}_{i})}{\partial M_{i+1}(X)^2 } \\\end{aligned}\]</span> where in regression <span class="math inline">\(\hat{r_i} = M_{i+1}(X)\)</span> and in classification <span class="math inline">\(\eta_ {i+ 1} = M_ {i+1}(X)\)</span>, and <span class="math inline">\(\Omega(T)\)</span> is a regularization term: <span class="math display">\[\Omega(T) = \gamma J + \frac{1}{2}\lambda \sum_{j=1}^J b_j^2\]</span> where J is number of leaf nodes under node T, <span class="math inline">\(b_j\)</span> is values on those leaf nodes, <span class="math inline">\(\gamma, \lambda\)</span> are hyper parameters control the degree of penalty.</p><p><strong>Splitting Strategy</strong></p><p>Unlike traditional CART, the XGB use a generalized gain deduced from the objective function.</p><p>Let <span class="math inline">\(R_ j\)</span> represent the collection of samples belong to the current node t. In equation 32, we know that the prediction of residual on a node t <span class="math inline">\(\hat{ r}_t\)</span> is actually decided by the samples, which is the average of samples beneath the t's subtree, which is the leaf values <span class="math inline">\(b_j\)</span>. If we express <span class="math inline">\(r_i\)</span> in <span class="math inline">\(\sum_j^ Jbj\)</span>: <span class="math display">\[r_t = [L(y_0,\hat{y}_{i})+G_{j}b_j+\frac{1}{2}(H_{j}+\lambda )b_j^2] + \gamma J\]</span> where <span class="math inline">\(G_j = \sum_{k \in R_j}g_k\)</span>, representing the sum of first derivatives of samples belong to this node. Same with <span class="math inline">\(H_j\)</span>.</p><p>When the samples belongs to the node <span class="math inline">\(R_ j\)</span> is given, in other word, a split has been made, the only unknown variable in this function is <span class="math inline">\(b_j\)</span>. Now we calculate the optimal value of <span class="math inline">\(b_j\)</span> using <span class="math inline">\(\frac{d r_{i+ 1}}{db_j} = 0\)</span>, and we obtain: <span class="math display">\[b^* = -\frac{G_j}{H_j+\lambda}\]</span> and bring the optimal back to objective function to get the best error function given a split: <span class="math display">\[obj_{R_j} =-\frac{1}{2}[\frac{ G_ j^ 2}{H_ j + \lambda}] + \gamma J]\]</span> In a splitting process, XGB need to compare the error of using a node as a leaf node or splitting it into 2 child nodes. In such scenario, <span class="math inline">\(J, J_L, J_R\)</span> all equal to 1. Thus we define the Gain of a split as: <span class="math display">\[\begin{aligned}Gain &amp; = Obj_{L+R} - [Obj_L + Obj_R ] \\&amp; = [-\frac{1}{2} \cdot \frac{ (G_L + G_R)^ 2}{H_L+H_R + \lambda} + \gamma] - [-\frac{1}{2} \cdot (\frac{G_L^ 2}{H_L + \lambda} + \frac{G_R^ 2}{H_R + \lambda})+2\gamma]  \\&amp; = \frac{1}{2}[\frac{G_L^ 2}{H_L + \lambda} + \frac{G_R^ 2}{H_R + \lambda} - \frac{ (G_L + G_R)^ 2}{H_L+H_R + \lambda}] + \gamma\end{aligned}\]</span></p><p>Gain is naturally an error decrease measure, it is quiet similar to information gain <span class="math inline">\(IG = H( D) - [H( D| left) + H(D|right) ]\)</span> . The only difference is it replace the entropy with a best error transformed from the objective function.</p><p>According to the definition, higher gain bring more error decrease. Thus, we can use gain as a criterion to decide the best split. The specific splitting strategy is just like the MSE in Regression Tree, the only difference is the the criterion.</p><p><img src="/2023/02/27/decision-tree/7.png"></p><p><strong>Other Improvement</strong></p><ol type="1"><li>XGB can do both row and column subsampling, which means it can drop some feature just like RF</li><li>XGB support linear regression model as basic estimator. (Dart is also supported)</li><li>XGB support customized loss function, as long as it is second derivative</li><li>XGB sort the samples respectively by each feature and save the ranking as blocks. This makes in best-split-find process can be done parallelly across different feature.</li></ol><h4 id="pruning-strategy-4">3.3.2 Pruning Strategy</h4><p>XGB support both pre-pruning and post-pruning. For pre-pruning, XGB can set a threshold for gain when splitting. For post-pruning, XGB can set a threshold, that if a subtree contains no child node that has a greater gain than this threshold, it will be pruned.</p><p>Like GDBT, more common overfitting control method in XGB is:</p><ul><li>Raw and column subsampling rate</li><li>shrinkage parameter $$</li><li>Structural hyper parameters like max_depth, min_leaf_sample_size</li></ul><h4 id="missing-value-processing-4">3.3.3 Missing Value Processing</h4><p>As mentioned above, XGBoosting support linear estimator. When the booster is gblinear, XGB can not deal with missing value. It would fill all missing value with 0.</p><p>When the booster is gbtree or dart, the XGB use same missing value processing strategy like GDBT.</p><h4 id="feature-importance-2">3.3.4 Feature Importance</h4><p><strong>Gain</strong></p><p>The Gain method is similar to the mean impurity decrease method in Random Forest. The different is, instead of impurity, the gain method use the gain to calculate the decrease.</p><p>Let T denote a collections of spliter using <span class="math inline">\(X_ i\)</span> in the <span class="math inline">\(j+1^{th}\)</span> tree model: <span class="math display">\[Gain(X_i, DT_{j+1}) = \sum_{t \in T} Gain(t)\]</span> suppose there are n DTs: <span class="math display">\[Gain(X_i) = \frac{ 1}{n} \sum _j^n Gain(X_i,DT_j)\]</span> We can than use this gain as a feature importance. We can also normalize the feature importance so its scale is 1: <span class="math display">\[\hat{Gain(X_i)} = \frac{Gain(X_i)}{\sum_i Gain(X_i)}\]</span></p><p><strong>Weight</strong></p><p>The frequency of <span class="math inline">\(X_i\)</span> being used as a spliter in the whole ensemble model. Note that when we make column subsampling, we should consider carefully whether to use weight as a feature importance, as sample important feature may just happened to be sampled only a few times.</p><p><strong>Cover</strong></p><p>For a feature <span class="math inline">\(X_i\)</span>, let <span class="math inline">\(R_t\)</span> denote the number of sample under a node t that split using <span class="math inline">\(X_i\)</span>. Let T denote a collection of all nodes in the ensemble models that uses <span class="math inline">\(X_i\)</span> as spliter, <span class="math inline">\(n_T\)</span> denote the length of T(how many times the feature is used). <span class="math display">\[cover = \frac{\sum_{t \in T}R_t}{n_T}\]</span> <strong>Difference of these 3 methods:</strong></p><ul><li>weight gives higher FI to numerical features.</li><li>Gain give higer FI to unique features. If we want to applied, drop features that is unique or close to unique like personal ID</li><li>cover gives higher FI to categorical features.</li></ul><h3 id="other-ensemble-models">3.4 Other Ensemble Models</h3><p><a href>ongoing</a></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Decisions Tree</tag>
      
      <tag>Ensemble Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linear Regression and Generalized Linear Model</title>
    <link href="/2023/02/23/linear-regression/"/>
    <url>/2023/02/23/linear-regression/</url>
    
    <content type="html"><![CDATA[<h1 id="linear-regression-and-generalized-linear-model">Linear Regression and Generalized Linear Model</h1><h2 id="linear-regression">1. Linear Regression</h2><h3 id="hypothesis-space-and-basic-assumption">1.1 Hypothesis Space and Basic Assumption</h3><p>The Linear Regression is a statistical model used to discover the correlation between an output variable Y and a set of input variable X. Basically, a linear regression model should follow such <strong>hypothesis space</strong>: <span class="math display">\[Y = \beta _1X+\beta_0 +\epsilon\]</span> where <span class="math inline">\(\beta_1\)</span> is a vector consists of parameters for each feature. <span class="math inline">\(\beta_0\)</span> is a constant, <span class="math inline">\(\epsilon\)</span> represents the random error of the observation.</p><p>To modeling the data with a linear regression model, five assumption should be fulfilled:</p><p><strong>1. Linearity</strong></p><p>Linearity Assumption requires there to has a linear mapping relationship between Y and X. That is, the real mapping relationship should be consistent with the hypothesis space. Otherwise, there would be bais in the model.</p><p><strong>2. <span class="math inline">\(\epsilon\)</span> Independency</strong></p><p>The linear model assume the <span class="math inline">\(\epsilon\)</span> is a completely randomized error. That is, a sample's <span class="math inline">\(\epsilon\)</span> shoud not be correlated with other samples. If this assumption cannot be satisfied, we say there exists autocorrelation in the data. Autocorrelation would make the model underestimate the randomized error and increase bias. It usually happened when there exists time-sequence influence among the samples.</p><p><strong>3. X Independency</strong></p><p>The features should be independent to each other, otherwise we said there exists multicollinearity. Multicollinearity would increase the variance of the model and cause overfitting</p><p><strong>4.<span class="math inline">\(\epsilon\)</span> Homoskedasticity</strong></p><p>The variance of <span class="math inline">\(\epsilon\)</span> should be same on different scale of Y. Otherwise, we said there is a Heteroskedasticity. Heteroskedasticity.increase the variance of the model and make the learned parameter unstable.</p><p><strong>5. <span class="math inline">\(\epsilon\)</span> Normality</strong></p><p><span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span> . If the randomzied error are not normally distributed, the data points with high error would make the learned parameter unstable and increase the variances of the model. Notice that when <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span>, and <span class="math inline">\(Y = \beta _1X+\beta_0 +\epsilon\)</span>, we would have <span class="math inline">\(Y \sim N(\beta _1X+\beta_0,\sigma^2)\)</span>. This means (Y|X) should follows a normal distribution. Notice that this is not equivalent to <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>. We do not need to do normality test on Y before the learning.</p><p>The detection techniques and solutions for the violation of the above assumptions can be found in <a href="http://zhengyuanyang.com/2022/09/22/evaluation-for-regression/">this article</a></p><p>Linear Regression is:</p><ul><li>A supervised learning model</li><li>A parameter learning model</li><li>A non-probability learning model</li><li>A discriminative model</li></ul><h3 id="training-process-of-linear-regression">1.2 Training Process of Linear Regression</h3><p>With the hypothesis space given:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">Initialize the weight<br>for each sample:<br>calculate output<br>Calculate overall error based on loss function<br>while the error &lt; threshold:  #or current error &gt; previous error <br>for each sample and error:<br>update w_new = w_old + a*w&#x27; #with a optimization algorithm<br>return to step2<br><br></code></pre></td></tr></table></figure><p><strong>Optimizer</strong></p><p>In the training process, the weight is updated n times where n is the sample size. <span class="math inline">\(\alpha\)</span> is the a hyper-parameter called learning rate. It control the step size of optimal searching process.</p><p>For regression models, most optimizers are based on gradient descending. In the pseudocode above, <span class="math inline">\(\omega&#39;\)</span> represents the partial derivatives of the parameters: <span class="math display">\[\omega&#39; = \frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial Y}\cdot\frac{\partial Y}{\partial \theta}\]</span> For more on optimizer for linear regression, refer to <a href>this article</a></p><p><strong>Loss function:</strong></p><p>Common loss functions for linear regression includes MSE, MAE and RMSE. For more on loss functions, refer to<a href="http://zhengyuanyang.com/2022/09/22/loss-function/#numerical-output">this article</a></p><h2 id="regularization-lasso-regression-and-ridge-regression">2. Regularization: Lasso Regression and Ridge Regression</h2><p>A common problem in machine learning is the overfitting. For the details of overfitting, refer to <a href="http://zhengyuanyang.com/2022/09/19/overfitting/">this article</a></p><p>To solve overfitting, we can apply the structural Risk Minimization. We would add a penalty term regarding to the complexity of the model to the loss function. For details, refer to <a href="http://zhengyuanyang.com/2022/09/12/Principle/#structural-risk-minimization">this article</a></p><p>The application of SRM on linear regression create Lasso and Ridge Regression. The Lasso and Ridge Regression have same training process like linear regression, but it add a regularization term of <span class="math inline">\(\omega\)</span> in the loss function to penalize overfitting:</p><p><strong>Lasso Regression</strong> <span class="math display">\[J = MSE + \lambda ||\omega||_1\]</span> <strong>Ridge Regression</strong> <span class="math display">\[J = MSE + \lambda ||\omega||_2^2\]</span> where <span class="math inline">\(\lambda\)</span> is a penalty parameter decides the extent of penalty. The regularization term is a L-P norm of the parameter vector. It is a generalized distance concept. Refer to the <a href="http://zhengyuanyang.com/2022/10/08/distance/#minkowski-distance">Minkowski Distance</a></p><p>The basic function of Lasso Regression and Ridge Regression are both minimizing the parameters(complexity), but they have the following differences:</p><ul><li>The Lasso regression can reduce <span class="math inline">\(\omega_i\)</span> to 0, thus can be used for feature selection.</li><li>The Ridge mostly only reduce the parameter close to 0, thus is not very suitable for feature selection. We can still try it when the features are scaled though.</li></ul><h2 id="logistic-regression">3. Logistic Regression</h2><h3 id="basic-assumption">3.1 Basic Assumption</h3><p>Logistics Regression is a model for classification. Unlike normal linear model, which assume the output is continuous and normally distributed, the logistic regression assume the output Y follows a Bernoulli Distribution.</p><p>The hypothesis space if logistic regression:</p><p>Let Z denote the linear transformation contributed by the a linear regression model: <span class="math display">\[Z = \beta _1X+\beta_0 +\epsilon\]</span> Use an activation function sigmoid to convert such a transformation into a probability <span class="math inline">\(P(Y|X)\)</span>. For activation functions, refer to <a href>this article</a>. The decision boundary is thus: <span class="math display">\[P(Y=1|X=x) = sigmoid(Z)\]</span> Decide the classification of Y by <span class="math display">\[y_{pred}  = argmax \{P(Y=1|X=x) , P(Y=0|X=x)\}\]</span> The Logistic Regression model is a discriminative probably model, as it directly modeling on <span class="math inline">\(P(X|Y)\)</span></p><h3 id="training">3.2 Training</h3><p><strong>Loss Function</strong></p><p>The common loss function for logistic regression is <strong>Log Loss</strong>. Refer to <a href="http://zhengyuanyang.com/2022/09/22/loss-function/#log-losscross-entropy-loss">this article</a> for details.</p><p><strong>Optimizer</strong></p><p>The optimizer for logistic regression is the same as the linear regression. Notice that now: <span class="math display">\[\omega&#39; = \frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial A}\cdot\frac{\partial A}{\partial Z}\cdot \frac{\partial Z}{\partial \theta}\]</span></p><h2 id="generalized-linear-model">4. Generalized Linear Model</h2><p>From the previous section we can find that the most important different between linear regression and logistic regression is that the output variable Y follows different distribution type. In fact, linear regression and logistic regression both belongs to Generalized Linear Model(GLM)</p><h3 id="exponential-family-distribution">4.1 Exponential Family Distribution</h3><p>Exponential Family Distribution refer to those distributions taht can be reformed as a exponential format: <span class="math display">\[p(x|\lambda)  = \frac{1}{Z(\lambda)}h(x)exp\{ \phi(\lambda)^TT(X)\}\]</span> where:</p><ul><li><span class="math inline">\(\lambda\)</span> is the original parameter of the distribution, it can be a constant or a vector(e.g. <span class="math inline">\(N(\lambda_1 = \mu, \lambda_2 = \sigma^2)\)</span>)</li><li>h(x) is a function called basic measure</li><li>T(X) is a function called sufficient statistic</li><li><span class="math inline">\(\phi(\lambda)\)</span> is a function of <span class="math inline">\(\lambda\)</span>, we can can let <span class="math inline">\(\theta = \phi(\lambda)\)</span>. In such form, we call <span class="math inline">\(\theta\)</span> the <strong>canonical parameter</strong>, it can be a constanr or a vector, with same size of <span class="math inline">\(\lambda\)</span></li><li><span class="math inline">\(Z(\lambda)\)</span> is the partition function of this probability <span class="math inline">\(Z(\lambda) = ln\int h(x)exp\{ \phi(\lambda)^TT(X)\}\)</span></li></ul><p>By moving <span class="math inline">\(h(x), Z(\lambda)\)</span> in to the exponential, let <span class="math inline">\(\theta = \phi(\lambda)\)</span>, we can obtain that all Exponential Family Distribution can be expressed in such format: <span class="math display">\[p(x|\theta) = \{ \theta^T T(x) + S(x) - A(\theta)\}\]</span> In this form, we <span class="math inline">\(A(\theta)\)</span> the cumulant function</p><p>For example, the PDF of a Bernoulli function: <span class="math display">\[p(x|p) = \lambda^x(1-\lambda)^{1-x} = exp\{ln[\lambda^x(1-\lambda)^{1-x}]\} = exp\{xln(\frac{\lambda}{1-\lambda})+ln(1-\lambda)\}\]</span> where:</p><ul><li>T(x) = x</li><li><span class="math inline">\(\theta\)</span> = <span class="math inline">\(ln(\frac{\lambda}{1-\lambda})\)</span></li><li><span class="math inline">\(A(\theta) = -ln(1-\lambda) = ln(1+e^\theta)\)</span></li><li><span class="math inline">\(S(x) = 0\)</span></li></ul><p>Common Exponential Family Distribution includes gaussian distribution, exponential distribution, gamma distribution, bernoulli distribution, binomial distribution, multinomial distribution, poisson distribution and so on.</p><p><strong>Natural EFD</strong></p><p>The Natural EFD is a subset of the EFD. If T(x) = x, we call this distribution a natural EFD. For example, Gaussian Distribution, Bernoulli Distribution, Exponential Distribution, Poisson Distribution, Gamma Distribution and Multinomial Distribution are all natural EFD.</p><p><strong>Statistics of EFD</strong></p><p>One properties of Exponential Family Distribution is that: <span class="math display">\[\frac{dA}{d\theta} = \frac{d}{d\theta}\{ln \int h(x) exp\{\theta^TT(x)\}\} = E[T(x)]\]</span> When T(x) is given, we can calculate E[x] according to the properties of Expectation. For natural EFD, which T(x) = x, <span class="math inline">\(E(T(x)) = E[x]\)</span>. Use the Bernoulli Distribution as an example: <span class="math display">\[\frac{dA}{d\theta} = \frac{d }{d\theta}ln(1+e^\theta) = \frac{1}{1+e^{-\theta}} = p\]</span> Samely, we can deduce that <span class="math inline">\(\frac{d^2A}{d\theta^2} = Var[T(x)]\)</span></p><p>This indicate a very important attribute of EFD: the canonical parameter <span class="math inline">\(\theta\)</span> has a mapping relationship with the moments(<span class="math inline">\(\mu, \sigma^2, skewness,...\)</span>) of the distribution. We know that the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> of a EFD is decided by its original parameter <span class="math inline">\(\lambda\)</span>, and we know <span class="math inline">\(\theta = \phi (\lambda)\)</span>, so there exists are reversible function <span class="math inline">\(\theta = \psi(\mu)\)</span>(if the size of <span class="math inline">\(\theta\)</span> is 1)</p><p><strong>Canonical Form of EFD</strong></p><p>Some EFD has 2 original parameters and thus has two canonical parameters <span class="math inline">\(\theta = [\theta_1,\theta_2]\)</span>.</p><p>For example, the cumulant function of a gaussian distribution is: <span class="math display">\[A(\theta) = -\frac{\theta_1^2}{4 \theta_2} - \frac{1}{2}ln(-2\theta_2)\]</span> In such cases, some time we would find <span class="math inline">\(\mu,\sigma^2\)</span> are tangled with <span class="math inline">\(\theta_1, \theta_2\)</span> that <span class="math inline">\([\theta_1,\theta_2]= f(\mu,\sigma^2)\)</span>. This make it difficult to calculate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. Thus, we would reconstruct the nature EFD as: <span class="math display">\[p(y|\theta_\mu) = exp \{ \frac{y\theta_\mu - b(\theta_\mu)}{a(\theta_v)} + c(y,\theta_v)\}\]</span> Both <span class="math inline">\(\theta_\mu\)</span> and <span class="math inline">\(\theta_v\)</span> are a function of <span class="math inline">\((\theta_1,\theta_2)\)</span>, and we would need to ensure: <span class="math display">\[A(\theta) = \frac{b(\theta_{\mu})}{a(\theta_v)}\]</span> Also, we would ensure <span class="math inline">\(\theta_{\mu}\)</span> would only be a function decided by <span class="math inline">\(\mu\)</span>. <span class="math inline">\(\theta_{mu} = \psi(\mu)\)</span> is <strong>stilled called canonical paraparameter</strong> and it control the location of the EFD. The <span class="math inline">\(\theta_v\)</span> is called a <strong>dispersion parameter</strong>, it is basically a scaler to control the shape of the EFD and is associated with the variance of the distribution. The <span class="math inline">\(a(\theta_v)\)</span> is called a dispersion function. For EFD that only has one original parameter, which means the distribution does not has a <span class="math inline">\(\theta_2\)</span>, the dispersion function is simply <span class="math inline">\(a(\theta_v) = 1\)</span>, as the distribution does not has a second parameters to control shape.</p><p>Now we can revise the calculation of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\mu = \frac{\partial A}{\partial \theta_\mu} = b&#39;(\theta_\mu)\]</span></p><p><span class="math display">\[\sigma^2 = \frac{\partial^2 A}{\partial \theta_\mu^2} = a(\theta_v)b&#39;&#39;(\theta_\mu)\]</span></p><h3 id="generalized-linear-model-1">4.2 Generalized Linear Model</h3><p>A Generalized Linear Model is a model used to predict output Y with input X. Thus, a good idea is to estimate <span class="math inline">\(E[Y|X]\)</span>. For example, the prediction of a linear regression is <span class="math inline">\(E[Y|X]\)</span>, the prediction of a logistic regression is <span class="math inline">\(E[Y|X] = 0*P(Y=0|X) + 1*P(Y=1|X) = P(Y=1|X)\)</span>, and the prediction muti-category regression(softmax regression) is <span class="math inline">\(E[\vec{Y}|X,\vec{\pi}=\vec{p}] = [P(Y=0|X),P(Y=1|X),P(Y=k|X)]\)</span>.</p><p>Now we nake the following assumption:</p><ol type="1"><li>The variable (Y|X) follows a natural EFD. Usually this can be guarantee by knowing Y follows a EFD.</li><li>The canonical parameter of the EFD <span class="math inline">\(\theta_\mu\)</span>, in a Bayesian context, follows a normal distribution, and thus can be learned from a linear hypothesis space</li></ol><p>To achieve the expectation, a GLM consists of:</p><ul><li>A linear predictor <span class="math inline">\(\eta = \beta X+b\)</span></li><li>A function that can transfer the linear prediction <span class="math inline">\(\eta\)</span> to the expectation <span class="math inline">\(\mu\)</span> we want. <span class="math inline">\(\mu = g^{-1}(\eta)\)</span></li></ul><p>We know that Y does not necessary follows a normal distribution and can be difficult to learned from with a linear hypothesis space. Thus we add a non-linear function to enhance the non-linearity of the hypothesis space. This function is called <strong>Activation Function</strong>, and the inverse of it <span class="math inline">\(\eta = g( \mu)\)</span> is called a joing function.</p><p><img src="/2023/02/23/linear-regression/2.png"></p><p>We know from the assumption that <span class="math inline">\(\theta\)</span> is normally distributed and can be learned directly from the linear predictor. Thus, a good idea is to let <span class="math inline">\(g = \psi^{-1}\)</span>, so we have: <span class="math display">\[\eta = g(\mu) = \psi^{-1}(\mu) = \theta_\mu\]</span> In this case, the <span class="math inline">\(\eta\)</span> we are learning is exactly <span class="math inline">\(\theta_\mu\)</span>, which can be learned with a linear hypothesis space.</p><p>Hence, we we want to build a GLM to predict a variable <span class="math inline">\(Y|X \sim natural \ EFD(\theta)\)</span></p><ol type="1"><li>Figure out the distribution type of Y, and rewrite it into a canonical form of EFD</li><li>Read <span class="math inline">\(\theta_\mu = \phi(\mu), b(\theta_\mu)\)</span> out of the format and write down <span class="math inline">\(\mu = \psi(\theta_\mu)\)</span></li><li>Select the activation function as <span class="math inline">\(\psi^{-1}(\eta)\)</span></li><li>Train the model</li></ol><p>The GLM enable us to generalize linear regression to many other cases, as long as Y|X follows a EFD. Common application includes Logistic Regression(Bernoulli Distribution), Softmax Regression(multi-category distribution), Polynomial Regression and so on. When Y|X follows a gaussian distribution, the activation function is just <span class="math inline">\(\mu = \eta\)</span>, and the model is reduced to a simply linear regression, which Y can be directly learned from a linear hypothesis space.</p><p>Some common joint functions and and activation functions are listed as follows:</p><p><img src="/2023/02/23/linear-regression/3.png"></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linear Regression</tag>
      
      <tag>Generalized Linear Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A/B Testing: Experiment Sensitivity Improvement</title>
    <link href="/2023/02/18/sensitivity-improvement/"/>
    <url>/2023/02/18/sensitivity-improvement/</url>
    
    <content type="html"><![CDATA[<h1 id="ab-testing-experiment-sensitivity-improvement">A/B Testing: Experiment Sensitivity Improvement</h1><h2 id="about-sensitivity-improvement">1. About Sensitivity Improvement</h2><p>If you expected there to be an significance effect but the results turn out to be insignificant, there's a chance that the sensitivity of the experiment is not big enough. In other word, given current sample size, your experiment can not perform enough statistical power as your demand. The sensitivity is usually measured by the Minimum Detectable E Effect(MDE).</p><p>Notice that when MDE is smaller than the PSB, even if we further improve the sensitivity by reduce mde, the mean of difference on metrics would still be lower than the PSB(any valid significant difference <span class="math inline">\(\mu\)</span> should has <span class="math inline">\(\mu &lt; mde\)</span>, while <span class="math inline">\(mde &lt; psb\)</span>). In such cases, the results have no chance to be practically significant. In other word, <strong>we only need improve sensitivity when:</strong> <span class="math display">\[MDE &gt; PSB\]</span> Recall the formula of MDE: <span class="math display">\[MDE = (t_{\frac{1-\alpha}{2}} + t_ {1-\beta})\sqrt{\frac{\sigma_1^2}{n_ 1} + \frac{\sigma_2^2}{n_ 2}}\]</span> The formula suggests that if we want to maintain the desired statistical power, there are only two way to improve sensitivity:</p><p><strong>More Sample</strong></p><p>To collect more sample, we can:</p><ul><li>bring in more data by acquiring additional units</li><li>increase test duration</li><li>randomized on finer grain to increase sample size</li></ul><p><strong>Less Varaince</strong></p><p>According to the equation of <span class="math inline">\(n&#39;, \delta, \beta\)</span>, we know that if the variance of the population is lower, we would need less sample to detect identical effect, given desired fixed statistical power, that is, the real-time statistical power would get higher while the sampe size remains the same.</p><p>In real application this can be difficult to implement extra sample acquisition. Thus, for this article, we focusing on techniques that can lowering the variance to improve experiment Sensitivity</p><h2 id="variance-estimation">2. Variance Estimation</h2><p>Before lowering variance, first we need to make sure the variance of the parameters is estimated from the sample correctly. If the variance is incorrectly estimated, the p value and the confidence interval we calculated are both wrong.</p><p><strong>When Randomized Unit <span class="math inline">\(\ne\)</span> Analysis Unit</strong></p><p>As discussed in <a href="http://zhengyuanyang.com/2022/03/25/Hypothesis-Testing/#zt-test">this article</a>, when estimating the standard error, we would usually assume the variable for the results of single units are i.i.d. However, such assumption are not always fulfilled. For example, there might exist network effects. We need to expel these effects before variance estimation</p><p>For some metrics like CTR, the unit is a pageview instead of a user. As multiple view can came from the same user <span class="math inline">\(B_1,B_2,..B_n\)</span> may also be correlated. In thus case, we need to rewrite the metrics by aggregating it into a user level: <span class="math display">\[V[CTR] = V[\frac{user \ avg \ click }{user\ avg \ PV}] = V[\frac{\bar{X}}{\bar{Y}}]\]</span></p><p><strong>When Using Relevant Difference(<span class="math inline">\(\Delta \%\)</span>)</strong></p><p>Sometimes we would define the PSV using a relevant percentage instead of absolute value, like a 1% growth on GMV. In such case, we might want to judge the practical significance based on the confidence interval of a relevant of of the different between treatment and conrol group, let Y denote the measure on a single unit, <span class="math inline">\(\tilde{Y}\)</span> denote the paramter to test on (mean or ratio), <span class="math inline">\(\Delta\)</span> denote the difference of parameter between treatment and control group <span class="math display">\[\Delta\% = \frac{\Delta}{\tilde{Y_C}} = \frac{\tilde{Y_T}-\tilde{Y_C}}{\tilde{Y_C}}\]</span> Thus, suppose <span class="math inline">\(\tilde{Y}\)</span> is a mean parameter, when calculating the variance of <span class="math inline">\(\Delta\)</span>: <span class="math display">\[V[ \Delta] = V[\tilde{Y_T}-\tilde{Y_C}] = V[\tilde{Y_T}] + V[\tilde{Y_c}] = \frac{V[Y_T]}{n_1} + \frac{V[Y_C]}{n_2}\]</span> However, when calculating the variance of <span class="math inline">\(\Delta\%\)</span>: <span class="math display">\[V[\Delta \%] = V[\frac{\tilde{Y_T}-\tilde{Y_C}}{\tilde{Y_C}}] = V[\frac{\tilde{Y_T}}{\tilde{Y_C}}]\]</span> According to CTL, <span class="math inline">\(\tilde{Y_T},\tilde{Y_C }\)</span> both follow normal distribution. Thus, the variance of <span class="math inline">\(\frac{\tilde{Y_T}}{\tilde{Y_C}}\)</span> can be calculated from: <span class="math display">\[V[\Delta \%] =  \frac{1}{\tilde{Y_C}^2}V[ \tilde{Y_T}] + \frac{\tilde{Y_T}^2}{\tilde{Y_C}^4} V[\tilde{Y_C}] + 2\frac{\tilde{Y_T}}{\tilde{Y_C}^3} Cov[\tilde{Y_T},\tilde{Y_C}]\]</span> In a RCL, since treatment are randomly assigned, <span class="math inline">\(\tilde{Y_T}\)</span> and <span class="math inline">\(\tilde{Y_C}\)</span> are independent, thus <span class="math inline">\(Cov[\tilde{Y_T},\tilde{Y_C}] = 0\)</span>. Hence: <span class="math display">\[V[\Delta \%] =  \frac{1}{\tilde{Y_C}^2}V[ \tilde{Y_T}] + \frac{\tilde{Y_T}^2}{\tilde{Y_C}^4} V[\tilde{Y_C}]\]</span></p><h2 id="metrics-manipulation">3. Metrics Manipulation</h2><p>We can lower the variance by replace or transform the metrics</p><p><strong>Metrics Replacement</strong></p><p>Creating metrics with a smaller variance while capturing similar information. For example, the number of searches has a higher variance than the number of searchers; GMV has higher variance than order amount. The selection of lower-variance metrics should be validated in a real business environment.</p><p><strong>Metrics Transformation</strong></p><p>Transform the metrics so that it has lower variance. Common transformation includes discretization, log tranformation, and power transformation.</p><h2 id="construct-unbiased-estimator">4. Construct Unbiased Estimator</h2><h3 id="stratification">4.1 Stratification</h3><p>In stratification, you divide the sampling region into strata, sample within each stratum separately, and then combine results from individual strata for the overall estimate by weighted sum, which usually has smaller variance than estimating without stratification.</p><p>Let <span class="math inline">\(Y\)</span> be the value of the metrics(mean or ratio): <span class="math display">\[V[Y] = V[Y]_{within}+V[Y]_{between} = \sum_kp_k\sigma^2_k + \sum_kp_k(\bar{y}_k-\bar{y})^2\]</span> where <span class="math inline">\(p_k\)</span> is the weight of the strata</p><p>Let <span class="math inline">\(Y_k\)</span> be the within variance of the <span class="math inline">\(k^{th}\)</span> strata <span class="math display">\[V[\hat{Y}] = \sum_kp_kV[Y_k] = V[Y_{within}] &lt; V[Y]\]</span></p><p>The stratification eliminate the between-group variance, thus can reduce the overall variance. However, the covariate, which is the strata, must be discrete.</p><p><strong>Post-Stratification</strong></p><p>While stratification is most commonly conducted during the sampling phase (at runtime), it is usually expensive to implement at large scale. Therefore, most applications use post-stratification, which applies stratification retrospectively during the analysis phase.</p><p>For post-stratification, we still conducted a randomized sampling to decided treatment group and control group. When analyzing the results, we put the units from treatment group and control group into buckets according to their strata variable with a ratio of 1:1. This saves the costs of doing multiple randomization, but it may waste a small portion of samples.</p><h3 id="regression-covariant-control">4.2 Regression Covariant Control</h3><p>The covariate control method finds a continuous covariate that correlated with metric. The core idea is to make a regression relationship between the original metrics and its covariates. The covariates explain part of information of the original metric and contribute part of its variance. Thus, by taking out the part contributed by the covariate, the overall variance is lowered.</p><p>To some extent, the basic ideal of stratification and covariate control is the same. The difference is that covariate control method use regression to interpret the relationship between Y and its covariate.</p><p>Let X be the covariate. X should be constructed like the way Y does(mean or ratio). Construct new variable <span class="math inline">\(\hat{Y} = Y -\theta X\)</span>, if <span class="math inline">\(E[X_T] = E[X_c]\)</span>, then <span class="math inline">\(E[\hat{Y_T}-\hat{Y_C}] = E[Y_T-Y_c]\)</span>, the new variable is a unbiased estimation of the original metrics. It can be proven that <span class="math inline">\(V[\hat{Y}] &lt; V[Y]\)</span> if <span class="math inline">\(Cov[X,Y] &gt; 0\)</span>.</p><p><strong>Choosing Covariate</strong></p><p>One thing to notice when choosing the covariate is that the assumption <span class="math inline">\(E[X_T] = E[X_c]\)</span> should always be satisfied. Therefore, covariate should have no causal relationship with the experiment. If <span class="math inline">\(X \to T\)</span>, then the covariate is a confounder, then the covariate have different distribution on the treatment and control group,<span class="math inline">\(E[X_T] \ne E[X_C]\)</span>. On the other side, if <span class="math inline">\(T \to X\)</span>, then the X would be effected by the treatment, <span class="math inline">\(E[X_T] \ne E[X_C]\)</span> after the experiment is conducted. Either way, the estimation is biased. Thus, when choosing covariates:</p><ul><li>The covariates should be pre-experiment metrics or invariant metrics that won't be affected by the experiment</li><li>The covariates should be independent to the assigning of treatment and control group</li></ul><p>A common application it to apply the same metric before the experiment as the covariate, as the pre-experiment metric is usually highly correlated with post-experiment metric. This method is usually called <strong>Controlled-experiment Using Pre-Experiment Data(CUPED)</strong></p><p>Nevertheless, we can still used other covariate. Some thesis point out we can find such covariates through machine learning methods. These methods includes CUPAC(Control Using Predictions As Covariates) and MLRATE(machine learning regression-adjusted treatment effect estimator)</p><p><strong>Controlled-experiment Using Pre-Experiment Data(CUPED)</strong></p><p>Let X denots the value of the metric we want to test before the experiment, Y denots the value after the experiment. For an A/B test, we expect that <span class="math inline">\(E[X_T] = E[X_C], E[Y_T] \ne E[Y_C]\)</span>. Now construct two variables: <span class="math inline">\(\hat{Y_T} = Y_T - \theta X_T\)</span>, $  = Y_C - X_C$, where <span class="math inline">\(\theta\)</span> is a hyperparameter we can adjust. We know that: <span class="math display">\[E[ \hat{Y_T}-\hat{Y_C}] = E[(Y_T - \theta X_T) - (Y_C - \theta X_C)] = E[Y_T]-E[Y_C]\]</span> thus, estimate the treatment effect on CUPED variable is equivalent to estimate the effect on original metrics. Meanwhile: <span class="math display">\[V[\hat{Y_T}-\hat{Y_C}] =V [(Y_T - \theta X_T) - (Y_C - \theta X_C)] = V[Y_T- Y_C] +\theta^2V[X_T-X_C]-2\theta Cov[(Y_T-Y_C),(X_T-X_C)]\]</span> we can than regard <span class="math inline">\(V[\hat{Y_T}-\hat{Y_C}]\)</span> as a quadratic function of <span class="math inline">\(\theta\)</span>, where: <span class="math display">\[\Delta = 4Cov[(Y_T-Y_C),(X_T-X_C)]^2 - 4V[(Y_T - Y_ C)]V[X_T-X_C]\]</span> as we know: <span class="math display">\[Cov[X_1,X_2] \le \sqrt{V[X_1]V[X_2]}\]</span> we have that <span class="math inline">\(\Delta \ge 0\)</span>, thus: <span class="math display">\[V[\hat{Y_T}-\hat{Y_C}] \in [0,V[Y_T- Y_C]] \qquad \theta \ge 0\]</span> we have minimum <span class="math inline">\(V[\hat{Y_T}-\hat{Y_C}]\)</span> when <span class="math inline">\(\theta =\frac{Cov[(Y_T-Y_C),(X_T-X_C)]}{V[X_T-X_C]}\)</span></p><h3 id="variance-weighted-estimators">4.3 Variance-Weighted Estimators</h3><p><a href="ongoing"></a></p><p>see the thesis:</p><p>Variance-Weighted Estimators to Improve Sensitivity in Online Experiments</p><p>KEVIN LIOU, Facebook SEAN J. TAYLOR, Lyft</p>]]></content>
    
    
    <categories>
      
      <category>A/B Testing &amp; Case Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>A/B Testing</tag>
      
      <tag>Sensitivity Improvement</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Probability Density Estimation</title>
    <link href="/2023/02/13/prob-dense-estimation/"/>
    <url>/2023/02/13/prob-dense-estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="probability-density-estimation">Probability Density Estimation</h1><h2 id="about-probability-density-estimation">1. About Probability Density Estimation</h2><p>Probability Density Estimation(PDE) is a techniques used for estimating probability distribution of a variable through samples. Specifically, it can be categorized as:</p><ul><li>Parameter Estimation: Assuming the variable follows a type of distribution and estimating the parameter of that distribution through samples</li><li>Non-parameter Estimation: Trying to obtain the distribution of the variables directly from the data without any distribution type assumptions</li></ul><p>For PDE, we usually assume that all samples are i.i.d</p><h2 id="parameter-estimation">2. Parameter Estimation</h2><h3 id="maximum-likelihood-estimation">2.1 Maximum Likelihood Estimation</h3><p>Consider the Bayesian's Law: <span class="math display">\[P(X=x|Y=y) = \frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}\]</span> One <strong>critical</strong> thing to remember is the way to interpret the parameter <span class="math inline">\(\theta\)</span>.</p><p>According to Bayesian, the parameter <span class="math inline">\(\theta\)</span> itself sollows some kinds of distribution. Thus, the likelihood and posterior distribution in bayesian's law is exchangeable, <span class="math inline">\(\theta\)</span> is nothing but a random variable just like X and Y in equation 1. In such scenarios, likelihood and probability is the same thing <span class="math display">\[P(\theta|E) = \frac{P(E|\theta)P(\theta)}{P(E)}\]</span> where E represents the evidence variable.</p><p>However, according to frequentist, <span class="math inline">\(\theta\)</span> is a unknown constant values, and it does not have a distribution. Thus, there's <span class="math inline">\(\theta\)</span> cannot be put into the Bayesian's Law like a random variable. In this scenario, likelihood and probability is not one thing. Probability is for random variable, while likelihood is for parameter, which is a constant values. According to frequentist, given the i.i.d. assumption: <span class="math display">\[L(\theta| D) = P(D|\theta) = \prod_iP(d_i=x_i|\theta)\]</span> where D represents the results of all samples.</p><p>The MLE is a product of the frequentist. Thus, in MLE, to find best <span class="math inline">\(\theta\)</span>, we just need to find the <span class="math inline">\(\hat{\theta}\)</span> that maximize the likelihood function. For calculation, do average log operation on both sides: <span class="math display">\[\iota(\theta|D) = \frac{1}{n}lnL(\theta| D)\]</span></p><p><span class="math display">\[\hat{\theta} = argmax \ \iota(\theta|D)\]</span></p><p>The best parameter <span class="math inline">\(\theta\)</span> can thus be obtained calculating the zero point of the derivation function of the <span class="math inline">\(\iota(\theta|D)\)</span></p><h3 id="bayesian-estimation">2.2 Bayesian Estimation</h3><p>The Bayesian is the other side of view on probability. According to format (2): <span class="math display">\[P(\theta|E) = \frac{P(E|\theta)P(\theta)}{P(E)} = \frac{P(E|\Theta = \theta)P(\Theta = \theta)}{\sum_i P(E|\Theta =\theta_i)P(\Theta=\theta_i)}\]</span> In this equation:</p><ul><li><span class="math inline">\(P(\theta)\)</span> follows the prior distribution, which is decided by prior knowledge regardless of the sample information</li><li><span class="math inline">\(P(E|\Theta=\theta_i)\)</span> is the likelihood function decide by the knowledge of the mechanism between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(E\)</span> regardless of sample information</li></ul><p>The bayesian estimation is a process of updating the prior knowledge through sample information. The updated posterior distribution can be the new prior distribution. In real application, the accurate understanding of the likelihood function can be difficult, besides, the calculation can also be quiet difficult.</p><h3 id="maximum-a-posterior-estimation">2.3 Maximum A Posterior Estimation</h3><p>The Maximum A Posterior Estimation is an approximation of the bayesian estimation. it assume the marginal distribution <span class="math inline">\(P(E)\)</span> is independent of the parameter distribution <span class="math inline">\(P(\theta)\)</span>. It calculate the best <span class="math inline">\(\theta\)</span> by maximize the posterior probability <span class="math inline">\(P(\theta|E)\)</span> <span class="math display">\[\hat{\theta} = argmax \ f(D|\theta)g(\theta) = \prod_iP(d_i=x_i|\theta)P(\theta)\]</span> Such assumption make the calculation of the posterior probability much more easier. The MAP allows to add the prior distribution of <span class="math inline">\(\theta\)</span> into the estimation. It is a simpler version of Bayesian Estimation.</p><h2 id="non-parameter-estimation">3. Non-parameter Estimation</h2><h3 id="histogram">3.1 Histogram</h3><h3 id="kernel-density-estimation">3.2 Kernel Density Estimation</h3><h3 id="parzen-windows-k_n-neighbors">3.2 Parzen Windows &amp; <span class="math inline">\(K_n\)</span> Neighbors</h3>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Parameter Estimation</tag>
      
      <tag>Probability Density Estimation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Analytics: Models and Framework</title>
    <link href="/2023/02/05/analytics-framework/"/>
    <url>/2023/02/05/analytics-framework/</url>
    
    <content type="html"><![CDATA[<h1 id="analytics-models-and-framework">Analytics: Models and Framework</h1><p>Data analysts build models to perform various analytics jobs. Each type of models are designed to figure out a certain problem. For specification, in this article, we call the general categories of these models as <strong>Analytics Model</strong> and specific example of the model as <strong>Analysis Framework</strong>.</p><h2 id="user-model">1. User Model</h2><h2 id="event-model">2. Event Model</h2><h2 id="funnel-analysis-model">3. Funnel Analysis Model</h2><h3 id="aarrr">3.1 AARRR</h3><p>The AARRR is a framework for product analysis. It consists of 5 ordinal objectives:</p><ol type="1"><li>Acquisition: Let user be aware of the product<br></li><li>Activation: Let user do what we want them to do</li><li>Retention: Keep user active, engaged and exhibiting sticky behavior</li><li>Revenue: generate businesses outcome. Usually money but can vary from the business model</li><li>Referral: Let user spread the product</li></ol><h3 id="sean-elliss-startup-growth-pyramid">3.2 Sean Ellis’s Startup growth Pyramid</h3><p>The Startup growth Pyramid is a framework for starting up and enlarge business. It segment the business lifecycle into threee stages and discuss what we should do in each stage:</p><p><img src="/2023/02/05/analytics-framework/1.png"></p><p>The judgement on moving from the current stage to the next is based on user satisfaction. If 40% of people (or more) say they’d be very disappointed to lose the service, you’ve found a fit, and now it’s time to scale</p><h3 id="other-framework-and-comparison-table">3.3 Other Framework and Comparison Table</h3><p>Other Frameworks include Lean Analytics Stage, Maurya Lean Canvas and Lean Startup</p><p>The comparison of them are listed below</p><p><img src="/2023/02/05/analytics-framework/2.png"></p><h2 id="heatmap-model">4. HeatMap Model</h2><h2 id="customized-retention-analysis">5. Customized Retention Analysis</h2><h2 id="stickiness-analysis">6. Stickiness Analysis</h2><h2 id="full-behavior-route-analysis">7. Full Behavior Route Analysis</h2><h2 id="user-segmentation">8. User Segmentation</h2>]]></content>
    
    
    <categories>
      
      <category>A/B Testing &amp; Case Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Analytics Model</tag>
      
      <tag>Analysis Framework</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Case Interview: Measure Success</title>
    <link href="/2023/01/31/measure-success/"/>
    <url>/2023/01/31/measure-success/</url>
    
    <content type="html"><![CDATA[<h1 id="case-interview-measure-success">Case Interview: Measure Success</h1><h2 id="analytics-framework">1. Analytics Framework</h2><p>For metrics selection, the funnel anaysis model is the most frequently applied type of frameworks. In this case, we commonly applied the AARRR framework to allocate proper metrics. For details on models and frameworks, refer to <a href>ongoing</a></p><p>The according metrics of each objectives under various business models are listed below:</p><table><thead><tr class="header"><th>Business Model</th><th>Acquisition</th><th>Activation</th><th>retention</th><th>Revenue</th><th>referral</th></tr></thead><tbody><tr class="odd"><td>E-commerce</td><td></td><td></td><td></td><td></td><td></td></tr><tr class="even"><td>SaaS</td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td>Free Mobile APP</td><td></td><td></td><td></td><td></td><td></td></tr><tr class="even"><td>Media Site</td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td>UGC</td><td></td><td></td><td></td><td></td><td></td></tr><tr class="even"><td>TS Marketplace</td><td></td><td></td><td></td><td></td><td></td></tr><tr class="odd"><td>For all</td><td>DAU/MAU, DNU by platform</td><td></td><td></td><td></td><td></td></tr></tbody></table><h2 id="criterion-for-metrcis-selection">2. Criterion for Metrcis Selection</h2><h2 id="procedure-of-measure-success-problem">3. Procedure of Measure Success Problem</h2><p>This type of cases usually come as "find a metric to measure the extent.....(how good a new feature/strategy/product is)". IT evaluates one's ability to design a measure according to certain business objectives</p><ol type="1"><li><p><strong>Clarify the objective</strong>:</p><p>Ask questions about:</p><ul><li>How this feature works?</li><li>who is it for</li></ul></li><li><p><strong>Confirm the business objective</strong></p><p>according to the explanation from the interviewer, conclude the objective of the feature to the following aspects:</p><ul><li>Acquisition: bring in more user, help them discover the product</li><li>Activation: make user taking actions we want them to, keep users engaged</li><li>conversion and retention: keep user continuing to use the product, convert them from free-user to paid user</li><li>Money: make revenue from users</li></ul></li><li><p><strong>Select metrics according to objective</strong></p><table><thead><tr class="header"><th>Objective</th><th></th></tr></thead><tbody><tr class="odd"><td>Acquisition</td><td>DAU/MAU, DNU by platform,</td></tr><tr class="even"><td>Activation</td><td>ACU/PCU, PV/UV, time spending, interaction behaviour</td></tr><tr class="odd"><td>Conversion and Retention</td><td>CTR,CO, pay rate, 7 day retention rate/churn rate</td></tr><tr class="even"><td>Money</td><td>ROI,GMV, AOV, ACV, ARPU/ARPPU</td></tr><tr class="odd"><td></td><td></td></tr></tbody></table><ul><li><p>Select 1 main success metric, this metric should be directly relevant to the business objective</p></li><li><p>Select 1 second success metrics, this metrics should be a supplement. It improve the completeness of the logic relationship of the key metrics and business goal. To fing the second goal, consider the following questions:</p><ul><li>Can the change of key metrics be caused by factors other than the feature?</li><li>Is the key metric equivalent to the business goal? Is there any intermediate process</li><li>If the key metrics changes on a group level, should we have same changes on single user level?</li></ul><p>Alternatively, we can do a funnel analysis to find the secondary success metrics</p></li><li><p>Select 1 guardrail metric, this metric is critical to the main goal of the whole product. The feature should not have significant negative effect on it</p></li></ul></li><li><p><strong>structure the answer</strong></p><p>if the guardrail metric shift towards a negative direction, the feature is probably not success.</p><p>Otherwise, if the key success metric and the second both shift towards a positive direction, the feature is successful.</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>A/B Testing &amp; Case Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Measure Success</tag>
      
      <tag>Case</tag>
      
      <tag>Metric Selection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A/B Testing: Results Analysis</title>
    <link href="/2023/01/24/results-analysis/"/>
    <url>/2023/01/24/results-analysis/</url>
    
    <content type="html"><![CDATA[<h1 id="ab-testing-results-analysis">A/B Testing: Results Analysis</h1><h2 id="sanity-check">1. Sanity Check</h2><p>After we obtained the results data and before we make any analysis, we want to do some sanity check to ensure the experiment are running correctly and any differences are caused by the treatment instead of any other changes. A Sanity check refer to the simple process to verify the resutls by checking the consistency of those metrcis that are not supposed to be affected by the treatment(Invariant Metrics). There are two types of invariant metrics:</p><ol type="1"><li>Trust-related guardrail metrics: Metrics related to the business process but should not be effected by the treatment or should be configured the same for both group(e.g search amount for a watch-time duration test, cache hit rate, Telemetry fidelity)</li><li>Organiztional guradrail metrics: metrics that are important to the organization and expected to be an invariant for many experiments.(e. g. latency, GMV, PV)</li></ol><p>If these sanity checks fail, there is probably a problem with the underlying experiment design, infrastructure, or data processing.</p><h2 id="judgment-on-significance">2. Judgment on Significance</h2><h3 id="judgement-on-statistical-significance">2.1 Judgement on Statistical Significance</h3><p>The judgement on statistical significance should be based on p value calculated from the test results.</p><p><img src="/2023/01/24/results-analysis/1.png"></p><p>Note that the MDE is the real-time sensitivity estimated from the experiment results</p><h3 id="judgement-on-practical-significance">2.2 Judgement on Practical Significance</h3><p><strong>Confidence Interval</strong></p><p>From a frequentist probability theory POV, the confidence interval is a section describe how often the true value of a parameter would lie in the section given a significance level. In a testing we regard the observed difference as an estimation of the mean of the true difference, following a normal distribution. We can then calculated the CI through: <span class="math display">\[CI = [\mu- Z_{\frac{\alpha}{2}}*\sigma_{SE},\  \mu+ Z_{\frac{\alpha}{2}}*\sigma_{SE}]\]</span> where:</p><ul><li><span class="math inline">\(\mu\)</span> is the observed difference</li><li><span class="math inline">\(\sigma_{SE}\)</span> is the standard error estimated from the standard deviation from the sample</li></ul><p><strong>Practical Significance</strong></p><p>When checking on CI, the following situations may occur:</p><p><img src="/2023/01/24/results-analysis/2.png"></p><ol type="1"><li>The result is not statistically significant. It is also clear that there is no practical significance. This suggests that the change does not do much. You may either decide to iterate or abandon this idea.</li><li>The result is statistically and practically significant, which suggests lauching the feature.</li><li>The result is statistically significant but not practically significant. In this case, you are confident about the magnitude of change, but that magnitude may not be sufficient to outweigh other factors such as cost and thrive you business goal.</li><li>The boundry of the CI does exceed the PSB but it happens in both way. This ethier suggest the treatment effect is very unstable or the experiment is underpowered. Consider running a follow-up test with more units, providing greater statistical power</li><li>The result is likely practically significant but not statistically significant.There is a good chance that there is no impact at all. From a measurement perspective, the best recommendation would be to repeat this test but with greater power to gain more precision in the result.</li><li>The result is statistically significant, and likely practically significant. There is still a chance that the impact is not sufficient from a business perspective. While we still should consider repeating the test with more power, choosing to launch is a reasonable decision from a launch/no-launch decision</li></ol><p>From the analysis above we found only scenarios 2 and 6 could be considered launchable. Whether we should stick to scenario 2 depends on our preference. In most business practice we would presue scenario 2.</p><h3 id="judge-of-significance-in-multiple-testing-scenarios">2.3 Judge of Significance in Multiple Testing Scenarios</h3><h4 id="multiple-testing-scenarios">2.3.1 Multiple Testing Scenarios</h4><p><strong>Multiple Variants</strong></p><p>There are more then two variants. For example, suppose we want to examine the imporvement on CTR by changing the color of a button, but we have three alternative treatments. In this case we creat three tests. For each test, let <span class="math inline">\(\alpha = 0.05\)</span>, then: <span class="math display">\[P(no\ false  \ positive  \ in \ all \ three \ tests) = (1-\alpha)^3 = 0.857\]</span> The type I error of the <strong>overall experiment hypothesis</strong>, which is "changing color improves CTR" is high.</p><p><strong>Multiple Metrics</strong></p><p>Suppose we have only one 1 treatment but 100 metrics to test. Set <span class="math inline">\(\alpha = 0.05\)</span>, same as the multiple variants scenario, around 5 metrics would fall into the pitfall of type I error, which means they will be false positive, and this could mislead.</p><h4 id="corrections-of-significance-level-under-multiple-testing">2.3.2 Corrections of Significance Level under Multiple Testing</h4><p><strong>Bonferroni Correction</strong></p><p>Bonferroni Correction is a simple way to lower type I error by having smaller <span class="math inline">\(\alpha\)</span> for each test: <span class="math display">\[\alpha_i  = \frac{\alpha}{n}\]</span> where n is the number of tests(number of variants, number of metrics, or the product of them)</p><p>The bonferroni correction is easy to apply, but it is also to conservative. It increase the difficulty to reject the null hypothesis in each test.</p><p><strong>Control False Discovery Rate</strong></p><p>Define FDR as: <span class="math display">\[FDR  = E[\frac{n_{FP}}{n_{rej}}]\]</span> FDR tells in all tests that we judge as significant, how many of them are false positive. For example, suppose we have 1000 testing and reject 200 of them, and the FDR = 0.05, then we can expect around 10 metrics to be false positive. If we take this results as acceptable, we can move on to analysis. Otherwise, we should consider lower <span class="math inline">\(\alpha\)</span></p><p>In real application, the judgement of false positive can be difficult to detect. Thus, controlling PDR can be sometimes unaccessible.</p><p><strong>Benjaminiand Hochberg's Method</strong></p><p>Suppose we have n tests, each has a p value as a result. First, we can rank the list in a ascending order by P values. Then we correct the p value with: <span class="math display">\[q_i = p_i * \frac{n}{k}\]</span> Where k is the ranking of the <span class="math inline">\(i^{th}\)</span> test</p><table><thead><tr class="header"><th>T</th><th>P value</th><th>Q</th></tr></thead><tbody><tr class="odd"><td>T2</td><td>0.001</td><td>0.005</td></tr><tr class="even"><td>T5</td><td>0.003</td><td>0.0075</td></tr><tr class="odd"><td>T1</td><td>0.12</td><td>0.4</td></tr><tr class="even"><td>T3</td><td>0.045</td><td>0.54</td></tr><tr class="odd"><td>T4</td><td>0.048</td><td>0.048</td></tr></tbody></table><p>we then comepare the Q value with <span class="math inline">\(\alpha\)</span> to judge the significance.</p><p>This process can lower the overall type I error. However, we may also notice that through this method, alternatives with greater p value can pass while alternatives with smaller p values can fail, which sometimes may mislead.</p><p><strong>Two-step Rule-of-thumb</strong></p><p>Another method when dealing with multiple metrics is to segment the metrics in serval tiers, such as:</p><ul><li>First-order metrics: those you expect to be impacted by the experiment</li><li>Second-order metrics: those potentially to be impacted</li><li>Third-order metrics: those unlikely to be impacted</li></ul><p>and apply tiered significance levels to each group (e.g., 0.05, 0.01 and 0.001 respectively).</p><h2 id="validity-checktruthworthyness-analysis">3. Validity Check(Truthworthyness Analysis)</h2><p>The Validity Check is a process is a procedure to find out if the results are truthworthy. It is usually an analysis to rule out some potential effects that could threat the validity. Such analysis can be decomposed on two dimensions:</p><p><strong>Internal Threats/External Treats</strong>:</p><ul><li>internal: Making the testing results itself is incorrect without attempting to generalize the results to other population or time periods</li><li>external: Making the conclusion incorrect when trying to generalize it to other population or time periods</li></ul><p><strong>Overestimate/Underestimate</strong></p><ul><li>Overestimate: The testing results is significant but the real effect or generalized effect is not so</li><li>Underestimate: The testing results is insignificant but the real effect or generalized effect is not so</li></ul><h3 id="threats-to-internal-validity">3.1 Threats to Internal Validity</h3><h4 id="violation-of-sutva">3.1.1 Violation of SUTVA</h4><p>A random controlled experiment requires Stable Unit Treatment Value Assumption (SUTVA). It states that experiment units (e.g., users) do not interfere with one another. In some cases, this assumption could be violated</p><p><strong>Network Effect</strong></p><p>A feature might spillover to other user through the social network of treatment unit. This usually undermine the difference between control and treatment group. If the treatment is applied to all users, the real effect can be bigger. To deal with this kind of effects, we can implemented social network clustering and select randomized unit for control and treatment group in different clusters. There is also a method called ego-network randomization</p><p><strong>Two-sided marketplaces:</strong> For two-sided platform like auctions platform or Uber. The treatment on the unit might affect other unit through the "other-side". For example, lowering prices for riders in Treatment group make them more attractive to drivers, and there would be less drivers available for control group. In this case, the GMV difference between two groups are overestimated. The real differences between when the treatment applied to all users might be lower. To deal with this kind of effects, we can make geo-based or time-based isolation on treatment and control group</p><p><strong>Shared Resources:</strong> If the Treatment leaks memory and causes processes to slow down due to garbage collection and possibly swapping of resources to disk, all variants suffer.This could lead to underestimating of real effect. For example, the Treatment crashed the machine in certain scenarios. Those crashes also took down users who were in Control, so the delta on key metrics was not different —both populations suffered similarly.</p><h4 id="violation-of-intention-to-treatment">3.1.2 Violation of Intention-to-Treatment</h4><p>In an online experiment, the treatment group shoud be those whose treatment are "delivered" rather than "fulfilled". Our measuring is based on the offer, or intention to treatment, not whether it was actually applied. Analyzing only those who participate, results in selection bias and commonly overstates the Treatment effect. For example, if our treatment is "adding an subscribe button" and our target is to increase revenue, then our treatment group should be those who are demonstrated with the button instead of those who actually click the button. We can add a second success metric "CTR" to track the effectiveness of the button though.</p><h4 id="sample-ratio-mismatch"><strong>3.1.3 Sample Ratio Mismatch</strong></h4><p>If the ratio of users (or any randomization unit) between the variants is not close to the designed ratio, the experiment suffers from a Sample Ratio Mismatch (SRM). A SRM indicates the decision of exposing a unit to the treatment is not independent to treatment effect. This could lead to either overestimate or underestimate.</p><p>For example, the ratio designed for treatment and controlled group is one-to-one. Suppose the decision is randomly obtained through a coin flip, then the expected sample ratio should be 0.5. We can regard the sample ratio as a parameter and test it if through t distribution. If the p value is samll enough, we can make the claim that there's some problem with the coin that lead to a sample ratio mismatch.</p><p>The cause of the SMR, in most cases, is for the developer to worry. Debugging SRMs requires the developer to examine their randomization strategy during experiment configuration.</p><h3 id="threats-to-external-validity">3.2 Threats to External Validity</h3><h4 id="primacy-and-novelty-effect">3.2.1 Primacy and Novelty Effect</h4><p><strong>Primacy Effect:</strong> When a change is introduced, users may need time to adopt, as they are <em>primed</em> in the old feature, that is, used to the way it works. In other word, users are reluctant to changes. This effect would diminish over time, but if we made the conclusions while the primacy effect still exists, than we may underestimate the real treatment effect</p><p><strong>Novelty Effect:</strong> Being opposite to the primacy effect, the novelty effect refer to that a new feature might be attractive for the users at first, but not so interesting after the users try them for the first time. This effect would diminish over time, but if we made the conclusions while the novelty effect still exists, than we may overestimate the real treatment effect</p><p>Solutions for these two effects:</p><ul><li>ensure the experiment run long enough so that these two effects vanish over time</li><li>rule out these effects by constraining the target group in only first-day user</li><li>compare the CATE of first- time and existing user to see if the effects do exist</li></ul><p><img src="/2023/01/24/results-analysis/3.png"></p><h2 id="segmentation-difference-outlier">4. Segmentation Difference &amp; Outlier</h2><h3 id="segmentation-analysis">4.1 Segmentation Analysis</h3><p>Analyzing a metric by different segments can provide interesting insights and lead to discoveries. Things we can do about segmentation includes:</p><p><strong>Segmental View on Metrics</strong>: Inspecting the segmentation metrics values before and after the experiment. If the metrics are distinctly different for a group, then this group may have a strong difference comparing to other groups. We can dive in to the problem and make debugging or take this group out of the target group</p><p><strong>Segmental View on Effect</strong>: Inspecting the CATE of the segmentations. For any strongly distinct CATE, we can doubt that the treatment takes effect with a special mechanism on this particular group. We can dive in to the problem and make debugging or take this group out of the target group</p><p><strong>Migration Among Segmentations</strong> :</p><p>Suppose we want to examine the improvement of CTR of users brought by a treatment. We find the CTR of treatment and control group are respectively 25 v.s. 20 for Chinese users, and 15 v.s. 10 for non-Chinese users .Can we know make the conclusion that the treatment is effective?</p><p>The answer is no. We need to first ensure there are no unit migrations among segmentation. If the assignment of the feature make some chinese decide to change their language options and turn them to non-Chinese users. The overall treatment effect may go down</p><p>This indicates that analysis on segmentations can mislead due to migration. The segmentation should be done based on the status of units before the experiment. Otherwise, migrations among groups can happen</p><h3 id="outlier-influence">4.2 Outlier Influence</h3><p>The existence of outliers can drastically change the conclusion regarding significance.</p><p><img src="/2023/01/24/results-analysis/4.png"></p><p>Thus, it is necessary to eliminate the outliers in the samples before making conclusions. For example, a user who has 10000 pageviews is probability a robot. For outlier detection techniques, refer to <a href="http://zhengyuanyang.com/2022/10/07/outlier/">this article</a>.</p><h2 id="improvement-of-experiment-sensitivity">5. Improvement of Experiment Sensitivity</h2><h3 id="sensitivity-improvement">5.1 Sensitivity Improvement</h3><p>For sensitivity improvement methods, refer to <a href="http://zhengyuanyang.com/2023/02/18/sensitivity-improvement/">this article</a></p>]]></content>
    
    
    <categories>
      
      <category>A/B Testing &amp; Case Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>A/B Testing</tag>
      
      <tag>Results Analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Probability Graphical Model</title>
    <link href="/2023/01/10/PGM/"/>
    <url>/2023/01/10/PGM/</url>
    
    <content type="html"><![CDATA[<h1 id="probability-graphical-model-basis">Probability Graphical Model: Basis</h1><h2 id="about-probability-graphical-model">1. About Probability Graphical Model</h2><p>The Probability Graphical Models are a series model that express the dependency/ joint distribution in a graphic way(edges and nodes). The apllications of PGM includes:</p><ul><li>PGM representation: present the correlation or causality relationship among variables in a graphical way</li><li>PGM learning: including parameter learning and structural learning</li><li>PGM inference: reasoning the correlation/causality structure of variables</li></ul><h2 id="terminology">2. Terminology</h2><p><strong>Factor</strong></p><p>A factor is a funtion of one or more variables <span class="math display">\[\phi(X_1,..X_n)\]</span> where <span class="math inline">\(\phi: Val(X_1,...X_n) \to R\)</span></p><p><strong>Scope</strong></p><p>The list of variables included in a factor: <span class="math inline">\(\{ X_1, ...X_n\}\)</span></p><p><strong>Factor Product</strong></p><p><img src="/2023/01/10/PGM/1.png"></p><p><strong>Factor Marginalization</strong></p><p><img src="/2023/01/10/PGM/2.png"></p><p><strong>Factor Reduction(Conditioning)</strong></p><p><img src="/2023/01/10/PGM/3.png"></p><h2 id="categories-of-pgm">3. Categories of PGM</h2><h3 id="bayesian-network">2.1 Bayesian Network</h3><p>The Bayesian Network use directed acyclic graph to represent the causally of random variables.</p><p>Each node <span class="math inline">\(X_i\)</span> has a contional probability distribution(CPD) <span class="math inline">\(P(X_i|Par(X_i))\)</span></p><p>where par returns all parent nodes of <span class="math inline">\(Xi\)</span> in the graph. If <span class="math inline">\(X_i\)</span>does not has parent nodes, then replace the CPD with <span class="math inline">\(P(X_i)\)</span></p><p>A BN represent the JPD of all variables included, which can be calculated from: <span class="math display">\[P(X_1,X_2,...X_n) = \prod_i^n P(X_i|par(X_i))\]</span> We can say a joint probability P factorize over a graph G, if P can be represented by the above equation.</p><p>The Naive Bayes Model is an example of the simplistic BN. In naive bayes, assumptions are made that each feature are caused by a categorical variables, while the features remain independent to each other</p><p><img src="/2023/01/10/PGM/4.png"></p><h3 id="markov-random-field">2.2 Markov Random Field</h3><h4 id="markov-chain-and-transition-probability-matrix">2.2.1 Markov Chain and <strong>Transition Probability Matrix</strong></h4><p><strong>Markov Chain</strong></p><p>For a series of random variable <span class="math inline">\(X_1,X_2,..X_n\)</span> we call this series a markov chain if: <span class="math display">\[P(X_m+1 = i_m+1| X_0=i_0,X_1=i_1,...X_m=i_m)  = P(X_m+1 = i_m+1|X_m=i_m)\]</span> In other word, the state of <span class="math inline">\(X_m+1\)</span> depends only on the previous one state, which is the value of <span class="math inline">\(X_m\)</span></p><p>In many applications, we can use a markov chain to model the transition of states over time for a single random variable.</p><p><strong>Transition Matrix</strong></p><p>For any moment n, if the probability <span class="math inline">\(P(X_{n+1} = j|X_n =i)\)</span> is independent to n, then we can this probability the one-step transition probability between state i and j, denoted as <span class="math inline">\(p_{i,j}\)</span></p><p>For all possible states of the random variable, the one-step probabilities between each two combinations consist the transition matrix. The sum of each row would be 1. Note that the sum of each column does not necessarily have to be 1.</p><h4 id="random-field-and-clique">2.2.2 Random Field and Clique</h4><p>In a random field, each position(node) represent a random variable and can be assigned with a value.</p><p>In a graph, a clique is a subset of nodes that each two nodes in this subset are connected</p><p>If adding any nodes to a clique would no longer make that subset a clique, we call sucn a clique a maximal clique</p><h4 id="markov-property-of-random-field">2.2.3 Markov property of Random Field</h4><p><strong>Global Markov Assumption</strong></p><p>For any two subsets of nodes S and T(S and T are not overlapped), given a separating subset C(a subset of nodes that all paths between S and T need to travel through this subset), if S and T are conditionally independent on C(<span class="math inline">\(P(S|T,C) = P(S| C)\)</span>)</p><p><strong>Local Markov Assumption</strong></p><p>Let V be any nodes in the graph, W be the adjacent nodes of V, O be other nodes in the graph. Given random variables <span class="math inline">\(X_W\)</span>, <span class="math inline">\(X_V\)</span> and <span class="math inline">\(X_O\)</span> are conditonally independent.</p><p><strong>Pairwise Markov Assumption</strong></p><p>Let U and V be two disconnected nodes in a graph, O be other nodes. Given random variables <span class="math inline">\(X_O\)</span>, <span class="math inline">\(X_U\)</span> and <span class="math inline">\(X_V\)</span> are conditionally independent</p><p>If any of the three assumption are satisfied in a random field, we call such a random field a Markov Random Field(MRF)</p><p>The Global Markov Assumption is the strongest one, the satisfaction of it implies the other two assumption</p><h4 id="potential-function-of-mrf">2.2.4 Potential Function of MRF</h4><p><strong>Potential Function</strong></p><p>The potential function is defined on a clique in a MRF. It measure the correlation among the variables in the clique. the potenial function should be a non-negative function, and it should have greater value where one feels two variable correlated. For example, we can define a potential function that have maximum when to factors are equal.</p><p>Usually, we would define the potential function as a exponential function. <span class="math display">\[\phi_Q(X_Q) = e^{-H_Q(X_Q)}\]</span> Where <span class="math inline">\(H_Q\)</span> is called an energy function. The reason to adopt an exponential potential is that we can add the energy functions together when we multiply a potential function with another.</p><p>The usually format of a energy function: <span class="math display">\[H_Q(X_Q) = \sum_{u,v\in Q,u \ne v} \alpha_{u,v}t(x_u,x_v) + \sum_{v \in Q}\beta_{u,v}s(x_v)\]</span> where:</p><ul><li><span class="math inline">\(\alpha, \beta\)</span> are hyper parameters</li><li>t, s are functions to measure the energy between two nodes and asingle nodes. The could be just <span class="math inline">\(t(x_u,x_v) = x_ux_v, \ s(x_v)= x_v\)</span></li></ul><p><strong>Joint Probability</strong></p><p>The joint probability a MRF represent can be calculated from a series of product of potential functions: <span class="math display">\[P(X_1,X_2,...X_n) = \frac{1}{Z} \prod_{Q \in C} \phi_Q(X_Q)\]</span> Where Z is a scaling constant to ensure the probability is legal. In real application, the calculation of Z is very difficult because the number of cliques can be huge. We can instead calculate P based on maximal cliques: <span class="math display">\[P(X_1,X_2,...X_n) = \frac{1}{Z^*} \prod_{Q \in C^*} \phi_Q(X_Q)\]</span> where <span class="math inline">\(C^*\)</span> is the collections of all maximal clique in the MRF or subset of MRF</p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Probability Graph Models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Case Interview: Metrics Diagnosis</title>
    <link href="/2022/12/25/case-metrics-diagnosis/"/>
    <url>/2022/12/25/case-metrics-diagnosis/</url>
    
    <content type="html"><![CDATA[<h1 id="case-interview-metrics-diagnosis">Case Interview: Metrics Diagnosis</h1><h2 id="procedure-of-metrics-shift-diagnosis">1. Procedure of Metrics Shift Diagnosis</h2><p>This type of cases usually come as a important metrics shift(usually towards a negative direction), and the objective the problem is <strong>to find the root cause of the shift and try to fix it</strong></p><p>Framework:</p><ol type="1"><li><p><strong>Clarify the definition of the metric:</strong></p><p>how the metrics is defined. This help you decompose the metrics correctly. This is useful for metric decomposition</p></li><li><p><strong>Check Data:</strong></p><p>Check the log of the data source and data pipeline to make sure the shift is not caused by any bugs, logical errors or outliers</p></li><li><p><strong>Check external factors</strong>:</p><p>Figure out whether the shift is caused by any external factors, including:</p><ul><li><strong>Seasonality:</strong> Is there a seasonal factor in the lifecycle of the product? Like there would be a drastic decline in certain weeks or months of every year. Conduct year-on-year comparison to see such decline always appear in a particular stage in a year</li><li><strong>Industry/market trend</strong>: If there exist a descending trend in the industry? Look to the data of the metrics for the previous few weeks or month. Also see this trend happens on other products or competitors or industry. Figure out the scope of the trend</li><li><strong>Competitors</strong>: If the competitors are doing a sales campaign? Does the loss of the metrics goes to the competitors?</li><li><strong>Special events:</strong> Is there a change of policy? Is there any social events or phenomenon happening that it might have a potential impact on the metrics?</li></ul></li><li><p><strong>Check Internal facotrs</strong>:</p><ol type="1"><li><p><strong>Change or treatment on product:</strong></p><p>is there any change or treatment implemented on the product around the time of the metric's decline? if so, we can do a causal inference to evaluate the effect</p></li><li><p><strong>Decompose the metrics</strong>:</p><p>decompose the metrics in a additive or multiplicative way, for example: order amont = show pv * CTR * C_O, ETA = waiting time for a drive to take the order + time the driver arrive the pick up point + time the rider find the driver</p><p>For each sub-metrics, calculate the changing rate and find out which submetric is the most influential part of the original metrics, focus on these sub- metrics</p></li><li><p><strong>Segment the metric by user demographic channel, or user behavior</strong></p></li></ol><p>Segment the metric by dimensions like region, new/old user, platform, content category. For each subgroup, calculate its contributions ot the decline of the metrics. For example, if you find some facts like 90% of the decline are caused by new user, than you probably would suspect there's a association, like the gui is not friendly to new user</p></li></ol><h2 id="fermi-estimation">2. Fermi Estimation</h2><p>Fermi estimation is a method of making rough estimates of unknown quantities using approximate values and simple reasoning. The basic idea is to decompose the metric we want to estimate into some submetrics in a additive or multiplicative way. For example, to answer to question: how many red vehicles are there in New York?</p><p>We can make such decomposition:</p><p>Red vichle in New York = Population in New York * Vichle ownership rate * City factor(large city usually has higher ownership rate) * the ratio of red car in all cars</p><p>Then, for each metirc, we given them a upper boundary and a lower boundary according to our common sense. We use the median of the range as an estimation and use them to calculate the estimation for the target metric.</p>]]></content>
    
    
    <categories>
      
      <category>A/B Testing &amp; Case Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Case Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Bayesian Model in Machine Learning</title>
    <link href="/2022/12/13/bayesian-model/"/>
    <url>/2022/12/13/bayesian-model/</url>
    
    <content type="html"><![CDATA[<h1 id="bayesian-model-in-machine-learning">Bayesian Model in Machine Learning</h1><h2 id="naive-bayesian">1. Naive Bayesian</h2><h2 id="maximum-entropy-model">2. Maximum Entropy Model</h2><h2 id="bayesian-network">3. Bayesian Network</h2>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Bayesian Model</tag>
      
      <tag>Learning Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sampling Methods for Machine Learning</title>
    <link href="/2022/12/05/sampling/"/>
    <url>/2022/12/05/sampling/</url>
    
    <content type="html"><![CDATA[<h1 id="sampling-and-simulation-for-machine-learning">Sampling and Simulation for Machine Learning</h1><h2 id="about-sampling-and-simulation">1. About Sampling and Simulation</h2><p>The word "Sampling" can be used in different context in ML. From a statistical perspective, Sampling is a process that collect a subset of units from the real sample population. The objective of sampling is to describe characteristics of sampling through statistics on the sample.</p><p>From a computational perspective, however, the word sampling means "Given a probability distribution, generate scenarios to represent the distribution", which means "Sampling", in a computer context, actually means "Simulation". To make a difference, in this article, sampling for a statistical context is called "Sampling", sampling for a computational context is called "simulation".</p><p><strong>For Sampling</strong></p><p>There are various sampling method that have each different application. Sampling methods can be categorized as:</p><ol type="1"><li><strong>Probability Sampling</strong>: Every Single unit in the population has a chance to be selected as a sample point</li><li><strong>Non- Probability Sampling</strong>: Some units in the population does not have a chance to be selected as a sample point</li></ol><p>From another dimension sampling methods can be categorized as:</p><ol type="1"><li><strong>Without Replacement </strong>: when a unit is selected, it would be taken out from the sample population</li><li><strong>With Replacement</strong>: when a unit is selected, it would not be taken out from the sample population</li></ol><p>This article focuses on sampling methods that has application in machine learning.</p><h2 id="sampling-method">2. Sampling Method</h2><h3 id="simple-random-sampling"><strong>2.1 Simple Random Sampling</strong></h3><p>In a random sampling, each unit in the sample population has a equal probability to be selected.</p><p><strong>Pros</strong>:</p><ul><li>Easy to apply</li></ul><p><strong>Cons</strong></p><ul><li>If the selection strategy is by any way associated with statistics through a confounder, a <strong>simpson paradox</strong> would be created</li><li>If the researcher has any interests on the group differences among partition of the sample frame, SRS is not accommodate to it</li></ul><h3 id="systematic-sampling">2.2 Systematic Sampling</h3><p>Systematic Sampling arranges all units in the population into some order, and then selecting elements at regular intervals through that ordered list.</p><p><strong>Pros</strong>:</p><ul><li>It help avoid selection bias. The sample point are uniformly distributed on the section sorted by a particular order</li></ul><p><strong>Cons</strong></p><ul><li>If there exists periodicity, and inner period of the order list is a multiple or factor of the interval we selected, the sample would be unrepresentative</li></ul><h3 id="stratified-sampling">2.3 Stratified Sampling</h3><p>Stratified Sampling separates units into some "Strata" according to some categorical characteristics. The percentage of these strata is called a <strong>sampling fraction</strong>. A subsection of the sample is then assigned to each strata so that the proportion of each category of sample points is still the sample as sampling fraction. In each strata, sample are selected through SRS or systematic sampling.</p><p><strong>Pros</strong>:</p><ul><li>It help avoid selection bias. The proportion of the category remains the same after sampling</li><li>Researcher can apply different sampling method on different subgroup. This allow them to choose most suitable method for a subgroup</li></ul><p><strong>Cons</strong></p><ul><li>Time-consuming and hard to design when a sample point has multiple characteristics</li></ul><p>Stratified Sampling is the basic idea of many over-sampling and under sampling methods.</p><h2 id="monte-carlo-simulation">3. Monte Carlo Simulation</h2><h3 id="inverse-transformation-sampling">3.1 Inverse Transformation Sampling</h3><p>We know that if a randam variable <span class="math inline">\(u = CDF(X)\)</span>, then $u U(0,1) $ (refer to this <a href="http://zhengyuanyang.com/2022/11/04/distribution/#distribution-transformation">article</a>). The Inverse Transformation Sampling generate such a uniformly distributed random variab <span class="math inline">\(u \sim U(0,1)\)</span>. Then the sample value X for each u would be <span class="math display">\[X = CDF^{-1}(u )\]</span> <strong>Pros:</strong> easy to apply</p><p><strong>Cons</strong>: given <span class="math inline">\(P(X)\)</span>,$ CDF^{-1}(X)$ cannot always been easily obtained</p><h2 id="mcmc-sampling">4. MCMC Sampling</h2><p>[WIP]</p><p>​</p><h2 id="oversampling-and-undersampling">5. Oversampling and Undersampling</h2><p>Oversampling and Undersampling are not specific sampling or simulation methods. They are two topics in Data Processing. The objective of these two techniques is to solve the problem of the <strong>Imbalanced Sample</strong></p><p>[WIP]</p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Sampling</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic of Probability Theory for Data Science</title>
    <link href="/2022/12/02/Basic-prob/"/>
    <url>/2022/12/02/Basic-prob/</url>
    
    <content type="html"><![CDATA[<h1 id="basic-of-probability-theory-for-data-science">Basic of Probability Theory for Data Science</h1><h2 id="basic-concepts-for-probability">1. Basic Concepts for Probability</h2><p><strong>Random Experiment</strong>:</p><ul><li>A trail that can have more than one possible outcome.</li><li>The trail should be replicable under fixed conditions</li><li>The outcome of the trail is unpredictable</li></ul><p><strong>Event </strong>: A specific outcome of a random experiment(e.g X=1)</p><p><strong>Fundamental Event</strong>: The minimum grain of event defined according to the objective of the random experiment(Not possible or necessary to split into smaller grain). For example, for a throw of dice, the fundamental events would be face = 1,2,...6.</p><p><strong>Compound Event</strong>: A event consists of multiple fundamental events(e. g, for a throw of dice: Face &lt; 5)</p><p><strong>Sample Space</strong>: A collection consists of all possible fundamental events. (e. g, for two flips of a coin <span class="math inline">\(\Omega = \{(face,face),(back,face),(face,back),(back,back)\}\)</span>)</p><p><strong>Random Variable</strong>: A function that map each sample point <span class="math inline">\(\omega\)</span> in the sample space <span class="math inline">\(\Omega\)</span> into a real number <span class="math inline">\(X = X(\omega)\)</span> . strictly, the definition of a event is a collection <span class="math inline">\(\{\omega | X(\omega = a)\}\)</span>, but in most real implementation, just understand event as an outcome.</p><h2 id="interpretation-of-probability">2. Interpretation of Probability</h2><p>Probability describe how likely a event would happen. In the probability theory, the following axiom are give: <span class="math display">\[0 \le P(A) \le 1\]</span></p><p><span class="math display">\[P(\Omega) = 1\]</span></p><p><span class="math display">\[P( A_1+A_2) = P(A_1)+P(A_ 2)\]</span></p><p>Where <span class="math inline">\(\Omega\)</span> is a definite event(containing all fundamental events), <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are exclusive</p><h3 id="classical-model-of-probability">2.1 Classical Model of Probability</h3><p>In Classical Interpretation of probability, two assumptions are considered satisfied:</p><ol type="1"><li>The sample space contains finite fundamental events</li><li>The happening of each fundamental event are equally likely</li></ol><p>Under such assumptions, the probability of an event A can be defined as <span class="math display">\[P(A) = \frac{n_a}{n_s}\]</span> where <span class="math inline">\(n_ s\)</span> is the number of fundamental events in the sample space, and <span class="math inline">\(n_ a\)</span> is the number of fundamental events in event A.</p><p>For most classical probability case, <span class="math inline">\(n_s\)</span> and <span class="math inline">\(n_ a\)</span> can be calculated from permutation and combination: <span class="math display">\[P_n^m = \frac{n! }{(n- m)!}\]</span></p><p><span class="math display">\[C_m^n = \frac{n! }{m!(n- m)!}\]</span></p><h3 id="geometric-model-of-probability">2.2 Geometric Model of Probability</h3><p>Define a geometric measure of a event(e. g length of line segment, area) <span class="math display">\[P(A) = \frac{M(A)}{M(S)}\]</span></p><h3 id="frequency-and-statistical-probability">2.3 Frequency and Statistical Probability</h3><p>Suppose n times of random experiment are conducted, and event A happened m times, the define the frequency of event A as: <span class="math display">\[\omega(A) = \frac{m}{n}\]</span> Then the Statistical Probability of event A is: <span class="math display">\[P(A) = \lim_{n\to\infty}w(A)\]</span> Note that probability is a inner properties of a random variable. Statistical Probability is a mathematic approximation of th real probability</p><h2 id="baisc-theroms-in-probability-theory">3. Baisc Theroms in Probability Theory</h2><p><span class="math display">\[P(A) = 1-P(\bar{A})\]</span></p><p><span class="math display">\[P(A-B) = P(A) - P(A\cap B)\]</span></p><p><span class="math display">\[P(A+B) = P(A) +P(B) - P(A\cap B)\]</span></p><h2 id="conditional-probability-joint-probability-and-independency">4. Conditional Probability, Joint Probability and Independency</h2><h3 id="conditional-probability">4.1 Conditional Probability</h3><p>Let A, B be two events in sample space <span class="math inline">\(\Omega\)</span>, the probability of the A given the condition that B has happened is called conditional probability, denoted as <span class="math inline">\(P(A| B)\)</span></p><p>The sample sapce of <span class="math inline">\(P(A| B)\)</span> is B, not <span class="math inline">\(\Omega\)</span>. Conditioning means "Compression" on the sample spcae. According to the axiom of probability theory, the probability of the whole sample space is 1. Thus, let A be the random variable X = a: <span class="math display">\[\sum_a P(A|B) = 1\]</span> A and B can be two events of a same variable, or each be a event for a separate variable.</p><h3 id="law-of-total-probability">4.2 Law of Total Probability</h3><p>Let <span class="math inline">\(B_1,B_ 2,...,B_n\)</span> be a series of collectively exhaustive events, A be another event: <span class="math display">\[P(A) = \sum_i^n P(B_i)P(A|B_i)\]</span></p><h3 id="joint-probability">4.3 Joint Probability</h3><p>Assume a two-dimension sample space is determined by two random experiment, which means we have two random variables X and Y for a sample space. Let A, B be a certain outcome of variable X and Y respectively, the probability taht events A and B both happen is called the joint probability of A and B, denoted as <span class="math inline">\(P(A, B)\)</span>. The joint probability has the following properties: <span class="math display">\[\sum_x\sum_y P(X=x,Y =y) = 1\]</span> If X and Y are independent: <span class="math display">\[P(X,Y) = P(X)P(Y)\]</span> For such a sample sapce: <span class="math display">\[P(A| B ) = \frac{P(A,B)}{P(B)}\]</span> associated with the Law of Total Probability: <span class="math display">\[P(A)= \sum_i^n P(B_i)P(A|B_i) = \sum_i^n P(A,B_i)\]</span></p><h3 id="bayesian-law">4.4 Bayesian Law</h3><p>Let <span class="math inline">\(A_1,A_ 2,...,A_n\)</span> be a series of collectively exhaustive events, B be another event: <span class="math display">\[P(A_i|B) = \frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B|A_i)}{\sum_i^nP(A_i)P(B|A_i)}\]</span> where we call:</p><ul><li><span class="math inline">\(A_i\)</span>: hypothesis event, an event we want to attest its probability distribution through observations on evidence</li><li>B: evidence, an event used to update knowledge on the hypothesis event</li><li><span class="math inline">\(P(A)\)</span>: prior probability, representing the knowledge before the evidence emerge</li><li><span class="math inline">\(P(B|A)\)</span>: likehood, representing the probability of B under events A</li><li><span class="math inline">\(P( A_i|B)\)</span>: posterior probability, representing the updated knowledge after evidence emerge</li></ul><p>Specific examples of bayesian inference can be referred via this <a href="http://zhengyuanyang.com/2022/09/29/bayes-inference/#%E5%B8%B8%E8%A7%81%E9%A2%98%E5%9E%8B%E5%92%8C%E4%BE%8B%E9%A2%98%E8%A7%A3%E6%9E%90">article</a></p><h3 id="independency-of-events">4.5 Independency of Events</h3><p>If the probability of A is not affected by whether event B happen, then A is independent to B. In conditional probability form: <span class="math display">\[P(A) = P(A|B)\]</span></p><p><span class="math display">\[P(A,B) = P(A)P(B)\]</span></p><p>Note that <span class="math inline">\(A \perp \!\!\! \perp B\)</span> and <span class="math inline">\(A\cap B\)</span> cannot be both true</p><h2 id="probability-distribution-probability-density-function">5. Probability Distribution &amp; Probability Density Function</h2><h3 id="discrete-random-variable-and-probability-distribution">5.1 Discrete Random Variable and Probability Distribution</h3><p>If the possible value of a random variable is countable, then it is a discrete random variable. The probability distribution of a discrete random variable X is defined as a <strong>function</strong>: <span class="math display">\[P(X) = P(X=x_k)\]</span> The PDF has the following properties: <span class="math display">\[P(X) \ge 0\]</span></p><p><span class="math display">\[\sum_k P(X) = 1\]</span></p><h3 id="continuous-random-variable-and-probability-density">5.2 Continuous Random Variable and Probability Density</h3><p>If a randome variable can be any value on a range <span class="math inline">\([a , b]\)</span> and there exists a integratable function <span class="math inline">\(f( x)\)</span>, so that: <span class="math display">\[P( a&lt; X \le b  ) = \int_a^ b f(x)dx\]</span> Then X is called a continuous random variable, <span class="math inline">\(f(x )\)</span> is called the probability density function of X.</p><p>For a continuous random variable, the probability of each single sample point would be 0. Instead of an actual probability, the distribution of a continuous random variable is described by the probability density of each data point. The value of <span class="math inline">\(f(x)\)</span> on a specific point represents the probability density of that sample point: <span class="math display">\[f(x= a) = \lim_{\Delta \to \infty } P(a&lt;X \le a+ \Delta  )\]</span></p><h3 id="distribution-type">5.3 Distribution Type</h3><p>For details about distribution type, refer to <a href="http://zhengyuanyang.com/2022/11/04/distribution/">here</a></p><h2 id="expectation-and-variance">6. Expectation and Variance</h2><h3 id="expectation">6.1 Expectation</h3><p>For discrete variable: <span class="math display">\[E[X] = \sum_i^n x_iP(X=x_i)\]</span></p><p>For discrete variable: <span class="math display">\[E[X] = \int_{-\infty}^\infty xf(x)dx\]</span> if E[X] can converge</p><p>Properties of Expectation: <span class="math display">\[E[X+C] = E[X]+C\]</span></p><p><span class="math display">\[E[CX] = CE[X]\]</span></p><p><span class="math display">\[E[X + Y] = E[X]+E[Y] \qquad \forall X, Y\]</span></p><p><span class="math display">\[E[XY] = E[X]E[Y] \qquad if \ X \perp \! \! \perp Y\]</span></p><p><span class="math display">\[E[g(X)] = \sum_i^n g(x_i)P(X=x_i) \qquad or \qquad \int_{-\infty}^\infty g(x)f(x)dx\]</span></p><p><span class="math display">\[E[X|Y=y] = \sum_i^nx_iP(X=x_i|Y=y) \qquad or \qquad \int_{-\infty}^\infty xf(x|y)dx\]</span></p><h3 id="variance">6.2 Variance</h3><p><span class="math display">\[V[X] = E[(X-E[X])^2] = E[ X^ 2]-(E[X])^2\]</span></p><p>Properties of Variance: <span class="math display">\[V[C] = 0\]</span></p><p><span class="math display">\[V[X+C] = V[X]\]</span></p><p><span class="math display">\[V[CX] = C^2V[X]\]</span></p><p><span class="math display">\[V[X\pm Y] = V[X]+V[Y] \pm 2Cov[X, Y]\]</span></p><p><span class="math display">\[V[g(X)] = g&#39;(E[X])^2V[X]\]</span></p><h3 id="covariance">6.3 Covariance</h3><p><span class="math display">\[Cov[X, Y] = E[(X-E[X])(Y- E[Y])] = E[XY]-E[X]E[Y]\]</span></p><p>Covariance is a measure of the joint variability of two random variables, it represent thedegree that two variables variate samely in directions.</p><p>Properties of Variance: <span class="math display">\[Cov[X,Y] = 0 \qquad if \ X \perp \! \! \perp Y\]</span></p><p><span class="math display">\[Cov[X, Y] \le \sqrt {V[X]V[Y]}\]</span></p><p><span class="math display">\[Cov[X,X] = V[X]\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Probability</tag>
      
      <tag>Basic Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Structural Causal Model</title>
    <link href="/2022/11/10/structual-causal-model/"/>
    <url>/2022/11/10/structual-causal-model/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-of-structual-causal-model">Introduction of Structual Causal Model</h1><h2 id="about-structural-causal-model">1. About Structural Causal Model</h2><p>The Structural Causal Model(SCM) is another framework of causal inference. It is a way of describing the relevant features of the world and how they interact with each other. It is widely used in causal relationship digging.</p><h2 id="basis-of-graph-theory">2. Basis of Graph Theory</h2><p><strong>Graph</strong>: a collection of vertices(nodes) and edges. The nodes are connected by edges.</p><p><strong>Adjacent</strong>: If there is a edge between two nodes, then these two nodes are adjacent</p><p><strong>Compete Graph</strong>: A graph is a complete graph if every pair of nodes in it is adjacent</p><p><img src="/2022/11/10/structual-causal-model/1.png" style="zoom:15%;"></p><p><strong>Path</strong>: A sequence of adjacent nodes and their edges between node X and node Y is called a path between X and Y</p><p><strong>Directed/Undirected Graph</strong>: An edge can be directedor undirected. If all edges in a graph is directed, this graph is a directed graph</p><p><strong>Directed Path:</strong> If every edges in a path have same directions, then this path is a directed path</p><p><strong>Parent/Child</strong>: The Start and End nodes of the directed graph</p><p><strong>Ancestor/Descendant</strong>: For a directed path,the ancestor nodes are all nodes before this node, the descendant nodes are all nodes after that node</p><p><strong>Cyclic</strong>: when a directed graph contains a path that its starting node and ending node are the same node, then this path is called a cyclic</p><p><img src="/2022/11/10/structual-causal-model/2.png"></p><p><strong>DAG</strong>: A directed graph with no cyclics in it is called a Directed Acyclic Graph(DAG)</p><h2 id="basic-theory-of-scm">3. Basic Theory of SCM</h2><h3 id="causility">3.1 Causility</h3><p>In SCM, the causality is considered as function with a time order. For two variable X and Y, we call X a <strong>direct cause</strong> of Y if: <span class="math display">\[Y = f(X)\]</span> note that there's a time order for the happenings of X and Y. We cannot regard the cause as <span class="math inline">\(X= f^{-1}(Y)\)</span></p><p>We call X a <strong>indirect cause</strong> of Y is X is a cause of any cause(direct or indirect) of Y.</p><p>In SCM, the relationship of a direct cause X, an outcome Y and their causality f is presented with a parent node. a child node and the directed edge between them.</p><p>In most case, a causility would demonstrate statistical dependence on observations. From a probability aspect, we can also interpret this correlation as a joint probability. For an causal graph: <span class="math display">\[P(X_1,X_2,...X_n) = \prod_i^n P(X_i|par(X_i))\]</span> where <span class="math inline">\(par(X_i)\)</span> is all parent nodes of <span class="math inline">\(X_i\)</span>. If <span class="math inline">\(X_i\)</span> does not have a parent node(Exogenous), then just apply<span class="math inline">\(P(X_i)\)</span></p><h3 id="exogenousendogenous-variable">3.2 Exogenous/Endogenous Variable</h3><p>In SCM, the exogenous variables, usually denoted as U, are variables considered external to the model, and we would not explained how these variables are caused. Oppositely, the endogenous variable,usually denoted as V, are variables considered a descendant of at least one exogenous variable. Exogenous variables have no ancestors, only descendants, while endogenous variables have both ancestors and descendants.</p><p>With such assumption, we can represent the causality of variables with a DAG.</p><p><img src="/2022/11/10/structual-causal-model/3.png"></p><p>In a causality DAG, only when there's a directed path between two variables can we regard as there exists a direct or indirect causality between these two variable.</p><h3 id="intransitive-case">3.3 Intransitive Case</h3><p>As discussed in <a href="http://zhengyuanyang.com/2022/11/07/basic-causal-inference/">this article</a>, we know that correlation does not imply causality. On the opposite, if X causes Y, then in most case X and Y are statistically dependent. Nevertheless, there still exists some extreme case where causality does not show statistical correlation.</p><p>For example, suppose gene A can increase a person's risk of getting a cancer, while there exists another gene B that can depress people's risk of getting cancer. If we observe a sample where each person has both gene A and B, we probably would get the conclusion that both gene are independent to the rate of getting a cancer.</p><p>Another example is the "exclusive or" logic, if the two inputs are both binary variable with equal probability, and the output is a EO calculation of the two binary example, then the output are independent to each of the two inputs from a probability perspective.</p><p>Other cases might be found in real application, we cannot always expect that causality would always generate statistical significant difference. However, we still believe in most cases the significance would be showned, which is the basic assumption for causal inference</p><h3 id="basic-structure-for-causality-graph">3.4 Basic Structure for Causality Graph</h3><h4 id="chain-fork-and-colider">3.4.1 Chain, Fork and Colider</h4><p>The basic causal structure in SCM can be concluded into three types, and each type describes the relationship among three Causal Structure in SCM</p><p><strong>Chain</strong>: For a chain, the first node and the last node are statistically depent, association flows through the directed path, to block the association, we need to conditioning on <span class="math inline">\(X_ 2\)</span></p><p><img src="/2022/11/10/structual-causal-model/4.png"></p><p><strong>Fork</strong>: For a fork, the two descendant nodes are statistically depent, association flows through the path even if its indirected, to block the association, we need to conditioning on <span class="math inline">\(X_ 2\)</span></p><p><img src="/2022/11/10/structual-causal-model/5.png"></p><p><strong>Colider </strong>: For a colider, the two parent nodes are not statistically depent, the intermediate nodes naturally block the association without conditioning</p><p><strong>Conditioning </strong>: conditioning means segment samples into subgroups according to certain values. In a SCM context, if we condition the intermediate variable, we can regard it as a constant in causality:</p><ul><li>For a chain, before conditioning, <span class="math inline">\(X_3 = f(X_2,u_3) = f(g(X_1),u_3)\)</span>, after conditioning, <span class="math inline">\(X_3 = f(u_3)\)</span></li><li>For a fork, before conditioning, <span class="math inline">\(X_3 = f(X_2,u_3), X_1 = f(X_2,u_1)\)</span>, after conditioning, <span class="math inline">\(X_3 = f(u_3), X_1 = f(u_1)\)</span></li><li>For a colider, before conditioning, <span class="math inline">\(X_1 = f(u_1),X_ 3=f(u_ 3)\)</span>, after conditioning on <span class="math inline">\(X_2\)</span> or any of its descendants, although the cause of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> remains the same, we now have <span class="math inline">\(X_ 2 = f(X_1,X_3) = c\)</span>, so statistical dependence would appear between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span></li></ul><p><img src="/2022/11/10/structual-causal-model/7.png"></p><p><img src="/2022/11/10/structual-causal-model/8.png"></p><h4 id="d-separate">3.4.2 D-separate</h4><p>D-separation is a method used to determine whether two nodes in a causal graph is statistically independent.</p><ul><li><span class="math inline">\(X \ and \ Y \ are \ d-sep \iff X \ and \ Y \ are \ statistically \ indepentdent\)</span></li><li>$X  d-sep  Y | Z X  and  Y  are  statistically  indepentdent | Z $</li></ul><p>From the previous section we know that in a V structure,</p><ol type="1"><li>If the cenral node are not conditioned, two nodes are blockd by a colider and unblocked(d-connected) by a chain or a fork.</li><li>If the cenral node are conditioned, two nodes are blockd by a chain or a fork and unblocked by a colider.</li></ol><p>If every paths between two nodes X and Y are blocked, we called X and Y d-separated.</p><h2 id="causality-search">4. Causality Search</h2><h3 id="test-on-causality-model">4.1 Test on Causality Model</h3><p>With the definition of the D-separation, we can test the causal model we assume by examine where the statistical independence inferred from the model can be validated through data. For example, for such a causal model:</p><p><img src="/2022/11/10/structual-causal-model/9.png"></p><p>The path <span class="math inline">\(W \to X \to Z_1\)</span> should be blocked if we conditioned on X, which means this model implies: <span class="math inline">\(Z_1 \perp \!\!\! \perp W\ |X\)</span></p><p>We can then regress a linaer model <span class="math inline">\(Z_1 = r_1W + r_2X\)</span>, if <span class="math inline">\(r_ 1\)</span> is obviously greater than 0, we can regard as the assumed causal model is incorrect. We can also apply other statistical method to test correlation.</p><p>Through this kind of ideas, we can infer the true causal model by testing potential causal models based on dataset</p><h3 id="equivalent-class">4.2 Equivalent Class</h3><p>For some causal relationship, we may found two different potential causal graph of it appears to have same statistical independent on data. For example, a fork and a chain have same relationship on statistical independence. When we try to infer the causal relationship, we might found some causility, or we can say the directions of some edgs in the DAG, cannot be decided through data. In this case, we call these potential models with same statistical independence appearance <strong>Equivalent Classes</strong></p><p><img src="/2022/11/10/structual-causal-model/10.png"></p><p>A intuitive method to judge whether two graph are equivalent classes is to check each V-structure in for both graph and earse the direction of the edges if the two V-struture are equivalent. If there are no directions left, then the two classes are equivalent. If there are remaining directed edges or V-structure that are different, then the two graphs are not equivalent.</p>]]></content>
    
    
    <categories>
      
      <category>Causal Inference</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Structural Causal Model</tag>
      
      <tag>Introduction</tag>
      
      <tag>Causal Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Potential Outcome Framework</title>
    <link href="/2022/11/07/potential-outcome/"/>
    <url>/2022/11/07/potential-outcome/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-of-potential-outcome-framework">Introduction of Potential Outcome Framework</h1><h2 id="potential-outcomes-framework">1. Potential Outcomes Framework</h2><p>The Potential Outcomes framework is a series of notation widely used in causal inference.</p><h3 id="conditioning-v.s.-intervening">1.1 Conditioning v.s. Intervening</h3><p>In statistics, conditioning and intervening is two different concepts.</p><p><strong>Conditioning</strong>: Segmenting a dataset into subset bu applying conditions on variables</p><p><strong>Intervening:</strong> Intervene a process with certain treatment, so that every unit is affected by treatment variable</p><p><img src="/2022/11/07/potential-outcome/1.PNG"></p><p>we use the <strong>do</strong> operands the represent that we intervene a process. Let Y donate the outcome statistics of the process, the probability of Y = y under a intervention such that T=t is noted as <span class="math inline">\(P(Y=y|do(T=t))\)</span> or <span class="math inline">\(P(y|do(t))\)</span></p><p>When there exists confounder in the process, the conditional probability does not equal the interventional probability: <span class="math display">\[P (Y|do(t),X) = E_X[P(Y|T=t,X)] \ne  P(Y|T=t,X)\]</span></p><h3 id="potential-outcomes">1.2 Potential Outcomes</h3><p>Suppose we want to evaluate the cause effect of a pill on a single person.</p><p><img src="/2022/11/07/potential-outcome/2.PNG"></p><p>Let T denotes whether this person take a pill, Y denotes the degree that his headache is relieved after taking the pill. The Cause Effect can be noted as: <span class="math display">\[Y|do(T=1) \ - Y|do( T =0) = Y(1)-Y(0)\]</span> The cause effect of a treatment on a single unit is called <strong>Individual Treatment Effect</strong>(ITE)</p><p>A problem of evaluating the effect this way is that for a single person we can only observe Y for once. If we intervene the treatment so that T = 1, we won't obtain any observations about Y(0).</p><p>In such case, we call Y(1),Y(0) the potential outcomes under T. A potential outcome is the outcome of the statistics under our potential treatment.</p><p>For a specific unit, suppose we applied treatment T=t on it, we call the observed outcome <span class="math inline">\(Y^F= Y(T=t)\)</span> the factual outcome. The potential outcome that not applying T=t, <span class="math inline">\(Y^{CF} = Y(T\ne t)\)</span> is called the counterfactual outcome. <span class="math inline">\(Y^{CF }(T =1) = Y^F(T=0)\)</span></p><h3 id="average-treatment-effect">1.3 Average Treatment Effect</h3><p>Due to the problem discussed in 1.2, we usually cannot get the ITE of a treatment directly. However, if we do not put specific emphasis on the effect of individual, we can obtain the overall effect size by calculating the <strong>Average Treatment Effect(</strong>ATE) of a treatment on a population: <span class="math display">\[ATE = E[ITE] =E[Y^F(T=1)] - E[Y^{CF}(T=1)] = E[Y(1)] - E[Y(0)]\]</span> When there exists no confounders: <span class="math display">\[E[Y(1)] = E[Y|T=1]\]</span></p><p>When there exists any confounders: <span class="math display">\[E[Y(1)] = E_X[E[Y|T=1,X=x]]\]</span> here the X follows its original distribution <span class="math inline">\(X \sim D(\theta)\)</span></p><p>However, when we try to interpret the effect size from observational data, what we actually obtained is <span class="math display">\[E[Y|T=1] - E[Y|T=0] = E_{X_1}[E[Y|T=1,X_1]] - E_{X_0}[E[Y|T=0,X_0]]\]</span> Here <span class="math inline">\(X_T\)</span> and <span class="math inline">\(X_C\)</span> follow conditional probability distributions given T instead of X's original distribution: <span class="math inline">\(X_1 = (X| T =1), X_0 = (X|T=0)\)</span>, <span class="math inline">\(X_1 \sim D(\theta_1), X_0 \sim D(\theta_0)\)</span></p><p>when confounder exists, X would have different distribution on the treatment and control group, thus <span class="math display">\[P(X_1) \ne^d P(X_0)\]</span> Thus, when confounder exists: <span class="math display">\[E[Y(1) - Y(0)] = E_X[E[(Y|T=1,X)] - E[(Y|T=0,X)]] \ne E[Y|T=1] - E[Y|T=0]\]</span></p><p>In most real application cases, there are multiple confounders in the causal inference process. To estimate the ATE, we need to "block" the effects of confounder. We call the treatment effect under a segmentation a Conditional Average Treatment Effect(CATE) and calculate the weighted average of the CATE as the ATE estimation. The weight is the probability of the segmentation in the whole population. Such kind of methods are called <strong>Backdoor Adjustment</strong></p><h3 id="basic-assumptions-about-potential-outcomes">1.4 Basic Assumptions about Potential Outcomes</h3><p><strong>Stable Unit Treatment Value Assumption(SUTVA)</strong>: The potential outcome of any unit won't be changed by treatment on other units. This assumption emphasize the independency of units</p><p><strong>Positivity Assumption</strong>: For units of any possible values of background variable, any assigned treatment is possible. <span class="math display">\[P(T=t|X=x) \in (0,1) \quad \forall t\ and \ x\]</span> <strong>Consistency Assumption </strong>: If T=t is applied on a group of unit, then the outcomes of all units in that group would be Y(T=t). <span class="math display">\[T = t \Rightarrow Y=Y(t)\]</span> The consistency assumption make sure we have $Y(t) = Y|T=t $ when there's no confounders</p><p><strong>Ignorability Assumption</strong>: The determination of the treatment T is independent to the potential outcomes: <span class="math display">\[(Y(1),Y(0)) \perp \!\!\! \perp T\]</span> Under the Ignorability Assumption: <span class="math display">\[E[Y(1)] = E[Y(1)|T=1] = E[Y| T=1]\]</span> The Ignorability Assumption is usually considered satisfied when we conduct a random control trail. If we randomized the determination of the treatment, this assumption implies the influence of the confounders are blocked. However, in a non-experimental context, since T is not randomized, the influence of the confounder still exists, and the Ignorability Assumption is not satisfied. In such scenarios, we would weaken Ignorability Assumption into the following assumption.</p><p><strong>Conditional Ignorability Assumption</strong>: Given background variables X, the determination of the treatment T is independent to the potential outcomes <span class="math display">\[(Y(1),Y(0))|X \perp \!\!\! \perp T\]</span> Under the conditional Conditional Ignorability Assumption: <span class="math display">\[E[Y(1)|X=x_i] = E[Y(1)|T=1,X=x_i] = E[Y| T=1,X=x_i]\]</span> With these assumption, we can make the following statement for a causal inference process:</p><ul><li><p>The SUTVA, Positivity and Consistency should always be satisfied</p></li><li><p>If a RCT is conducted, we can regard as <span class="math inline">\((Y(1),Y(0)) \perp \!\!\! \perp T\)</span>, and The ignorability assumption can be considered satisfied. Thus a RCT is a best solution for causal inference</p></li><li><p>If RCT is not implementable, we can condition on the confounder to satisfy the conditional ignorability assumption and estimate the CATE, then we may estimate the distribution of the confounders and calculate the exception of the CATE to obtain ATE</p><p><img src="/2022/11/07/potential-outcome/3.PNG"></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Causal Inference</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distribution</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Introduction of Causal Inference</title>
    <link href="/2022/11/07/basic-causal-inference/"/>
    <url>/2022/11/07/basic-causal-inference/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-of-causal-inference">Introduction of Causal Inference</h1><h2 id="about-causal-inference">1. About Causal Inference</h2><p>Causal Inference refer to the process that we want to discover the real causality between variables based on data. Specifically, there are two domains of causal inference:</p><ul><li>Causality Discovery: find the causal structure among variables</li><li>Causal Effect Estimation: give a quantitative evaluation on the causal effect of a variable to another</li></ul><h2 id="why-causal-inference">2. Why Causal Inference</h2><p>The core reason why we cannot discover causal realtionship and estimate causal effect through traditional statistical methods is that: <strong>Correlation does not imply Causality</strong>. Use traditional Statistics methodology, which is basically based on statistical dependence, on causal reasoning would lead to bias</p><h3 id="simpson-paradox-confounder">2.1 Simpson Paradox: Confounder</h3><p>Suppose we have two treatment A and B for a certain disease and want to evaluate if A has better effect by cure rate. There are two categories pf patients: mild patients and severe patients. Suppose the truth is condition of patients have effects on both the selection of treatment and the cure rate:</p><p><img src="/2022/11/07/basic-causal-inference/1.png"></p><p>However, since we failded to recognize the ture causal relationship among these variables, we assume there's no other facor other than treatment and effect:</p><p><img src="/2022/11/07/basic-causal-inference/2.png"></p><p>Under these assumption, we obtained the following data:</p><table><thead><tr class="header"><th>Treatment</th><th>Mild</th><th>Severe</th><th>Total</th></tr></thead><tbody><tr class="odd"><td>A</td><td><span class="math inline">\(\frac{30}{100} = 30\%\)</span></td><td><span class="math inline">\(\frac{210}{1400} = 15\%\)</span></td><td><span class="math inline">\(\frac{240}{100} = 16\%\)</span></td></tr><tr class="even"><td>B</td><td><span class="math inline">\(\frac{100}{500} = 20\%\)</span></td><td><span class="math inline">\(\frac{5}{50} = 10\%\)</span></td><td><span class="math inline">\(\frac{105}{550} = 19\%\)</span></td></tr></tbody></table><p>Here a simpson paradox appears: treatment A is better than treatment B on every segments, but the overall effect of A is actually lower.</p><p>The occurrence of simpson paradox usually suggest the existence of <strong>confounder</strong> variables. If one or more variables have causal effects on both of two variable, we call these variables confounders of the two variables. In this case, the condition is a confounder.</p><p>Let Y denote the cure rate, X denote the condition of patients, and T denote the selection of treatment. For these data, what we observed is: <span class="math display">\[E[Y|T= A] - E[Y| T= B] = 16\%-19\% = -3\%\]</span> under the wrong assumption that there's no confounding variables, we believe: <span class="math display">\[E[Y(A) - Y(B)] =E[(Y|T=A) - (Y|T=B)] = E[Y|T= A] - E[Y| T= B] = -3\%\]</span> and we draw the conclusion that A has negative effect.</p><p>However, the truth is, the when there are confounders, the true interpretation of the results ought to be: <span class="math display">\[E[Y(A) - Y(B)] = E_x[E[(Y|T=A,C) - (Y|T=B,C)]] = \sum_i P(C=i)E[(Y|T=A,C) - (Y|T=B,C)]\]</span></p><p><span class="math display">\[E[Y(A) - Y(B)] = \frac{600}{2050}*(0.3-0.2)+\frac{215}{2050}(0.15-0.1) = 3.4\%\]</span></p><p>What we obtained from the data is: <span class="math display">\[E[Y|T= A] - E[Y| T= B] = E_x[E[Y|T=A,X]] - E_x[E[Y|T=B,X]] = -3\%\]</span> Note that the X here follows a conditional probability distribution given T instead of its original distribution: <span class="math display">\[E_x[E[Y|T=A,X]] = \sum_iP(X = i|T=A)E[Y|T=A,X=i]\]</span> As the conditional probability distribution of X given <span class="math inline">\(T = A\)</span> and <span class="math inline">\(T=B\)</span> are different <span class="math display">\[E_x[E[(Y|T = A,X) - (Y|T=B,X)] \ne E_X[Y|T= A,X] - E_X[Y| T= B,X]\]</span> When there's confounder, the statistical denpendy can not accurately represent the causality. In this case, treatment A has positive effect on cure rate. However, since the patients who choose cure A are mostly severe patients and has lower cure rate, the condition variable create a negative association between T and Y. Thus the statistical correction of T and Y are undermined because of X.</p><p>When there exists confounders: <span class="math display">\[E[Y|do(T= A)] - E[Y|do(T=B)] \ne E[Y|T= A] - E[Y| T= B]\]</span></p><h3 id="berkson-paradox-selection-bias">2.2 Berkson Paradox: Selection Bias</h3><p>Suppose X and Y represent the academic performance and professional experience of a student, and we want to know if X has a causality on Y. Suppose the real situation is X and Y are independent. Also, there's a fact that students with high academic scores and outstanding work experience are more likely to be admitted by a university. Let Z donate the probability of a student being admitted by a university, the true causal structure of the problem is:</p><p><img src="/2022/11/07/basic-causal-inference/3.png"></p><p>However, we failed to recognize such a variable Z that both caused by X and Y. When we draw the sample, we go to a university to investigate the academic performance and professional experience of its students. Thus, all samples drawn are student admitted by university.</p><p><img src="/2022/11/07/basic-causal-inference/4.png"></p><p>To make it easy, consider X, Y and Z as binary variables. Suppose we have following data:</p><table><thead><tr class="header"><th></th><th>Y = high</th><th>Y =low</th></tr></thead><tbody><tr class="odd"><td>X = high</td><td><span class="math inline">\(\frac{130}{150}\)</span></td><td><span class="math inline">\(\frac{40}{155}\)</span></td></tr><tr class="even"><td>X= low</td><td><span class="math inline">\(\frac{50}{155}\)</span></td><td><span class="math inline">\(\frac{5}{150}\)</span></td></tr></tbody></table><p>In each cell, the numerator and the denominator respectively represent the number of admitted students and all students. Its easy to find that X and Y are independent by conducting a <span class="math inline">\(\chi^2\)</span> test based on the denominator values.</p><p>Suppose there exists no confounders. The real ATE should be: <span class="math display">\[E[Y|X= high] - E[Y|X=low] = (150+155) - (155+150) = 0\]</span> However, as we conditioned on Z, the ATE we know observed become: <span class="math display">\[E[Y| X =high, Z = admitted] - E[Y|X=low,Z = admitted] = (130-40)-(50-5) = 45\]</span> We draw incorrect conclusions that X have positive causal effects on Y. The reason is that the the admitted student cannot represent the whole population. In this case, Z is a selection variable, it is caused by both X and Y. If we condition on Z, then association is created between X and Y, and we cannot accurately estimate the causal effect from X to Y through statistical dependency. This is called a selection bias.</p><h3 id="reverse-causation">2.3 Reverse Causation</h3><p>Suppose we want to discover the causal relationship from longer marriage to longer life. The truth is that having a longer marriage has no positive on one's longevity. However, on the other hand, elder people usually have longer marriage as they spent more time. Thus, the true causal relationship between X(longevity) and Y(marriage length) is:</p><p><img src="/2022/11/07/basic-causal-inference/5.png"></p><p>However, according to our assumption, the causality is reversed, we expected to test the following structure:</p><p><img src="/2022/11/07/basic-causal-inference/6.png"></p><p>without any doubts, we would find a statistical dependency between X and Y as statistical association is not directed. However, letting people have longer marriage does not help extend thei lifetime. The causality we discovered is wrong as it is reversed.</p><h2 id="basic-methodology-of-causal-inference">3. Basic Methodology of Causal Inference</h2><h3 id="experimental-data-and-observational-data">3.1 Experimental data and Observational Data</h3><p><strong>Observational Data</strong></p><p>Data are observed and collected for each subjects. No manipulations(intervening) on these subjects occur. We can condition on certain variables, but what we have would then be a subset of the original data</p><p><strong>Experimental Data</strong></p><p>Subjects are manipulated(intervened). The mechanism the data generated are artificially changed.</p><p>Causal inference is very easy on experimental data, while can be much more difficult on observational data.</p><h3 id="randomized-control-experiment">3.2 Randomized Control Experiment</h3><p>We can estimate causality based on Experimental Data through Randomized Control Experiment(RCT).</p><p>Suppose we want to explore the causal relationship from increasing exercise to physical health. Let</p><ul><li>T: denote whether increase exercise</li><li>Y: physical condition of a unit</li><li>X: any confounder that can create a statistical correction between T and Y, such as gender, weight and age</li><li>Z: any common results of Y and T, such as physical condition evaluation score</li></ul><p>To collect observational data, we just record the T, Y and X of units we can observed</p><p>To collect experimental data, we need to split all subject into treatment group(increase exercise) and control group(not increase exercise). The process of deciding which group to assign for a subject should be completely randomized. This means X should have same distribution for treatment group and control group. We can such a scenario Covariate Balance <span class="math display">\[P(X|T=1)=^d P(X|T=0)\]</span></p><p>Under such intervention, we can regard as the causality from X to T are <strong>blocked</strong>(cannot demonstrate dependency). Also, such a randomized selection ensure that there's no conditioning on Z.</p><p><img src="/2022/11/07/basic-causal-inference/7.png"></p><p>The randomized process ensured there's no other floating association from T to Y other than the directed chain path, which is the one we care about. Under this scenarios, we can estimate causal effect directly from</p>]]></content>
    
    
    <categories>
      
      <category>Causal Inference</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Common Distribution Types</title>
    <link href="/2022/11/04/distribution/"/>
    <url>/2022/11/04/distribution/</url>
    
    <content type="html"><![CDATA[<h1 id="common-distribution-types">Common Distribution Types</h1><h2 id="random-variable-and-probability-distribution">1. Random Variable and Probability Distribution</h2><p><strong>Random Variable</strong></p><p>A random variable is a quantity whose values depends on the outcome of a random event. Random variable is terminology for numerical data. The probability that the value of a random variable X equals a certain outcome can be given as <span class="math inline">\(P(X = x_1)\)</span></p><p><strong>Distribution</strong></p><p>The probability distribution function is a function <span class="math inline">\(P = f(x)\)</span>, where:</p><ul><li>x donates the possible values of random variable</li><li>y donates the probability when X = x</li></ul><p>Note that the integrate of PDF is 1.</p><p><strong>Parameter</strong></p><p>A parameter is used to determine the shape of a type of PDF. Different type of PDF has different kinds of parameters. <span class="math display">\[P = f_X(x,\theta)\]</span></p><h2 id="discrete-distribution">2. Discrete Distribution</h2><h3 id="bernoulli-distribution">2.1 Bernoulli Distribution</h3><p>A trial is performed with probability p of success, and X is a random variable indicates success or not. In such case, X has 2 possible values(1 means success), and it follows a bernoulli distribution. <span class="math inline">\(p\)</span> is the only parameter for a Bernoulli Distribution.</p><p>The PDF of a bernoulli distribution: <span class="math display">\[f(x) = p^ x (1-p)^{1-x}\]</span> The expectation of a bernoulli distribution is: <span class="math display">\[E[X] = \sum_i x_i f( x) = 0+p =p\]</span></p><p>The variance of a bernoulli distribution is: <span class="math display">\[Var[X] = \sum_i(x_i-E[x]^2)f(x) = p(1-p)\]</span></p><h3 id="binomial-distribution">2.2 Binomial Distribution</h3><p><img src="/2022/11/04/distribution/1.png"></p><p><strong>Story</strong></p><p>Let randome variable X donates the number of success we achieved in n independent bernoulli trials. Each trail has a same probability p of success. The parameters of a Binomial Distribution includes n and p</p><p><strong>Formulation</strong></p><p>The PDF of a binomial distribution: <span class="math display">\[f_X(X = k,n,p) = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\]</span> where k = 0,1,2...n</p><p>The CDF of a binomial distribution: <span class="math display">\[F(k,n,p) = P(X\le k) = \sum_{i=0}^k \tbinom{n}{i}p^i(1-p)^{k-i}\]</span> The expectation of a binomial distribution is: <span class="math display">\[E[X] = np\]</span></p><p>The variance of a binomial distribution is: <span class="math display">\[Var[X] = np(1-p)\]</span> <strong>Properties</strong></p><ul><li>if <span class="math inline">\(X \sim bin( n, p), Y \sim bin(m,p)\)</span>, then <span class="math inline">\(X+Y \sim bin(n+m,p)\)</span></li><li>If p is small, <span class="math inline">\(Bin(n,p)\)</span> is approximately <span class="math inline">\(Pois(\lambda = np)\)</span> when <span class="math inline">\(n \to \infty\)</span></li><li>if n is large and p is not near 0 and 1, <span class="math inline">\(Bin(n,p)\)</span> is approximately <span class="math inline">\(N(np, np(1-p))\)</span></li></ul><h3 id="geometric-distribution">2.3 Geometric Distribution</h3><p><img src="/2022/11/04/distribution/6.PNG"></p><p><strong>Story</strong></p><p>A Geometric Distribution basically have same story like a binomial distribution, except now the X donates times of trails to observe the first success. The only parameter of a Geometric Distribution is p.</p><p><strong>Formulation</strong></p><p>The PDF of a Geometric Distribution: <span class="math display">\[f(X=k) = (1-p)^{k-1}p\]</span> The CDF of a Geometric Distribution: <span class="math display">\[F(X =k) = 1-(1-p)^k\]</span> The expectation of a Geometric Distribution is: <span class="math display">\[E[X] = \frac{1}{p}\]</span></p><p>The variance of a Geometric Distribution is: <span class="math display">\[Var[X] = \frac{1-p}{p^2}\]</span> <strong>Properties</strong></p><ul><li>Memoryless: if <span class="math inline">\(x \sim Geom(p)\)</span>, then <span class="math inline">\(P(T&gt;t+s|T&gt;t) = P(T&gt;s)\)</span>. This suggests no matter how many previous failures happened, the <strong>additional number of times of trail</strong> needed to observe a success still follows a <span class="math inline">\(\sim Geom(p)\)</span></li></ul><h3 id="possion-distribution">2.4 Possion Distribution</h3><p><img src="/2022/11/04/distribution/2.png"></p><p><strong>Story</strong></p><p>Suppose an event(X = x) happens with a certain probability(usually low probability), let X be the times that event happens in a unit time, then X follows a Possion Distribution. <span class="math inline">\(\lambda\)</span> is the only parameter for a Possion Distrbution, which is the expectation of X.</p><p><strong>Formulation</strong></p><p>The PDF of a Possion Distribution: <span class="math display">\[f_X(X = k,\lambda) = \frac{e^ {-\lambda}\lambda^k}{k!}\]</span> where k = 0,1,2...<span class="math inline">\(\infty\)</span></p><p>The CDF of a Poission distribution: <span class="math display">\[F_X(X=k, \lambda) = e^{-\lambda}\sum_i^k\frac{\lambda^i}{i!}\]</span></p><p>The expectation of a poisson function is: <span class="math display">\[E[X] = \lambda\]</span></p><p>The variance of a poisson function is: <span class="math display">\[Var[X] = \lambda\]</span></p><p><strong>Properties</strong></p><ul><li><p>Let N(t) denote the number on occurrence of the event in a time t instead of a unit time, it can be proved that <span class="math inline">\(P(N(t) = n) \sim Pois(\lambda t)\)</span></p></li><li><p>Consider a Poisson Process with rate <span class="math inline">\(\lambda\)</span>, let <span class="math inline">\(T_n\)</span> denote the time interval between the <span class="math inline">\(n-1^{th}\)</span> and <span class="math inline">\(n^{th}\)</span> occurrence of the event, then <span class="math inline">\(T_1,T_2,..T_n\)</span> is i.i.d. exponential random variable with rate <span class="math inline">\(\lambda\)</span></p></li><li><p>if <span class="math inline">\(X \sim pois(\lambda_1), Y \sim pois(\lambda_2)\)</span>, then <span class="math inline">\(X+Y \sim pois(\lambda_1+\lambda_2)\)</span>. Oppositely, if a Poisson Process's result can be further classified with n category with probability <span class="math inline">\(p_1,p_2,..p_n\)</span>, where <span class="math inline">\(\sum_i^np_i = 1\)</span>, then we can treat the original poisson process as n sub poisson processes with rate <span class="math inline">\(\lambda p_1, \lambda p_2,...\lambda p_n\)</span></p></li><li><p>Consider 2 poisson process with rate <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>, let <span class="math inline">\(S_n^1\)</span> denote the time of <span class="math inline">\(n^{th}\)</span> event of the first process,and <span class="math inline">\(S_m^2\)</span> the time of the <span class="math inline">\(m^{th}\)</span> event of the second process: <span class="math display">\[P(S_n^1 &lt; S_m^2) = \sum^{n+m-1}_{k=n} {n \choose m}(\frac{\lambda_1}{\lambda_1+\lambda_2})^k(\frac{\lambda_2}{\lambda_1+\lambda_2})^{n+m-1-k}\]</span></p></li><li><p>Let <span class="math inline">\(X,Y\)</span> be poisson random variable with rate <span class="math inline">\(\lambda_1,\lambda_2\)</span>, <span class="math inline">\(X|(X+Y=n) \sim Bin(n,\frac{\lambda_1}{\lambda_1+\lambda_2})\)</span></p></li></ul><h2 id="continuous-distribution">3. Continuous Distribution</h2><h3 id="uniform-distribution">3.1 Uniform Distribution</h3><p><strong>Story</strong></p><p>The Uniform Distribution, as the name suggests, refers to a probability distribution where all outcomes in a range are equally likely.A uniform distribution has two parameters <span class="math inline">\(a ,b\)</span>, respectively represents the beginning and end of the range of possible outcome</p><p><strong>Formulation</strong></p><p>The PDF of a uniform distribution: <span class="math display">\[f(x) = \left \{ \begin{aligned}&amp;\frac{1}{b-a} \qquad&amp; for \ a \le x \le b \\&amp; 0  &amp; elsewhere\end{aligned}\right.\]</span> The CDF of a uniform distribution: <span class="math display">\[F(x) = \left \{ \begin{aligned}&amp; 0 &amp; x &lt; a\\&amp;\frac{x-a}{b-a} \qquad&amp; for \ a \le x \le b \\&amp; 1  &amp; x &gt; b\end{aligned}\right.\]</span> The expectation of a uniform distribution: <span class="math display">\[E[X] = \frac{a+b}{2}\]</span> The variance a uniform distribution: <span class="math display">\[V[X] = \frac{(b-a)^2}{12}\]</span></p><h3 id="normal-distribution">3.1 Normal Distribution</h3><p><img src="/2022/11/04/distribution/8.png"></p><p><strong>Story</strong></p><p>If how the certain value of a continuous random variable X is unknown, we can assume it follows a normal distribution. The parameters of a normal distribution include <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></p><p><strong>Formulation</strong></p><p>The PDF of a normal distribution: <span class="math display">\[f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span> A normal distribution does not have a closed-form CDF. It is usually obtained by simulation and approximation. Specifically, we call the denote of a <span class="math inline">\(N(0,1)\)</span> as <span class="math inline">\(\Phi\)</span>. In real application, we can search the table for the exact value.</p><p>The expectation of a normal distribution is: <span class="math display">\[E[X] = \mu\]</span></p><p>The variance of a normal distribution is: <span class="math display">\[Var[X] = \sigma^2\]</span> <strong>Properties</strong></p><ul><li>if <span class="math inline">\(X \sim N(\mu_1,\sigma_1^2), Y \sim N(\mu_2,\sigma_2^2)\)</span>, then <span class="math inline">\(X+Y \sim N(\mu_1+\mu_2,\sigma_1^2 + \sigma_2^2)\)</span></li><li>if <span class="math inline">\(X \sim N(\mu_1,\sigma_1^2)\)</span>, then $aX+b N(a+ b, (a)^2) $</li><li>if <span class="math inline">\(X_1,X_2,..X_n\)</span> all follows normal distribution, than <span class="math inline">\(X_1^2+X_2^2+...X_n^2\)</span> follows <span class="math inline">\(\chi^2(n)\)</span></li><li>For 2 normal distributed variable, it is equivalent to claim that they are uncorrelated(<span class="math inline">\(E[X_1X_2] = E[X_1]E[X_2]\)</span>) and that they are independent(<span class="math inline">\(P(X_1,X_ 2) = P( X_ 1)P( X_ 2)\)</span>)</li></ul><h3 id="exponential-distribution">3.2 Exponential Distribution</h3><p><img src="/2022/11/04/distribution/3.png"></p><p><strong>Story</strong></p><p>The Exponential Distribution describe same story as poisson distribution, exception the random variable X now donates the time interval between two occurrence of the events. The only parameter for exponential distribution is <span class="math inline">\(\beta\)</span>, where <span class="math inline">\(\beta = \frac{1}{\lambda}\)</span>, representing the probability of occurrence of the event in a unit time.</p><p><strong>Formulation</strong></p><p>The PDF of an exponential distribution: <span class="math display">\[f(x) = \left\{             \begin{array}{lr}             \lambda e^{-\lambda x} &amp; x \ge 0 \\             0 &amp; x&lt;0\\             \end{array}\right.\]</span></p><p>The CDF of an exponential distribution: <span class="math display">\[F(x) = \left\{             \begin{array}{lr}             1 -  e^{-\lambda x} &amp; x \ge 0 \\             0 &amp; x&lt;0\\             \end{array}\right.\]</span> The expectation of an exponential distribution is: <span class="math display">\[E[X] = {1\over \lambda } \]</span></p><p>The variance of an exponential distribution is: <span class="math display">\[Var[X] = {1\over \lambda^2}\]</span> <strong>Properties</strong></p><ul><li><p>Memoryless: if <span class="math inline">\(x \sim Expo(\beta)\)</span>, then: <span class="math display">\[P(T&gt;t+s|T&gt;t) = P(T&gt;s)\]</span> That is given the information of the state of the current moment, the <strong>additional time</strong> for the random event is still a <span class="math inline">\(Expo(\beta)\)</span></p></li><li><p>If <span class="math inline">\(X_1,...,X_n\)</span> are independent exponential random variables with common rate λ, then <span class="math inline">\(\sum_i^nX_i\)</span> is a $(n, λ) $random variable. If they instead follow exponential distribution with different rate <span class="math inline">\(\lambda_1,\lambda_2,...\lambda_n\)</span>, then: <span class="math display">\[P(\sum_i^nX_i) = \sum_i^nC_{i,n}\lambda_ie^{-\lambda_it}\]</span> Where: <span class="math display">\[C_{i,j} = \prod_{j\ne i} \frac{\lambda_i}{\lambda_j-\lambda_i}\]</span></p></li><li><p>Let <span class="math inline">\(X_1,X_2,...X_n\)</span> be exponentially distributed random variable with rate <span class="math inline">\(\lambda_1, \lambda_2,...\lambda_n\)</span>, then: <span class="math display">\[P(min(X_1,X_2,...X_n) = X_i) = \frac{\lambda_i}{\sum_i^n\lambda_n}\]</span></p></li><li><p>if we have independent <span class="math inline">\(X_i \sim Expo( \lambda_ i)\)</span>, then: <span class="math display">\[\begin{aligned}min(X_1,X_2,..X_k) &amp;\sim Expo(\lambda_1+\lambda_2+...+\lambda_k) \\ max(X_1,X_2,..X_k) &amp;\sim Expo(\lambda)+Expo(2\lambda)+...+Expo(k\lambda)\end{aligned}\]</span></p></li><li><p>if <span class="math inline">\(X \sim Expo(\lambda)\)</span>, then <span class="math inline">\(C*X \sim Expo({\lambda \over C})\)</span></p></li></ul><h3 id="gamma-distribution">3.3 Gamma Distribution</h3><p><strong>Story</strong></p><p>Suppose in the exponential distribution scenario, you want to observe the event for <span class="math inline">\(\alpha\)</span> times before you stop, then the time you need to do that would follows a Gamma distribution</p><p><strong>Formulation</strong></p><p>The PDF of a Gamma distribution: <span class="math display">\[f(x) = \frac{x^{\alpha-1}\lambda^\alpha e^{-\lambda x}}{\Gamma(\alpha) }\]</span></p><p><span class="math display">\[\Gamma(\alpha) = (\alpha-1)! \qquad \alpha \ is \ Z\]</span></p><p><span class="math display">\[\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1) \qquad \alpha \ is \ R\]</span></p><p><span class="math display">\[\Gamma(\frac{1}{2}) = \sqrt{\pi}\]</span></p><p>The PDF of a Gamma distribution: <span class="math display">\[F(X)= \frac{\gamma(\alpha,\beta x)}{\Gamma(\alpha)}\]</span> where <span class="math inline">\(\gamma(\alpha,\beta x)\)</span> is a lower incomplete gamma function: <span class="math display">\[\gamma(s,x) = \int_0^x t^{s-1}e^{-t}dt\]</span> The expectation of a gamma distribution is: <span class="math display">\[E[X] = \alpha \beta\]</span></p><p>The variance of a gamma distribution is: <span class="math display">\[Var[X] = \alpha \beta^2\]</span> <strong>properties</strong></p><ul><li><p>if <span class="math inline">\(X \sim \Gamma(\alpha_1,\beta), Y \sim \Gamma(\alpha_2,\beta)\)</span>, then <span class="math inline">\(X+Y \sim \Gamma(\alpha_1+\alpha_2, \beta)\)</span></p></li><li><p>When <span class="math inline">\(\alpha = 1\)</span>, a gamma distribution is equivalent to a exponential distribution</p></li></ul><h3 id="beta-distribution">3.4 Beta Distribution</h3><p><img src="/2022/11/04/distribution/4.png"></p><p><strong>Story</strong></p><p>The beta distribution is the conjugate prior distribution of a binomial distribution in a Bayesian Inference Context. Thus, it can be interpreted as the PDF of the parameter of a binomial distribution, which is the probability to success in the trail. In other world, it is a likelihood function.</p><p><strong>Formulation</strong></p><p>The PDF of a beta distribution: <span class="math display">\[f(x) = \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\]</span> where:</p><ul><li><p><span class="math inline">\(\alpha\)</span> is the number of observations of success</p></li><li><p><span class="math inline">\(\beta\)</span> is the number of observations of failure</p></li><li><p><span class="math inline">\(B(\alpha, \beta)\)</span> is a standard B function. It is applied to make the integrate of the PDF one <span class="math display">\[B(\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\]</span></p></li></ul><p>The CDF of a beta distribution: <span class="math display">\[F(x;\alpha,\beta) = \frac{B_x(\alpha,\beta)}{B(\alpha,\beta)}\]</span></p><p>where <span class="math inline">\(B_x(\alpha,\beta)\)</span> is a incomplete beta function: <span class="math display">\[B_x(\alpha,\beta) = \int_0^x t^{\alpha - 1}(1-t)^{\beta-1} dt\]</span> The expectation of a beta distribution is: <span class="math display">\[E[X] = \frac{\alpha}{\alpha + \beta}\]</span></p><p>The variance of a gamma distribution is: <span class="math display">\[Var[X] = \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\]</span> <strong>Properties</strong></p><ul><li><p>if <span class="math inline">\(X \sim \Gamma(a,\lambda), Y \sim \Gamma(b,\lambda)\)</span>, X and Y are independent, then <span class="math inline">\(\frac{X}{X+Y} \sim B(a,b)\)</span></p></li><li><p>After n trial, if x success is observed, the distribution of p is updated to <span class="math inline">\(B(\alpha+x,\beta+n-x)\)</span></p></li></ul><h3 id="chi2-distribution">3.5 <span class="math inline">\(\chi^2\)</span> Distribution</h3><p><img src="/2022/11/04/distribution/5.png"></p><p><strong>Story</strong></p><p>Suppose k independent random variable <span class="math inline">\(Z_1,..Z_ k\)</span> all follow standard normal distribution, then define random variable X as <span class="math inline">\(\sum_iZ_i^2\)</span>, then X follows a <span class="math inline">\(\chi^2\)</span> distribution. k is the only parameter of <span class="math inline">\(\chi^2\)</span> distribution, representing the number of independent variable(degree of freedom)</p><p><strong>Formulation</strong></p><p>The PDF of a <span class="math inline">\(\chi^2\)</span> distribution: <span class="math display">\[f(x) = \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})}x^{\frac{k}{2}-1}e^{\frac{-x}{2}}\]</span> The CDF of a <span class="math inline">\(\chi^2\)</span> distribution: <span class="math display">\[F_k(X)= \frac{\gamma({k \over 2},{x \over 2})}{\Gamma({k \over 2})}\]</span></p><p>The expectation of a <span class="math inline">\(\chi^2\)</span> distribution is: <span class="math display">\[E[X] = k\]</span></p><p>The variance of a <span class="math inline">\(\chi^2\)</span> distribution is: <span class="math display">\[Var[X] = 2k\]</span> <strong>Properties:</strong></p><ul><li><p>If <span class="math inline">\(\chi^2(k_1)\)</span> and <span class="math inline">\(\chi^2(k_2)\)</span> are independent, then <span class="math inline">\(\chi^2(k_1) + \chi^2(k_2) \sim \chi^2(k_1+k_2)\)</span></p></li><li><p>When k is large, a <span class="math inline">\(\chi^2\)</span> distribution is similar to a normal distribution</p></li></ul><h2 id="multivariate-distribution">4. Multivariate Distribution</h2><p>A Multivariate Distribution is the joint PDF of a vector of variables</p><h3 id="multinomial-distribution">4.1 Multinomial Distribution</h3><p>Suppose there are n items fall into k buckets, where the probabilities each items fall into the k buckets is <span class="math inline">\(\vec{p} = (p_1,p_2,...p_k)\)</span>, let <span class="math inline">\(X_i\)</span> donates the number of items fall into the <span class="math inline">\(i^th\)</span> bucket, then a vector <span class="math inline">\(\vec{X} = (X_1,X_2,...X_k)\)</span> follows a Multinomial Distribution MulNm(n,<span class="math inline">\(\vec{p}\)</span> )</p><p>The Joint PDF of a Multinomial Distribution is: <span class="math display">\[P(\vec{X} = \vec{n})=\frac{n!}{n_1!n_2!...n_k!}p_1^{n_1}p_2^{n_2}...p_ k^ {n_k}\]</span> where <span class="math inline">\(\vec{n}\)</span> is a specifc combination of numbers of items fall into each buckets:<span class="math inline">\(\vec{n} = (n_1,n_2...n_k)\)</span>, and $n_ 1+n _2+...+n_k = n $</p><p>The Expectation Vector <span class="math inline">\(\vec{ E }\)</span> would be <span class="math inline">\((np_1,np_2,...np_k)\)</span></p><p>The Variance vector <span class="math inline">\(\vec{ V }\)</span> would be <span class="math inline">\((np_1(1-p_ 1),np_2(1-p_2),...np_k(1-p_k))\)</span></p><p>The Covariance for <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> would be <span class="math inline">\(Cov(X_i,X_j) = -np_ip_j\)</span></p><h3 id="multivariate-normal-distribution">4.2 Multivariate Normal Distribution</h3><p>For a vector <span class="math inline">\(\vec{X} = (X_1,X_2,...X_k)\)</span> , if every linear combination of this vector is normally distributed, then <span class="math inline">\(\vec{ X }\)</span> follows a Multivariate Normal Distribution MulNorm(<span class="math inline">\(\vec{\mu},\vec{\sigma^2}\)</span>).</p><p>The Joint PDF of a MVN Distribution is: <span class="math display">\[f(\vec{X}) = \frac{1}{\sqrt{(2\pi)^k|\Sigma|}} e^{-\frac{1}{2}(X-\mu)^T\Sigma(X-\mu)}\]</span> Where <span class="math inline">\(|\Sigma|\)</span> is the determinant of the Covariance Matrix</p><p>The Covariance for <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_ j\)</span> is calculated by: <span class="math display">\[Cov(X_i,X_j) = E[(X_i-\mu_{X_i})(X_ j- \mu_{X_j})] = E[X_iX_j]-E[X_i]E[X_j]\]</span></p><h2 id="distribution-transformation">5. Distribution Transformation</h2><p>Suppose the PDF of a random variable x is <span class="math inline">\(f_X(x)\)</span>, define <span class="math inline">\(y = t(x)\)</span>, then: <span class="math display">\[x = t^-{1}(y)\]</span> let the CDF of X and Y be <span class="math inline">\(F_X,F_Y\)</span> <span class="math display">\[P(Y&lt;y) = P(X&lt;x)\]</span></p><p><span class="math display">\[F_Y(y) = F_X(t^{-1}(y))\]</span></p><p><span class="math display">\[f_Y(y) = F&#39;_Y(y) = (t^{-1}(y))&#39;f_X(t^{-1}(y))\]</span></p><p>With Such rules , let <span class="math inline">\(Y = F(X)\)</span> <span class="math display">\[f_Y(y) = (F^{-1}(y))&#39;f_X(F^{-1}(y)) = F(F^{-1}(y))&#39;F^{-1}(y) = F(F^{-1}(y))&#39; = y&#39; = 1\]</span> Thus, if a randam variable <span class="math inline">\(u = CDF(X)\)</span>, then $u U(0,1) $</p><table style="width:100%;"><thead><tr class="header"><th>From</th><th>Transformation</th><th>to</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(Expo(\lambda)\)</span></td><td><span class="math inline">\(Y = 1-e^{-\lambda x}\)</span></td><td>U(0,1)</td></tr><tr class="even"><td>U(0,1)</td><td><span class="math inline">\(Y=\sqrt{2}h^{-1}(2X-1)\)</span>, <span class="math inline">\(h(x) = \frac{2}{\sqrt{\pi}}\int_0^xe^{-t^ 2}dt\)</span></td><td>N(0,1)</td></tr></tbody></table><p>In many applications, we want to transform a non-normal distribution into a normal one. We know that a normal distribution does not have a closed-form CDF, we need to approximate it through error function h(x). In some case, we use a transformation to directly let <span class="math inline">\(f_Y(y)\)</span> have an approximate formulation as the target distribution. Then we define an error function to measure the loss of y and E[y] under target distribution, and train the parameters of the tranformation.</p><table><thead><tr class="header"><th>From</th><th>Transformation</th><th>to</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(Expo(\lambda)\)</span></td><td><span class="math inline">\(Y =\frac{X^\theta-1}{\theta}\)</span></td><td><span class="math inline">\(N(\mu,\sigma^2)\)</span></td></tr><tr class="even"><td><span class="math inline">\(Expo(\lambda)\)</span></td><td><span class="math inline">\(Y=log_\theta（1+X）\)</span></td><td><span class="math inline">\(N(\mu,\sigma^2)\)</span></td></tr></tbody></table><p>There are many improved transformation algorithm like Box-cox, Yeo-johnson and Box-Muller. For more transformation methods for data distribution in machine learning, refer to <a href="http://zhengyuanyang.com/2022/10/22/distribution-type/">this article</a>.</p><h2 id="a-distribution-type-and-their-properties">A: Distribution Type and their properties</h2><p><img src="/2022/11/04/distribution/7.png"></p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distribution</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Encoding Methods for Categorical Variable</title>
    <link href="/2022/10/22/encoding/"/>
    <url>/2022/10/22/encoding/</url>
    
    <content type="html"><![CDATA[<h1 id="encoding-methods-for-categorical-variable">Encoding Methods for Categorical Variable</h1><h2 id="about-encoding">1. About Encoding</h2><p>Encoding is a process that map categorical variable to numerical variable. For nost models, untransformed categorical data is not accepted as inputs. Five types of encoder introduced below.</p><h2 id="label-encoder">2. Label Encoder</h2><p>Label Encoder is one of the simplest encoding methods. It gives a label to each category</p><p>A categorical variable before encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Dog</td></tr><tr class="odd"><td>Bird</td></tr></tbody></table><p>after encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>0</td></tr><tr class="even"><td>1</td></tr><tr class="odd"><td>2</td></tr></tbody></table><p>A Label Encoder has following properties:</p><ul><li>It changes a categorical variable into a multivalued discrete variable</li><li>It does not generate extra variable, thus is memory-saving</li><li>The encoded data has a magnitude relationship, thus Label encoder should be applied to ordinal variable instead of nominal variable</li><li>It does not change the number of categories</li></ul><h2 id="one-hot-encoder">3. One-hot Encoder</h2><p>One-hot encoder expand the categorical variable into c variables, where c is number of categories. For categories, these variables are exclusive.</p><p>A categorical variable before encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Dog</td></tr><tr class="odd"><td>Bird</td></tr></tbody></table><p>after encoding:</p><table><thead><tr class="header"><th>Cat</th><th>Dog</th><th>Bird</th></tr></thead><tbody><tr class="odd"><td>1</td><td>0</td><td>0</td></tr><tr class="even"><td>0</td><td>1</td><td>0</td></tr><tr class="odd"><td>0</td><td>0</td><td>1</td></tr></tbody></table><p>For some kinds of data, the generated variables could also be non-exclusive</p><table><thead><tr class="header"><th>Color</th><th>R</th><th>G</th><th>B</th></tr></thead><tbody><tr class="odd"><td>White</td><td>0</td><td>0</td><td>0</td></tr><tr class="even"><td>Purple</td><td>255</td><td>0</td><td>100</td></tr><tr class="odd"><td>Orange</td><td>100</td><td>120</td><td>10</td></tr></tbody></table><p>A One-hot Encoder has following properties:</p><ul><li>It changes a categorical variable into c multivalued/binary discrete variables</li><li>It generates c-1 extra variable, thus is memory-costing when c is big. It suits variable with few categories</li><li>It can be applied to both ordinal and nominal variable</li><li>It does not change the number of categories</li><li>Dummy-variables and True-False encoder are very similar to one-hot encoder, only small differences exist</li></ul><h2 id="target-encoder">4. Target-Encoder</h2><p>Target encoder transform the categorical variables according to the output variable.</p><p>For numerical output, the target encoder replace categorical variable with the mean of samples under each category</p><table><thead><tr class="header"><th>X(species)</th><th>Y(Weight)</th><th>X'</th></tr></thead><tbody><tr class="odd"><td>cat</td><td>10</td><td>12.5</td></tr><tr class="even"><td>cat</td><td>15</td><td>12.5</td></tr><tr class="odd"><td>dog</td><td>20</td><td>25</td></tr><tr class="even"><td>dog</td><td>30</td><td>25</td></tr></tbody></table><p>For categorical output, the target encoder replace categorical variable with <span class="math inline">\(P(y= y_i|x=x_i)\)</span></p><table><thead><tr class="header"><th>X(species)</th><th>Y(Size)</th><th>X1(size-Small)</th><th>X2(size-medium)</th><th>X3(size-BIG)</th></tr></thead><tbody><tr class="odd"><td>cat</td><td>big</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="even"><td>cat</td><td>big</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="odd"><td>cat</td><td>medium</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="even"><td>cat</td><td>small</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="odd"><td>dog</td><td>big</td><td>0.33</td><td>0</td><td>0.66</td></tr><tr class="even"><td>dog</td><td>big</td><td>0.33</td><td>0</td><td>0.66</td></tr><tr class="odd"><td>dog</td><td>small</td><td>0.33</td><td>0</td><td>0.66</td></tr></tbody></table><p>A Target Encoder has following properties:</p><ul><li>It changes a categorical variable into some continuous variables</li><li>For continuous and binary outputs, it does not generate extra variables, for multivalued categorical output, it generate k variables, where k is the number of categories of output variable. When k &lt; c, the target encoder can be more memory-saving than one-hot encoder</li><li>It can be applied to both ordinal and nominal variable</li><li>It does not change the number of categories</li><li>There several improved target encoders like smooth target encoder and bayesian target encoder</li></ul><h2 id="frequency-encoder">5.Frequency Encoder</h2><p>The frequency encoder convert categorical variable into discrete variables by counting each category's frequency in training dataset:</p><p>A categorical variable before encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Cat</td></tr><tr class="odd"><td>Dog</td></tr><tr class="even"><td>Bird</td></tr></tbody></table><p>after encoding:</p><table><thead><tr class="header"><th>Species</th><th>X'</th></tr></thead><tbody><tr class="odd"><td>Cat</td><td>2</td></tr><tr class="even"><td>Dog</td><td>1</td></tr><tr class="odd"><td>Bird</td><td>1</td></tr></tbody></table><p>A Frequency Encoder has following properties:</p><ul><li>It changes a categorical variable into a discrete variable</li><li>It does not generate extra variables, thus is memory-saving</li><li>The might be collision of variables, and change the number of categories, thus this endocing method does not fit small dataset</li><li>There would be magnitude in transformed variables</li></ul><h2 id="binary-encoder">6. Binary Encoder</h2><p>Binary Encoder use $log_2N $ variables to express the original variable with N categories</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Dog</td></tr><tr class="odd"><td>Bird</td></tr><tr class="even"><td>Snake</td></tr></tbody></table><p>A variable with four categories can be expressed in a 2-dimension vector</p><table><thead><tr class="header"><th>Species</th><th>X1</th><th>X2</th></tr></thead><tbody><tr class="odd"><td>Cat</td><td>0</td><td>0</td></tr><tr class="even"><td>Dog</td><td>0</td><td>1</td></tr><tr class="odd"><td>Bird</td><td>1</td><td>0</td></tr><tr class="even"><td>Snake</td><td>1</td><td>1</td></tr></tbody></table><p>The binary encoder has similar properties as One-hot Encoder, but:</p><ul><li>It saves more memory</li><li>The generated variables is less interoperable</li></ul><h2 id="hash-encoder">7. Hash Encoder</h2><p>The Hash Enocder map the original variable into a low-dimension space, and use the length of hash bin as transformed values. It is usually applied in a text processing scenario.</p><p>A text variable before encoding:</p><table><thead><tr class="header"><th>Message</th></tr></thead><tbody><tr class="odd"><td>I love python python is good</td></tr><tr class="even"><td>I dont like python</td></tr><tr class="odd"><td></td></tr></tbody></table><p>A text variable after encoding:</p><table><thead><tr class="header"><th>text</th><th>I</th><th>love</th><th>Python</th><th>is</th><th>good</th><th>dont</th><th>like</th></tr></thead><tbody><tr class="odd"><td>I love python python is good</td><td>1</td><td>1</td><td>2</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr class="even"><td>I dont like python</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr class="odd"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>A Hash Encoder has following properties:</p><ul><li>It changes a categorical variable into some discrete variables</li><li>Comparing to One-hot Encoder, it saves memory when the original variable is complex and repeatable, like text and graph</li><li>The might be collision of variables, and change the number of categories, thus this endocing method does not fit small dataset</li></ul><h2 id="embedding-encoder">8. Embedding Encoder</h2><p>Embedding methods is a techniques transforms the original categorical variable into a vector that reflect the similarity of the original categories. It is more frequently used in deep learning scenarios like NLP. Generally speaking, it can be regraded as a kind of encoding methods.</p><p>[ongoing]</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Encoding</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Normality Test and Distribution Type Transformation</title>
    <link href="/2022/10/22/Normality_Transformation/"/>
    <url>/2022/10/22/Normality_Transformation/</url>
    
    <content type="html"><![CDATA[<h1 id="normality-test-and-distribution-type-transformation">Normality Test and Distribution Type Transformation</h1><h2 id="normality-and-distribution-type-transformation">1. Normality and Distribution Type Transformation</h2><p>In feature engineering, sometimes we need to transform the distribution type to make the data fit the model. In most scenarios, we want to transform a non-normal distributed variable into a normal distributed variable.</p><h2 id="normality-test">2. Normality Test</h2><p>Before transformation, we need first to examine the normality of the data to decide which features to transform.</p><h3 id="visualized-method">2.1 Visualized Method</h3><p><strong>Histogram</strong></p><p>The histogram is one of the simplest method to test normality. If the histogram are bimodal cureve or a trailing curve, then there is a normality problem</p><p><strong>P-P Plot and Q-Q Plot</strong></p><p>P-P plot use the cumulative frequency of the sample as a X -axis, the cumulative probability of the variable under a normal distribution assumption as a Y-axis</p><p><img src="/2022/10/22/Normality_Transformation/1.png"></p><p>If the points in the graph are approximately around a line from (0,0) to (1,1), then sample reflect a normal distribution. Otherwise, there is a normality problem</p><p>The Q-Q Plot is a variant of P-P Plot. The only difference is the axis is now the quantile instead of the cumulative probability.</p><h3 id="hypothesis-testing-method">2.2 Hypothesis Testing Method</h3><p><strong>Skewness and Kurtosis</strong></p><p>Skewness and Kurtosis are both statistics.</p><p>Skewness is the standardized 3rd central moment of the sample: <span class="math display">\[S = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x}^3)/s^3\]</span> The kurtosis is the standardized 4rd central moment. In calculation we usually minus 3 to let the kurtosis of normal distribution be 0 <span class="math display">\[K = \frac{1}{n-1}\frac{\sum_{i=1}^n(x_i-\bar{x}^4)}{s^4}-3\]</span> where s is the standard deviation</p><p>When the variable folllows a normal distribution, both statistics should be 0. Thus we can use a hypothesis testing on these two parameters two decide normality. Let <span class="math inline">\(SE_S, SE_K\)</span> denote the standard error: <span class="math display">\[SE_S = \frac{6N(N-1)}{(N-2)(N+1)(N+3)}\]</span></p><p><span class="math display">\[SE_K = \frac{4(N^2-1)SE_s}{(N-3)(N+5)}\]</span></p><p>Construct Z score :<span class="math inline">\(Z_S = \frac{S}{SE_s}, Z_K = \frac{K}{SE_K}\)</span>. Given <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, we can do a Z test to decide normality.</p><p>A improved version of testing based on Skewness and Kurtosis is <strong>D'Agostino's K-squared test</strong>, which construct a <span class="math inline">\(\chi^2\)</span> test statistics by combining skewness and kurtosi. It is a common and powerful method for normality test.</p><p><strong>K-S Testing</strong></p><p>Kolmogorov–Smirnov test is method to test if a empiric distribution is different from a theoretical distribution. In other word, if the variable distributed as we want.</p><p>Let <span class="math inline">\(F_n,F_0\)</span> respectively represent the CDF of the empiric distribution and the theoretical distribution. n is the sample size. Construct test statistics as <span class="math display">\[D_n = sup_x \ |F_n(x) - F_0(x)|\]</span> where sup represent the supremum. If <span class="math inline">\(F_n = F_0\)</span> , the limit of <span class="math inline">\(D_n\)</span> should be zero. Given significance level <span class="math inline">\(\alpha\)</span>, find <span class="math inline">\(t_\alpha\)</span> such that <span class="math inline">\(P( \sqrt{n}D_n \ge t_\alpha) = \alpha\)</span>, the rejection area then would be <span class="math inline">\([\frac{t_\alpha}{\sqrt{n}},+\infty)\)</span></p><p>In normality test, we can use K-S test to decided whether the variable follows a normal distribution by replacing supreme with maximum. <span class="math inline">\(F_n,F_0\)</span> would be the observed cumulative frequency of sample and cumulatively probability under normal distribution assumption</p><p><strong><span class="math inline">\(\chi^2\)</span> Test for fitness</strong></p><p>The <span class="math inline">\(\chi^2\)</span> Test for fitness works similarly like the K-S test in normality test. Construct the <span class="math inline">\(\chi^2\)</span> test statistics with observed cumulative frequency of sample and cumulatively probability. <span class="math inline">\(\chi^2\)</span> Test for fitness is easier when the test statistics are discrete. For continuous variable, we need to discretize the variable</p><h2 id="distribution-type-transformation">3. Distribution Type Transformation</h2><h3 id="monte-carlo-integral">3.1 Monte Carlo Integral</h3><p>A theoretical method to inverse the CDF of the variable and transform the distribution into a normal distribution, for specific details, refer to <a href="http://zhengyuanyang.com/2022/11/04/distribution/#distribution-transformation">this article</a></p><p><strong>Box-Muller</strong></p><p>A simpler way to obtain normal-distributed vairable from uniformly distributed variable is the Box-Muller. Given two uniformly distributed variables <span class="math inline">\(U_1, U_2\)</span>: <span class="math display">\[Z_1 = \sqrt{-2lnU_1}cos(2\pi U2)\]</span></p><p><span class="math display">\[Z_2 \ \sqrt{-2 lnU1} sin(2\pi U2)\]</span></p><p>It can be proved that <span class="math inline">\(Z_1, Z_2 \sim N(0,1)\)</span>. Through this method, we can transform pairwise variable without knowing the error function of the single variable. However, the CDF of both variables should still be known.</p><p>In real application, it is difficult to obtain the accurate cumulative density function. Thus, MC method only works well when the CDF of the variable is given in a closed form. In most scenarios, we would use other transformation to approximate a normal distribution</p><h3 id="log-transformation">3.2 Log Transformation</h3><p>We can try to approximate normal distribution through log transformation: <span class="math display">\[X&#39; = log_a(X+k)\]</span> Where a and k are parameterswe can adjust. Greater a has stronger power to correct skewness. Usually we would try <span class="math inline">\([e, 10]\)</span>. K is a positive correction term ensuring there's no negative, zerr or extreme small values in the data.</p><h3 id="power-transformation">3.3 Power Transformation</h3><p>We can try to approximate normal distribution through power transformation: <span class="math display">\[X&#39; = X^a\]</span> when the vairable is a ratio, we can also add a arcsin transformation: <span class="math display">\[X&#39; = arcsin(X^a)\]</span> Improved power transformer includes:</p><p><strong>Box-Cox</strong> <span class="math display">\[X&#39; = \left\{\begin{aligned}\frac{X^\lambda-1}{\lambda} \qquad&amp;  \lambda \ne0\\ln(X) \qquad &amp; \lambda = 0 \\\end{aligned}\right.\]</span> <strong>Yeo-Johnson</strong></p><p><span class="math display">\[X&#39; = \left\{\begin{aligned}&amp;\frac{(X+1)^\lambda-1}{\lambda} \qquad&amp;  \lambda \ne0, X\ge 0\\&amp;ln(X)+1 \qquad &amp; \lambda = 0, X \ge 0\\&amp;\frac{-[(-X+1)^{2-\lambda}-1]}{2-\lambda} \qquad&amp; \lambda \ne 2, X&lt;0\\&amp;-ln(-X+1) \qquad&amp;\lambda=2, X&lt;0\end{aligned}\right.\]</span> where<span class="math inline">\(\lambda\)</span> is a parameter that can be optimized through Maximum Likelihood Estimation(MLE). The Box-Cox transformer requires X to be strictly positive, while the Yeo-Johnson transformer can accept any numbers.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distribution Type Transformation</tag>
      
      <tag>Feature Engineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Scaler in Machine Learning</title>
    <link href="/2022/10/21/scaler/"/>
    <url>/2022/10/21/scaler/</url>
    
    <content type="html"><![CDATA[<h1 id="scaler-in-machine-learning">Scaler in Machine Learning</h1><h2 id="about-scaling">1. About Scaling</h2><p>When adopting multiple variables, many models would be effected by the difference of scales of variables. For example, distance-based clustering would enlarge the impact of variables with larger scale. Another example is that Gradient D Descending is harder to converge on unscaled data. Data Scaling can significantly improve the performance and efficiency of some models, while on other models, like tree model or Naive Bayes, data scaling does not make an influence.</p><p>​</p><h2 id="scaler">2. Scaler</h2><p>Scaler is those algorithm used to scaling data. They usually do not change the type of the data distribution. Four types of sclaer are listed as follows:</p><h2 id="standard-scaler">2.1 Standard Scaler</h2><p>The Standard Scaler does the following transformation <span class="math display">\[x&#39; = \frac{x-\bar{x}}{s}\]</span> where</p><ul><li><p><span class="math inline">\(\bar{x}\)</span> is the average of the original data</p></li><li><p>s is the standard deviation of the original data</p></li><li><p>x' is the Z-score of thr original data.</p></li></ul><p>The Standarad Scaler has following properties:</p><ul><li><p>The original data should fit a normal distribution. This is not a necessary hypothesis but the scaler <strong>works better if the original data has strong normality.</strong> After the transformation, the data follows N(0,1)</p></li><li><p>If the original data is not normally distributed, the standard sclater will not change it's type of distribution. In such case, consider distribution transformation methods before applying standard scaler.</p><p><img src="/2022/10/21/scaler/1.PNG"></p></li><li><p>The <strong>range of transformed data is not for fixed</strong>. But most data points should lie in [-3,3].</p></li><li><p>The <strong>Outlier has an significant influence on the scaling</strong>. Consider applying outlier detection method before scaling</p></li></ul><h3 id="minmax-scalermean-normalization">2.2 MinMax Scaler/Mean Normalization</h3><p>The MinMax Scaler does the following transformation <span class="math display">\[x&#39; = \frac{x-x_{min}}{x_{max}-x_{min}}\]</span> The MinMax Scaler has following properties:</p><ul><li>The MinMax scaler is a linear transformation, it does not change the type of distribution, and <strong>it does not have hypothesis on data distribution</strong></li><li>The <strong>range of data after transformation would be [0,1]</strong></li><li>The Outlier has an significant influence on the scaling. Consider applying outlier detection method before scaling</li></ul><p>A variant of MinMax Scaler is the Mean Normalization, which is given as: <span class="math display">\[x&#39; = \frac{x-\bar{x}}{x_{max}-x_{min}}\]</span> The Mean Normalization has similar properties as MinMax Scaler, but the range of transformed data would be [-1,1]</p><h3 id="robust-scaler">2.3 Robust Scaler</h3><p>The Robust Scaler does the following transformation <span class="math display">\[x&#39; = \frac{x-x_{median}}{IQR}\]</span> where IQR is the difference of 75% percentile and 25% percentile</p><p>The MinMax Scaler has following properties:</p><ul><li>The Robust Scaler does not change the type of distribution, and <strong>it does not have hypothesis on data distribution</strong></li><li>The Robust Scaler reduce the influences of the outlier. If the outlier cannot be eliminated due to some reasons, we can consider this scaling method</li><li>The range of the transformed data is not fixed</li></ul><h3 id="l1l2-scaler">2.4 L1/L2 Scaler</h3><p>The L1/L2 Scaler does the following transformation <span class="math display">\[x_i&#39; = \frac{x_i}{|x|}\]</span></p><p><span class="math display">\[x_i&#39; = \frac{x_i }{||x ||}\]</span> where</p><ul><li>x is a feature vector: {<span class="math inline">\(x_1,x_ 2,...x_ n\)</span>}</li><li><span class="math inline">\(|x|,||x||\)</span> is the L1, L2 norm of the feature vector</li></ul><p>The L1/L2 Scaler has following properties:</p><ul><li>The L /L2 Scaler is a sample-level transomation, it can be applied on a single sample</li><li>The range of the transformed data is [-1,1]</li><li>The parameters of the distribution of the transformed data is hard to predict, since the calculated norm of each sample is different</li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Scaler</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic Procedure of A/B Testing</title>
    <link href="/2022/10/17/AB-Test/"/>
    <url>/2022/10/17/AB-Test/</url>
    
    <content type="html"><![CDATA[<h1 id="basic-procedure-of-ab-testing">Basic Procedure of A/B Testing</h1><h2 id="about-ab-testing">About A/B Testing</h2><p>A/B testing is a user research technology. It is a random controlled experiment usually involves two variants: treatment group and control group. From a hypothesis perspective, an A/B test is a two-sample independent test for difference.</p><h2 id="step1-problem-statement">Step1: Problem Statement</h2><p>An A/B test should always have an business drive. This can usually be concluded into several product sense framework like AAARR and STAR</p><p><a href>ongoing</a></p><h2 id="step-2-pre-experiment-analysis">Step 2: Pre-experiment Analysis</h2><p>The purpose of pre-experiment analysis is to figure out an specific solution for the problem stated, in other words, the treatment. Common techniques include EDA, user segmentation and machine learning.</p><p>In a A/B testing case interview, the treatment are usually provided by the interviewer. The task for the interviewee would be to infer the business drive beneath the treatment and find according metrics to measure success.</p><h2 id="step-3-metrics-selection">Step 3: Metrics Selection</h2><p>This task is very similar to another type of case <a href>"measure success"</a>. However, for an A/B test, the following facts about a metric should be considered:</p><ul><li>Sensitivity: The metrics should be sentitive and timely. Avoid metrics that needs a long time to demonstrate changes.</li><li>Attributable: Avoid those metrics associated with high-level business goal. These metrics are impacted by many facotrs. They can be insensitive to a particular experiment or be hard for us to attribute the significance on a single treatment.</li><li>Aggregated: The metric should be defined on an aggregation of a measure that is meaningful to every unit. Only by doing that can be regard the meaure on a single unit as a random variable and apply CLT.</li><li>Measurable : The metrics should be clearly defined and measurable. They should not exceed the time or space frame of the experiment</li></ul><h2 id="step-4-experiment-design">Step 4: Experiment Design</h2><h3 id="define-experiment-groups">Define Experiment Groups</h3><p><strong>Randomized Unit</strong></p><p>The randomized unit is the smallest grain that the metrics defined on. In most applications, the randomized unit is a user. Options for finer grains exists:</p><ul><li>Page Level: the smallest unit is a page view</li><li>Session Level: the smallest unit is a group of web page viewed in a single visit</li><li>User Level: the smallest unit is a user. The minimal grain of the metrics should be associated with a user ID, device ID or coockies, such as a UV. Shared account/device can lead to undercounting, while multiple account/device by one user can lead to overcounting. Cookies can be erased, leading to overcounting of users</li></ul><p>Finer grain can increase the number of available units and lower the varianve. Nevertheless, do not use finer grain if the objective of the feature is meant for the user level, such as user experience or user engagement. The analysis unit and the randomized unit is not on the same level if we do so. This can lead to Violation of SUTVA and lead to misinterpretation.</p><p>For example we want to know the probability a user clicked a new button. If we use CTR, based on a PV, then let n denote the number of users, <span class="math inline">\(K_i\)</span> denote the number of PV by the user i. Let <span class="math inline">\(N =\sum_iK_i\)</span> denote the total number of PV, and <span class="math inline">\(X_{i,j}\)</span> be a bernoulli variable denoting whether the user i click the button in his <span class="math inline">\(j^{th}\)</span> PV(suppose a user can only click the button once).</p><p>The CTR is defined as: <span class="math display">\[CTR = \frac{\sum_i\sum_jX_{i,j}}{N}\]</span> This definition is logically right, however, if the real randomization strategy is applied on user, not PV, then there is a violation of SUTVA. The randomization strategy ensures the user are exposed with the treatment randomly, not the PV. The behavior from the same user in different PV can be not independent. In other word, when i is fixed, <span class="math inline">\(X_{i, 1},X_{i, 2},...X_{i, j}\)</span> can be correlated. This lead to mismatch of randomization unit and analyzing unit. In this case, we cannot calculate the variance of CTR using the i.i.d assumption. The variance can then be incorrectly estimated and lead to a misinterpretation in the experiment.</p><p>Insteadly, we can aggregate the metric in to the average Click Through Probability: <span class="math display">\[average \ CTP = \sum_i^n\frac{ \frac{\sum_j^{K_i} X_{i,j}}{K_i} }{n}\]</span> It first aggregate the metrics to the probability that a single user click through <span class="math inline">\(\frac{\sum_j^{K_i} X_{i,j}}{K_i}\)</span> and use it as the analysis unit, which is consistent with the randomization strategy. According to the CLT, this variable should follow a normal distribution, and user are randomized, thus the CTP for each user should be i.i.d. We can then estimate the standard error under the i.i.d assumption.</p><p><strong>Target Group</strong></p><p>The target population the feature meant for. This is usually a subset of all users conditioned on the following dimensions:</p><ul><li>Region: Country, Province</li><li>New/Old User</li><li>Language: English, Chinese</li><li>Industry: Cloth/Furniture/...</li><li>Conversion Stage: We should focus on the users that the feature are meant to perform on. If a user have not yet gome through the user funnel and do not hold a chance to experience the feature, they should not be selected for the experiment</li></ul><h3 id="from-historical-data">From Historical Data</h3><p>Some parameters need to gathered from historical data:</p><p><strong>Baseline of Metrics</strong>: The historical level of metrics on control group and treatment group. The value should be the same(or very close). If there's a difference, conduct a pre-difference analysis(A/A test, metric decomposition)</p><p><strong>Variance</strong>: The estimation of variance on the target population. Since we have not calculate the sample size and split the samples into control and treatment group, it is difficult to estimate the vairiances for two different population from samples. A solution is to assume the variance of both population are the same and estimate the variance through the variance of all samples. If there are any histroical split, like a previous A/B test or A/A test, we can consider estimate both variance.</p><h3 id="from-business-goal">From Business Goal</h3><p><strong>Practical Significance Boundary</strong></p><p>The minimal extent of difference of interest from a business aspect. Usually the researcher first define a minimal relative difference(MRD, usually 1%~5%), and calculate PSB through baseline*MRD. The PSB is taken as an expected MDE to calculate the minimum sample size</p><h3 id="for-hypothesis-testing">For Hypothesis Testing</h3><p>The details of the hypothesis testing can be found in <a href="http://zhengyuanyang.com/2022/03/25/Hypothesis-Testing/#formula">this article</a>.</p><p><strong><span class="math inline">\(\alpha \ and \ \beta\)</span></strong></p><p>Decide the significance level and statistical power. Usually, we set <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(\beta = 0.8\)</span> as an initial value</p><p><strong>Minimum Sample Size</strong></p><p>The sample size is decided throguh experience formula. For A/B testing:</p><p>A mean test: <span class="math display">\[n = \frac{(\sigma_1^2+\sigma_2^2)(z_{\alpha/2}+z_{\beta})^2}{\delta^2}\]</span> A ratio test: <span class="math display">\[n = \frac{Z_{\alpha/2}\sqrt{\frac{p_1+p_2}{2}(1-\frac{p_1+p_2}{2})}+Z_\beta\sqrt{p_1(1-p_1)+p_2(1-p_2)}}{p_1-p_2}\]</span> In real applications, for a mean test, if we feel like the respective variances are hard to obtian or we can't decide whether we would apply a Z-test, we can simply apply the <strong>rule of thumb</strong>: <span class="math display">\[n = 16\frac{s^2}{\delta^2 } \qquad for \quad  two-tail\]</span></p><p><span class="math display">\[n = 8\frac{s^2}{\delta^2 } \qquad for \quad  one-tail\]</span></p><p><strong>Test Duration</strong></p><p>The longer the experiment runs, the more sample we can obtained. Usually the sample size can be calculated from: <span class="math display">\[Days \ of \ test = \frac{ Minimum \ sample \ size}{Daily \ available \ unit \ traffics}\]</span></p><p>However, the following aspects need to be considered when deciding the test duration</p><ul><li><strong>Day of week effect:</strong> Weekends might lead to different user behavior had raise up the metrics. It's important to ensure that your experiment capture the weekly cycle</li><li><strong>Seasonality:</strong> Holidays or annual events that can have a impact on the metrics. Try to avoid or correct the effect of these events if the test are for plain time</li><li><strong>Primacy and novelty effects:</strong> Some existing users might tend to be reluctant to change(Primacy Effect), while other existing users might tend to welcome change. Both effects will diminish as time growing. To ensure the stability of the test, the duration should cover the period when these two effects are working</li></ul><p><strong>Experiment Type</strong></p><p>An A/B testing is an test to examine differential of metrics between control group and treatment group. Thus, in most case we selecte a T/Z test.</p><p>For an A/B testing, the experiment should be a two-sample independent test. If the <span class="math inline">\(\sigma\)</span> of the population is unknown, we should consider a T test. If the <span class="math inline">\(\sigma\)</span> is known, for example it is a ratio test, we should apply a Z-test if available samples are more than 30, and a T test if the number is less than 30.</p><p><img src="/2022/10/17/AB-Test/1.png"></p><p><strong>Construct Test Statistics</strong></p><p>The construction of test statistics is decided by the type of experiment and the assumption that whether the variance of the two group are the same. We can consider examining the equality of variance through a F-test based on the experiment samples or any historical data.</p><h2 id="step-5-experiment-deployment">Step 5: <strong>Experiment Deployment</strong></h2><p><strong>Traffic Distribution</strong></p><p>As an A/B test is a random controlled experiment, the randomness of assignment of units must be ensured. The simplest method to ensure concurrent experiments for different tratments do not influence eanch other is the Simple Layer Method, which is to divide all traffic with each experiment variant receiving a specified fraction of the total traffic:</p><p><img src="/2022/10/17/AB-Test/2.png"></p><p>Nevertheless, this method requires that the total needed sample size for all concurrent experiments does not exceed the total available traffics. This put limits on the number of concurrent experiments</p><p>To handle with numerous concurrent experiments cases, we can apply multiple-layer orthogonal traffic method, where each layer is a copy of all or a fraction of total traffic. The re-assignment of a unit accross two layers are random. The groups of experiments are allocated on this layers and each group are expected to receive units randomly from all groups of the above layer.</p><p><img src="/2022/10/17/AB-Test/3.png"></p><p><strong>When to Stop</strong></p><p>The typical rule for experiment stopping is <strong>No Data Peeking</strong>: wait until the experiment reach its duration and then analyze the results. However, in real applications, we might need to monitor the experiment and decide whether to end an experiment after the duration is reached. When the duration is reached:</p><p><img src="/2022/10/17/AB-Test/4.png"></p><h2 id="step-6-experiment-results-analysis">Step 6: Experiment Results Analysis</h2><p>The analysis process of the experiment results can be found in <a href>this article</a></p><h2 id="step-7-lauch-decision">Step 7: Lauch Decision</h2><p>After we are assured that the effect of the treatment is significant and truthworthy, there's still some aspects we need to consider before we deciding to lauch the new feature</p><p><strong>Metrics Trade-off</strong></p><p><strong>Lauch Costs</strong></p><p><strong>Rampling Process</strong></p>]]></content>
    
    
    <categories>
      
      <category>A/B Testing &amp; Case Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>A/B Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distance in Machine Learning</title>
    <link href="/2022/10/08/distance/"/>
    <url>/2022/10/08/distance/</url>
    
    <content type="html"><![CDATA[<h1 id="distance-in-machine-learning">Distance in Machine Learning</h1><h2 id="euclidean-distance">1. Euclidean Distance</h2><p>The Euclidean distance between two data point X and Y is given as: <span class="math display">\[d = \sqrt{\sum(x_i-y_i)^2}\]</span></p><ul><li><p>E Distance is one of the most frequently used distances in machine learning, for its direct and intuitive demonstration.</p></li><li><p>E Distance treat all dimensions as same, thus would be effected by scale of variable. To eliminate that, we can used standard euclidean distance, by replace data values x with: <span class="math display">\[x&#39; = \frac{x-\bar{x}}{s}\]</span></p></li></ul><h2 id="manhattan-distance">2. Manhattan Distance</h2><p><img src="/2022/10/08/distance/MD.png"></p><p>In this demo, the euclidean distance between the beginning and the end in a district shown as the is the green line. However, as we cannot walk through the building straightforward, we must move vertically or horizontally along the blocks. This distance is called Manhattan distance.</p><p>The manhattan distance is the sum of <strong>axis difference</strong> between two data points. It is given as: <span class="math display">\[d = \sum_i^m |x_i-y_i|\]</span></p><ul><li>The manhatten distance only involves summing calculation. So it is faster then float calculation, and it does not generate accuracy lost.</li></ul><h2 id="chebyshev-distance">3. Chebyshev Distance</h2><p><img src="/2022/10/08/distance/2.png" alt="0.5" style="zoom:50%;"></p><p>The manhattan distance is the maxnium of <strong>axis difference</strong> between two data points. It is given as: <span class="math display">\[D = max |x_i-y_i|\]</span></p><h2 id="minkowski-distance">4.Minkowski Distance</h2><p>Minkowski Distance is the distance between two points in a p norm space <span class="math display">\[d = (\sum|x_i-y_i|^p)^\frac{1}{p}\]</span></p><ul><li>when p = 1, MK distance equals Manhattan Distance</li><li>when p = 2, MK distance equals Euclidean Distance</li><li>when p = <span class="math inline">\(\infty\)</span>, MK distance equals Chebyshev Distance</li></ul><p>The norm p is a hyper-parameter can be adjusted. It is more flexible, but choosing p can be difficult</p><h2 id="mahalanobis-distance">5. Mahalanobis Distance</h2><p>Mahalanobis Distance is the Euclidean distance in a standard principal component space(Space after PC decomposition). <span class="math display">\[D(X) = \sqrt{(X-\mu)^TS^{-1}(X-\mu)}\]</span> where:</p><ul><li>X is the feature vector of a sample</li><li>S is the covariance matrix of all features</li><li><span class="math inline">\(\mu\)</span> is the mean vector of all samples</li></ul><p>The M distance between two data point can be given as: <span class="math display">\[D(X,Y) = \sqrt{(X-Y)^TS^{-1}(X-Y)}\]</span></p><ul><li>The M distance is not effcted my scale of data, and it can avoid the correlation among features. Thus it can be a better metrics for measuring distance between data point and central point</li><li>The M distance is usually applied in outlier detection, combined with Grubb's Test</li><li>when the covariance matrix is a identity matrix, which means the distributions of features are independent to each other, the M distance equals to the E distance</li></ul><p><img src="/2022/10/08/distance/4.png"></p><p><img src="/2022/10/08/distance/5.png"></p><h2 id="cosine-distance">6. Cosine Distance</h2><p>The Cosine Distance uses cos of the inner angle of two vectors to represent their distance <span class="math display">\[d = 1- cos(\theta) = 1- \frac{a\cdot b}{|a|*|b|}\]</span></p><ul><li><p>The rule of cosine similarity of two vector is "according -&gt; 1, Orthogonal -&gt;, inverse -&gt; -1". Such rule remains same even when the dimension is extremely high. Since the Euclidean distance could be effected by the number of dimensions, consine distance can be a better measure when it comes to high-dimension data. FOr example, a long sentence and a short sentence share a same meaning can have large euclidean distance, but small cosine distance</p></li><li><p>cosine distance is not a strictly defined distance for mathematics, as it does not satisfy triangle inequality</p></li></ul><h2 id="jaccard-distance">7. Jaccard Distance</h2><p>The Jaccard Distance uses Jaccard similarity to represent distance: <span class="math display">\[d = 1-J(A,B) = 1-\frac{|A \cap B|}{|A \cup B|}\]</span> Where J(A,B) is the ratio of two set's intersection and their union</p><ul><li>Jaccard distance can represent the similarity of two set instead of to points. For example, it can measure the similarity of two customer's shopping records. It can be applied in user segmentation and recommendation</li></ul><h2 id="string-related-distance">8. String Related Distance</h2><p><strong>Hamming Distance:</strong> For two string s1 and s2, the hamming distance is the minimun times needed to transform s1 to s2 by replacing a single character</p><p><strong>Edit Distance</strong>：For two string s1 and s2, the edit distance is the minimun times needed to transform s1 to s2 by adding,deleting or replacing a single character</p><h2 id="other-measure-similar-to-distance">9. Other Measure similar to Distance</h2><p>Some measures, just like cosine distance, are not strictly defined distance by mathematics, but can take effects in a similar way, including:</p><ul><li>Correlation Distance(for two variables)</li><li>KL divergence(for two distributions)</li><li>Mutual Information(for two variables)</li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distance</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Outlier Detection in Feature Engineering</title>
    <link href="/2022/10/07/outlier/"/>
    <url>/2022/10/07/outlier/</url>
    
    <content type="html"><![CDATA[<h1 id="outlier-detection-and-processing">Outlier Detection and Processing</h1><h2 id="about-outlier">1. About Outlier</h2><p><strong>What is an outlier</strong></p><p>In statistics, an outlier is a data point that significantly differs from other observations.</p><p><strong>Why we need outlier detection? </strong></p><p>Outliers can lead to bias in machine learning. Many models, especially those have strong hypothesis on data distribution(LR,K- Means) would be influenced, or even be unable to converge if outliers exist in training data. Some business and engineering problem, like fraud detection and quality control, are also a outlier detection problem in nature. For this article, we only look to the outlier detection for feature engineering.</p><h2 id="statistical-method">2. Statistical Method</h2><h3 id="one-dimension-method">2.1 One-dimension Method</h3><h4 id="six-sigma-method">2.1.1 Six <span class="math inline">\(\sigma\)</span> Method</h4><p>Given a <strong>one-dimension</strong> variable X that satisfies <strong>Gaussian Distribution</strong>: <span class="math inline">\(X~N(\mu,\sigma)\)</span>, where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> is the mean and standard error of the sample:</p><p>If a point <span class="math inline">\(x \notin (\mu-3\sigma,\mu+3\sigma)\)</span>, then x can be flagged as an outlier</p><h4 id="grubbs-test">2.1.2 Grubb's Test</h4><p>Grubbs' Test is a kind of hypothesis testing that based ib the following hypothesis:</p><p><span class="math inline">\(H_0\)</span>: There are no outlier in the observations</p><p><span class="math inline">\(H_1\)</span>: There are only one outlier in the observations</p><p>There are several constraints for Grubb's Test:</p><ul><li>The Grubb's test can detect <strong>one outlier in a single test</strong>. If we want to test all data point, we can replace G statistics with z-score to test all samples. If we belive there are a large number of outliers exsits, we should consider other method.</li><li>To apply G test, the variable must follow a <strong>Gaussian Distribution</strong></li><li>The test variable must be <strong>univariate</strong>, if we want to apply the test on multivariate vector, we can calculate the distance between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\bar{x}\)</span>, and use the distance d as the test variable</li></ul><p>Define statistics G as: <span class="math display">\[G = \frac{\max|X_i - \bar{X}|}{s}\]</span> which is the z-score of the farthest data point</p><p>If: <span class="math display">\[G &gt; \frac{N- 1}{\sqrt{N}}\sqrt{\frac{t_{\frac{\alpha}{2N},N-2}^2}{N-2+t_{\frac{\alpha}{2N},N-2}^2}}\]</span></p><p>Where:</p><ul><li>N is the sample size</li><li>t represents the t score, suppose the orignal dataset can be trasformed to a t distribution</li><li><span class="math inline">\(\frac{\alpha}{2N}\)</span> is the significance level (If it's a single-side test, then <span class="math inline">\(\frac{\alpha}{N}\)</span>)</li><li>N-2 is the degree of freedom</li></ul><p>Then maxnium/minimum of these data points can be flagged as an outlier</p><h4 id="boxplot">2.1.3 BoxPlot</h4><p>Boxplot is depicted as follows:</p><p><img src="/2022/10/07/outlier/2.png"></p><p>It shows the maximun, 75 percentile, median, 25 percentile and minimum of a dataset.</p><p>Note that the maxinum and minimum are not the original values of the data, instead it's calculated as: <span class="math display">\[Max = \min(M,Q_3+1.5IQR )\]</span></p><p><span class="math display">\[Min = \max(m,Q1-1.5IQR)\]</span></p><p>where：</p><ul><li>M,m is the actual maximum, minimum of the dataset</li><li>Q1,Q3 is the 25,75 percentile of the dataset</li><li>IQR is the intermediate quantiles range of the data, <span class="math inline">\(IQR = Q3-Q1\)</span>, which is the length of the box</li><li>if the actual maximum is smaller than the calculated maximum, then plot with the actual maximun, and there are no outliers. The same apply to minimun</li><li>The data points outside the range of calculated minimum and maximum would be labeled as outliers</li></ul><p><img src="/2022/10/07/outlier/3.png"></p><h3 id="high-dimension-method">2.2 High-dimension method</h3><h4 id="chi2-test">2.2.1 <span class="math inline">\(\chi^2\)</span> Test</h4><p>[ongoing]</p><h4 id="gaussian-distribution-detection">2.2.2 Gaussian Distribution Detection</h4><p>The P value for a high-dimenstion data point is quiet difficult to calculate. We can instead set a threshold <span class="math inline">\(\epsilon\)</span>, suppose a data set <span class="math inline">\(D = \{ x^{(i)} : 0\le i \le m \}\)</span> ,let <span class="math display">\[\mu = \frac{1}{m}\sum_i^m x^{(i)},\sum=\frac{1}{m}\sum(x^{(i)}-\mu)(x^{(i)}-\mu)^T\]</span></p><p><span class="math display">\[p(x) = \frac{1}{2\pi^\frac{n}{2}|\sum|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\]</span></p><p>if <span class="math inline">\(p(x) &lt; \epsilon\)</span>, the data point can be regard as an outlier</p><h2 id="distance-based-method">3. Distance-based Method</h2><p>For details of distance metrics, refer to <a href="http://zhengyuanyang.com/2022/10/08/distance/">this article</a></p><h3 id="angle-based-method">3.1 Angle-based Method</h3><p><img src="/2022/10/07/outlier/1.png"></p><p>From this graph it is obvious to discover that the inner angle consists of an outlier and any two other points have almost same size. Thus we can define the Angle-based outlier factor as: <span class="math display">\[ABOF(o) = VAR_{x, y \ne o}[\frac{\vec{ox}\cdot \vec{oy}}{dist(o,x)^2dist(o,y)^2}]\]</span> where:</p><ul><li>o is any points in the dataset</li><li>x, y is all other point in the dataset</li><li>dist is the distance metirc, usually euclidean distance</li></ul><p>THe ABOF of an point is the variance of the consine of it and all data pairs consists of other data points. The smaller ABOF a data point has, the more likely it would be an outlier</p><p>The complexity of ABOF is <span class="math inline">\(O(n^ 3)\)</span>. Besides, when applying euclidean distance, the scale of data would have an effect</p><h3 id="nearest-neighbor-method">3.2 Nearest Neighbor Method</h3><p>The steps of Nearest neighbor method is given as follow</p><ul><li>Calculate the KNN of a data point</li><li>Add up the distances of the KNN</li><li>Repeat for every points and sort the sum of distance in descending order</li><li>The points with larger sum of distance is more likely to be outliers</li></ul><p>The tranditional KNN is not suitable for sparse dataset. To improve this, we can apply the following metrics to make judgement:</p><p><strong>Local Outlier Factor(LOF) </strong>: The ratio of the local average density of the KNN of the data point and the local average density of the point itself, the greater LOF a point has, the more likely it would be an outlier</p><p><strong>Connectivity Oulier Factor(COF) </strong>: Outlier are points p where average chaining distance of p is higer than average chaining distanc of p's neighbors. Points with higher COF has sparser surrounding than its neighbors</p><h3 id="distance-based-test">3.3 Distance-based Test</h3><p>As mentioned in 2.1.2, for high dimension data, we can apply tests on the distance of X and <span class="math inline">\(\bar{X}\)</span> instead of the original vector.</p><h2 id="model-based-method">4. Model-based Method</h2><h3 id="clustering-model">4.1 Clustering Model</h3><p>Clustering model can be used as outlier detection methods.</p><ul><li>For distance-based clustering, like K-Means, outliers are points have largest distance to their cluster centroids</li><li>For density-based clustering, like DB-SCAN, outliers would be labeled by the model</li></ul><p>For details of clustering algorithm, refer to <a href>ongoing</a></p><h3 id="one-class-svm">4.2 One-class SVM</h3><h3 id="isolation-forest">4.3 Isolation Forest</h3><h3 id="pca">4.4 PCA</h3><p>PCA can also be utilized as an outlier detection method. In PCA, we would decompose the covariance matrix, the <span class="math inline">\(m^{th}\)</span> eigenvector <span class="math inline">\(\vec{e_m}\)</span> represent present the direction of the <span class="math inline">\(m^{th}\)</span> principle component, and the eigenvalue <span class="math inline">\(\lambda_m\)</span> is the variance on that direction, which also indicates the importance of principal component. Therefore, <span class="math inline">\(\vec{x}\cdot \vec{e_m}\)</span> represents mapping all original features to the direction of <span class="math inline">\(m^{th}\)</span> principal component and adding up all decomposed vectors on that direction, as <span class="math inline">\(\vec{x}\)</span> is standardized.</p><p>Thus, we can define the deviation extent of an sample on a certain principle component direction: <span class="math display">\[d_m = \frac{\vec{x_i}\cdot \vec{e_m}}{\lambda_m}\]</span> The role of <span class="math inline">\(\lambda\)</span> is to make deviation extent more comparable, as the size of variance can be different to some degree.</p><p>With such definition, we might discover the deviation extent of an outlier could probably larger on each direction are generally higer, as shown below:</p><p><img src="/2022/10/07/outlier/4.png"></p><p>we can define the Anomaly score of a data point as: <span class="math display">\[Score(x_i) = \sum_m d_m\]</span> The higher this score is, the more likely a data point would be an outlier.</p><p>This score is equalvalent to the Mahalanobis Distance between the data sample and the mean, as the M distance is the euclidean distance in standard principal components space: <span class="math display">\[D = \sqrt{(X-\mu)^TS^{-1}(X-\mu)}=\sqrt{X^TE\lambda^{-1}E^{-1}X} = \sqrt{\frac{(EX)^2}{\lambda}} = \sqrt{d}\]</span> In this equation:</p><ul><li>X is standardized vector, <span class="math inline">\(\mu\)</span> is thus 0</li><li><span class="math inline">\(S^{-1}\)</span> is the inverse of the covariance matrix</li><li><span class="math inline">\(\lambda\)</span> is the eigenvalues matrix, which is a diagnal matrix, <span class="math inline">\(\lambda^{-1} = \frac{1}{\lambda}\)</span></li><li>E is the eigenvalues matrix, which is a orthonormal matrix, <span class="math inline">\(E^{-1} = E^T\)</span></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Outlier</tag>
      
      <tag>Data Preprocess</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>笔试中较难SQL考题总结</title>
    <link href="/2022/09/30/SQL-problems/"/>
    <url>/2022/09/30/SQL-problems/</url>
    
    <content type="html"><![CDATA[<h1 id="笔试常见sql难题">笔试常见SQL难题</h1><h2 id="用户连续登录">1. 用户连续登录</h2><p><img src="/2022/09/30/SQL-problems/1.PNG"></p><p><strong>思路</strong>：</p><ol type="1"><li><p>以用户为组，日期为序，每一行记录加上一个行号</p><p>注意，一个用户一天可以登录多次，如果全部排号，这个方法就失效了，所以加一个group by，但是不聚合，这相当于选择了distinct（user_id sales_date）的组合</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <br>    user_id,<br>    sales_date,<br>    <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> user_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> sales_date) rn<br><span class="hljs-keyword">from</span> sales_tb<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id,<span class="hljs-type">date</span>()<br></code></pre></td></tr></table></figure></li><li><p>将上表作为cte，用user_id, date_sub(current_date, interval rn day) group by， 这样相当于把连续的天数聚合到了同一个天（1/3 -1， 1/4 -2， 1/6 - 3 中断）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">with</span> cte <span class="hljs-keyword">as</span> (<br><span class="hljs-keyword">select</span> <br>    user_id,<br>    sales_date,<br>    <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> user_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> sales_date) rn<br><span class="hljs-keyword">from</span> sales_tb<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id,sales_date<br>)<br><span class="hljs-keyword">select</span> <br><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <br><span class="hljs-keyword">from</span> cte <br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id, date_sub(sales_date,<span class="hljs-type">interval</span> rn <span class="hljs-keyword">day</span>) <br><br></code></pre></td></tr></table></figure></li></ol><h3 id="变体连续签到领取金币">变体：连续签到领取金币</h3><p><img src="/2022/09/30/SQL-problems/4.PNG"></p><p>思路：</p><ol type="1"><li>与上题一样，将uid和登录日期combine-distinct后，加上一列行号</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SELECT</span><br>uid,<br><span class="hljs-type">date</span>(in_time) <span class="hljs-keyword">as</span> login_date,<br><span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span>  uid <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>(in_time)) <span class="hljs-keyword">as</span> rn<br><span class="hljs-keyword">from</span> <br>tb_user_log<br><span class="hljs-keyword">where</span> <span class="hljs-type">date</span>(in_time) <span class="hljs-operator">&gt;=</span> <span class="hljs-string">&#x27;2021-07-07&#x27;</span> <span class="hljs-keyword">and</span> <span class="hljs-type">date</span>(in_time) <span class="hljs-operator">&lt;</span> <span class="hljs-string">&#x27;2021-11-01&#x27;</span><br>             <span class="hljs-keyword">and</span> sign_in <span class="hljs-operator">=</span> <span class="hljs-number">1</span><br>            <span class="hljs-keyword">and</span> artical_id <span class="hljs-operator">=</span> <span class="hljs-number">0</span><br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> uid, <span class="hljs-type">date</span>(in_time)<br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>将上表作为cte，同样通过减去行号的方式，将连续的签到天数聚合到一天（past day）</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>uid,<br>date_sub(login_date, <span class="hljs-type">interval</span> rn <span class="hljs-keyword">DAY</span>) ad,<br><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">as</span> days,<br><span class="hljs-built_in">FLOOR</span>(<span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>) <span class="hljs-keyword">as</span> fac,<br><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>)<span class="hljs-operator">%</span><span class="hljs-number">7</span> <span class="hljs-keyword">as</span> res<br><span class="hljs-keyword">from</span> cte <br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">by</span> uid, date_sub(login_date, <span class="hljs-type">interval</span> rn <span class="hljs-keyword">DAY</span>)<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>将上表作为cte2， 增加两列<ol type="1"><li>将past day + lag（day counts）+ 1， 这样相当于找到了同uid下上一次连续登录了几天，由于当前行的past day 是由其所代表的连续签到天数的一天减掉其行号所得到的，而这个行号比上一行的最大连续签到天数再大1，所以是past + lag + 1</li><li>根据天数计算及金币数量</li></ol></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    uid,<br>    date_add(ad, <span class="hljs-type">interval</span> (ifnull(<span class="hljs-built_in">lag</span>(days) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> uid <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> ad),<span class="hljs-number">0</span>)<span class="hljs-operator">+</span><span class="hljs-number">1</span>) <span class="hljs-keyword">DAY</span>) cd,<br>    <span class="hljs-keyword">case</span><br>    <span class="hljs-keyword">when</span> res <span class="hljs-operator">&lt;</span> <span class="hljs-number">3</span> <span class="hljs-keyword">then</span> fac<span class="hljs-operator">*</span><span class="hljs-number">15</span> <span class="hljs-operator">+</span> res<br>    <span class="hljs-keyword">when</span> res <span class="hljs-operator">&gt;=</span> <span class="hljs-number">3</span> <span class="hljs-keyword">then</span> fac<span class="hljs-operator">*</span><span class="hljs-number">15</span> <span class="hljs-operator">+</span> res<span class="hljs-operator">+</span><span class="hljs-number">2</span><br>    <span class="hljs-keyword">end</span> coin<br><span class="hljs-keyword">from</span> cte2<br></code></pre></td></tr></table></figure><ol start="4" type="1"><li>以上表为cte3，根据uid，月份聚合，得到结果，注意时间格式抽取函数的运用</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    uid,<br>    date_format(cd,&quot;%Y%m&quot;),<br>    <span class="hljs-built_in">sum</span>(coin)<br><span class="hljs-keyword">from</span> cte3<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> uid, date_format(cd,&quot;%Y%m&quot;)<br></code></pre></td></tr></table></figure><h3 id="技巧mysql-时间格式化函数">技巧：MySQL 时间格式化函数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <br>date_format(target_datetime, <span class="hljs-string">&#x27;%Y-%m-...&#x27;</span>)<br><span class="hljs-keyword">from</span><br>tb_time<br></code></pre></td></tr></table></figure><p>格式具体代码如下</p><p><img src="/2022/09/30/SQL-problems/2.PNG"></p><p><img src="/2022/09/30/SQL-problems/3.PNG"></p><h2 id="同一时间最大观看人数">2. 同一时间最大观看人数</h2><p><img src="/2022/09/30/SQL-problems/5.PNG"></p><p>思路：</p><ol type="1"><li>将进入和登出union起来，并把进入记为1，退出记为-1</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    artical_id,<br>    uid,<br>    in_time <span class="hljs-keyword">as</span> tn,<br>    <span class="hljs-number">1</span> person<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">where</span> artical_id <span class="hljs-operator">!=</span> <span class="hljs-number">0</span><br><span class="hljs-keyword">UNION</span><br><span class="hljs-keyword">select</span> <br>    artical_id,<br>    uid,<br>    out_time <span class="hljs-keyword">as</span> tn,<br>    <span class="hljs-number">-1</span> person<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">where</span> artical_id <span class="hljs-operator">!=</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>以上表为cte，以文章为组，根据文章，时间点，人数排序，用sum记录当前实时人数 running total</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span><br><span class="hljs-operator">*</span>,<br><span class="hljs-built_in">sum</span>(person) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> artical_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> artical_id, tn <span class="hljs-keyword">asc</span>, person <span class="hljs-keyword">DESC</span> <span class="hljs-keyword">rows</span> <span class="hljs-keyword">BETWEEN</span> UNBOUNDED PRECEDING <span class="hljs-keyword">and</span> <span class="hljs-keyword">CURRENT</span> <span class="hljs-type">ROW</span> ) <span class="hljs-keyword">as</span> p_count<br><span class="hljs-keyword">from</span> <br>cte1<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>以上表为cte，选取各文章组别组别中实时人数最大的时间节点</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> artical_id, <span class="hljs-built_in">max</span>(p_count) <span class="hljs-keyword">as</span> max_read<br><span class="hljs-keyword">from</span> cte2 <br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> artical_id<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> max_read <span class="hljs-keyword">desc</span><br></code></pre></td></tr></table></figure><h2 id="新用户的次日留存率">3. 新用户的次日留存率</h2><p><img src="/2022/09/30/SQL-problems/6.PNG"></p><ol type="1"><li>由于跨天登入登出算都活跃，所以将in和out union起来，然后用group by uid，date(time) 来去除一天内重复的活跃记录（相当于只记录每天有没有活跃）</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    uid,<span class="hljs-type">date</span>(in_time) <span class="hljs-keyword">as</span> tn<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">union</span><br><span class="hljs-keyword">select</span> <br>    uid, <span class="hljs-type">date</span>(out_time) <span class="hljs-keyword">as</span> tn<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> uid,<span class="hljs-type">date</span>(tn)<br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>以上图为cte<ol type="1"><li>增加一列判断该用户本次登录时是否为新用户（以用户为组，时间为序，给每一次登录一个row number， 新用户row_numbe为1）</li><li>left join自身 on （1.day+1 = 2.day），增加一列判断该用户在后一天是否活跃（left join自身，如果后一天未活跃，则会得到null）</li></ol></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> a1.<span class="hljs-operator">*</span>, <br>    <span class="hljs-keyword">case</span><br>    <span class="hljs-keyword">when</span> <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a1.uid <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> a1.tn) <span class="hljs-operator">=</span> <span class="hljs-number">1</span> <span class="hljs-keyword">then</span> <span class="hljs-string">&#x27;NEW&#x27;</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;OLD&#x27;</span><br>    <span class="hljs-keyword">end</span> user_status,<br>    <span class="hljs-keyword">case</span><br>    <span class="hljs-keyword">when</span> a2.uid <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>    <span class="hljs-keyword">end</span> next_day<br><span class="hljs-keyword">from</span> active a1<br><span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> active a2 <span class="hljs-keyword">ON</span><br>date_add(a1.tn,<span class="hljs-type">interval</span> <span class="hljs-number">1</span> <span class="hljs-keyword">day</span>) <span class="hljs-operator">=</span> a2.tn<br><span class="hljs-keyword">and</span><br>a1.uid <span class="hljs-operator">=</span> a2.uid<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>以上表为cte，在where语句中加入只计算新用户，月份为11等条件，group by聚合，计算每天的的今日新用户次日留存数/今日新用户总数</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>tn,<br>round(<span class="hljs-built_in">sum</span>(next_day)<span class="hljs-operator">/</span><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>),<span class="hljs-number">2</span>) <span class="hljs-keyword">as</span> uv_left_rate<br><span class="hljs-keyword">from</span> cte <span class="hljs-keyword">where</span> user_status  <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;NEW&#x27;</span><br> <span class="hljs-keyword">and</span> <span class="hljs-keyword">month</span>(tn) <span class="hljs-operator">=</span> <span class="hljs-number">11</span><br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> tn<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> tn <br></code></pre></td></tr></table></figure><h2 id="行列置换">4.行列置换</h2><p>要对每一列的值进行聚合，在把其作为一行输出</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;column_1&#x27;</span>,<span class="hljs-built_in">sum</span>(column_1), <span class="hljs-built_in">count</span>(column_1) <br><span class="hljs-keyword">from</span> orders<br><span class="hljs-keyword">union</span><br><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;column_2&#x27;</span>,<span class="hljs-built_in">sum</span>(column_2), <span class="hljs-built_in">count</span>(column_2) <br><span class="hljs-keyword">from</span> orders<br></code></pre></td></tr></table></figure><h3 id="技巧从csv导入文件">技巧：从csv导入文件</h3><p>标准的导入语法为</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL">LOAD DATA INFILE <span class="hljs-string">&#x27;path&#x27;</span><br><span class="hljs-keyword">INTO</span> <span class="hljs-keyword">TABLE</span> tb<br>FIELDS TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;,&#x27;</span><br>ENCLOSED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;&quot;&#x27;</span><br>LINES TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;\n&#x27;</span><br>IGNORE <span class="hljs-number">1</span> LINES<br></code></pre></td></tr></table></figure><p>注意：</p><ol type="1"><li><p>文件必须放在MYSQL指定的安全文件夹中，才能导入，可以使用语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SHOW</span> VARIABLES <span class="hljs-keyword">LIKE</span> <span class="hljs-string">&#x27;secure_file_priv&#x27;</span><br></code></pre></td></tr></table></figure><p>来查看</p></li><li><p>文件路径中的反斜杠/</p></li><li><p>若在windows文件夹下，行截止符应为 ''</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs SQL">LINES TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;\r\n&#x27;</span><br></code></pre></td></tr></table></figure></li><li><p>IGNORE 用于跳过表头行</p></li></ol><h2 id="选取组内最小的另一种思路">5. 选取组内最小的另一种思路</h2><p><img src="/2022/09/30/SQL-problems/7.PNG"></p><p>思路：</p><p>这里不使用窗口函数和子语句，因为那样不方便查找min（date）所对应的device</p><ol type="1"><li>以所有该用户的event_date为子查询，选取组内小于等于任何event_date的 event date，得到的就是组内最小值，且可以简单的得到最小值对应行的其他数据</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SELECT</span> player_id, device_id <span class="hljs-keyword">from</span> Activity a1<br><span class="hljs-keyword">WHERE</span> event_date <span class="hljs-operator">&lt;=</span> <span class="hljs-keyword">ALL</span>(<span class="hljs-keyword">SELECT</span> event_date <span class="hljs-keyword">FROM</span> Activity a2 <span class="hljs-keyword">WHERE</span> a1.player_id <span class="hljs-operator">=</span> a2.player_id );<br></code></pre></td></tr></table></figure><h2 id="选取第二高的薪水">6. 选取第二高的薪水</h2><p><img src="/2022/09/30/SQL-problems/8.PNG"></p><p>思路：</p><p>可以使用dense_rank进行排序，但这里使用另一种方式</p><ol type="1"><li>选出distinct的salary，降序排列，然后使用limit offset进行筛选</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">DISTINCT</span><br>    Salary <span class="hljs-keyword">AS</span> SecondHighestSalary<br><span class="hljs-keyword">FROM</span><br>    Employee<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> Salary <span class="hljs-keyword">DESC</span><br>LIMIT <span class="hljs-number">1</span> <span class="hljs-keyword">OFFSET</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>加上ifnull</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span><br>    IFNULL(<br>      (<span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">DISTINCT</span> Salary<br>       <span class="hljs-keyword">FROM</span> Employee<br>       <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> Salary <span class="hljs-keyword">DESC</span><br>        LIMIT <span class="hljs-number">1</span> <span class="hljs-keyword">OFFSET</span> <span class="hljs-number">1</span>),<br>    <span class="hljs-keyword">NULL</span>) <span class="hljs-keyword">AS</span> SecondHighestSalary<br></code></pre></td></tr></table></figure><h3 id="技巧有关limit和offset的使用">技巧：有关limit和offset的使用</h3><ol type="1"><li><p>当limit后跟一个参数时，表示从头开始选取若干行</p><p><code>select * from tb limit 3</code></p></li><li><p>当limit后跟两个参数时，表示从头开始跳过a行，再选取之后的b行</p><p><code>select * from tb limit 2,3 --跳过1-2行，选取3-5行</code></p></li><li><p>limit offset 组合使用时，limit后只跟一个参数表示选取多少行，offset后一个参数，表示跳过多少行</p><p><code>select * from tb limit 3 offset 2 --效果同 2.</code></p></li></ol><h2 id="技巧从json导入数据">7.技巧：从json导入数据</h2><p>似乎有专门的导入工具，暂时可以使用<a href="https://data.page/json/csv">此网址</a>,输入json格式的数据（不用输入表头)，将其转化为csv文件，在参考<a href="#技巧：从csv导入文件">从csv导入文件</a></p><h2 id="选择连续且为空的座位">8. 选择连续且为空的座位</h2><p><img src="/2022/09/30/SQL-problems/9.PNG"></p><p>思路：</p><p>此题应该与<a href="#1.%20用户连续登录">用户连续登录</a>区分开来，此问题需要输出所有连续的行号，而不需要聚合，所以用row number判断并不恰当</p><ol type="1"><li>将表格和自身join起来，on<ol type="1"><li>abs（1.seat - 2.seat）= 1</li><li>1.free = 2.free = 1</li></ol></li><li>这样出现在表格中的每一行都一定有一个neighbor， 且neighbour也为空座，最后再选择distinct</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> a.seat_id<br><span class="hljs-keyword">from</span> cinema a <span class="hljs-keyword">join</span> cinema b<br>  <span class="hljs-keyword">on</span> <span class="hljs-built_in">abs</span>(a.seat_id <span class="hljs-operator">-</span> b.seat_id) <span class="hljs-operator">=</span> <span class="hljs-number">1</span><br>  <span class="hljs-keyword">and</span> a.free <span class="hljs-operator">=</span> <span class="hljs-literal">true</span> <span class="hljs-keyword">and</span> b.free <span class="hljs-operator">=</span> <span class="hljs-literal">true</span><br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> a.seat_id<br></code></pre></td></tr></table></figure><h2 id="技巧使用concat">9.技巧：使用concat（）</h2><p>concat()的作用是把两个列连接成一个列，当要使用类似如下语句时</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">where</span> concat(a,b) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> concat(c,d) <span class="hljs-keyword">from</span> cte)<br></code></pre></td></tr></table></figure><p>此语句逻辑为：只要a,b不同时等于某些特定组合，就可以选择</p><h2 id="累积求和差">10. 累积求和/差</h2><p>如图，若干位乘客要登上大巴，每位乘客有一登车次序，乘客的体重列在表中，大巴车能容纳1000KG的重量，要求返回最后一名能够上车的乘客的姓名</p><p><img src="/2022/09/30/SQL-problems/10.PNG"></p><p>思路：按照登车顺序，计算登车乘客体重的running total：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">with</span> cte <span class="hljs-keyword">as</span> (<br><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span>, <br>  <span class="hljs-built_in">sum</span>(weight) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> turn <span class="hljs-keyword">rows</span> <span class="hljs-keyword">BETWEEN</span> UNBOUNDED PRECEDING <span class="hljs-keyword">and</span> <span class="hljs-keyword">CURRENT</span> <span class="hljs-type">ROW</span>) <span class="hljs-keyword">as</span> roll_sum<br><span class="hljs-keyword">from</span> Quene <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> turn<br></code></pre></td></tr></table></figure><p>筛选体重running total小于等于1000的乘客，并获取最后一位乘客的姓名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">with</span> allow <span class="hljs-keyword">as</span> (<br>    <span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <br><span class="hljs-keyword">from</span> cte <br><span class="hljs-keyword">where</span> roll_sum <span class="hljs-operator">&lt;=</span><span class="hljs-number">1000</span><br>)<br><span class="hljs-keyword">select</span> person_name <br><span class="hljs-keyword">from</span> allow <br><span class="hljs-keyword">where</span> turn <span class="hljs-operator">=</span> (<span class="hljs-keyword">select</span> <span class="hljs-built_in">max</span>(turn) <span class="hljs-keyword">from</span> allow)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面试中的贝叶斯推断问题</title>
    <link href="/2022/09/29/bayes-inference/"/>
    <url>/2022/09/29/bayes-inference/</url>
    
    <content type="html"><![CDATA[<h1 id="面试中的贝叶斯推断问题">面试中的贝叶斯推断问题</h1><h2 id="有关贝叶斯推断">1. 有关贝叶斯推断</h2><p>贝叶斯推断是一项统计技术，基于贝叶斯定理，通过观察证据来更新假设的概率。贝叶斯推断将参数看作一个变量，因此可以用于参数估计，但是在数据岗位面试中，一般以计算后验概率的方式考察</p><p>定义：</p><ul><li>H：假设变量，代表某一事件各种可能结果，服从特定分布，其概率密度函数为<span class="math inline">\(f_H\)</span>（先验概率分布）</li><li>E：证据变量，用于更新先验概率，服从特定分布，其概率密度函数为<span class="math inline">\(f_E\)</span>（边际似然函数，证据分布）</li><li><span class="math inline">\(f_{E|H}\)</span>： 似然函数，即在固定H的前提下，E的概率密度函数</li></ul><p>根据贝叶斯定理及全概率公式： <span class="math display">\[f_{post}(H=\hat{H}|E=\hat{E}) = \frac{f_{E|H=\hat{H}}(E=\hat{E})f_H(H=\hat{H})}{f_E(E=\hat{E})} = \frac{f_{E|H=\hat{H}}(E=\hat{E})f_H(H=\hat{H})}{\int_{-\infty}^\infty f_{E|H}(E=\hat{E})f_H(H)\mathrm{d}H}\]</span> 其中：</p><ul><li><span class="math inline">\(f_{post}\)</span> 称作后验概率分布，是基于证据更新后的先验概率分布</li><li><span class="math inline">\(\hat{H}\)</span> 为假设的某一特定取值</li><li><span class="math inline">\(\hat{E}\)</span> 为证据的观测值</li></ul><p>式（1）可以简写为： <span class="math display">\[P(H|E)= \frac{P(E|H)P(H)}{P(E)} = \frac{P(E|H)P(H)}{\sum_i^m P(E|H_i)P(H_i)} = \frac{P(E|H)P(H)}{P(E|H_i)P(H_i)+P(E|\bar{H_i})P(\bar{H_i})}\]</span></p><h2 id="后验概率计算的基本框架">2. 后验概率计算的基本框架</h2><p>数据分析及数据科学面试中，大多数的贝叶斯推断问题都是具体取值下的后验概率计算问题，且一般假设变量的样本空间有限，先验分布是离散的，因此其基本做题框架可以归结如下：</p><ol type="1"><li>明确问题中的假设变量和证据变量分别是什么, 对于假设变量，确定一个完备事件组</li><li>根据要求的结果，确定关心的假设事件<span class="math inline">\(A_ k\)</span>和证据事件B, 并确定先验概率P(A)</li><li>对于完备事件组中每一个假设事件<span class="math inline">\(A_ i\)</span>, 计算相应的似然性<span class="math inline">\(P(B| A_i)\)</span></li><li>根据贝叶斯定理及全概率公式计算后验概率<span class="math inline">\(P(A_i| B)\)</span>， 如果有多轮迭代，把前一轮的后验概率当作后一轮的先验概率</li></ol><h2 id="常见题型和例题解析">3. 常见题型和例题解析</h2><h3 id="检测问题">3.1 检测问题</h3><p><strong>例题</strong>：已知某肺炎的患病率为0.01%。现在需要做检测，如果被测者患病则被检测为阳性的概率为99%。如果被测者没有病则被检测为阴性的概率为99.9%。现在一个人检查结果是阳性。问真正得病的概率是多少？</p><p>此类问题的特点为：先验分布和证据分布都为二值/多值分布</p><ul><li>假设变量：患者真正得病，有得病和不得病两种结果</li><li>证据变量：患者被检测出得病，有检测出得病和检测出不得病两种结果</li></ul><p>令：</p><ul><li>A事件为患者真正得病 <span class="math inline">\(P(A) = P(H=true\ positive)\)</span></li><li>B事件为患者诊断得病<span class="math inline">\(P(B) = P(E = diagnosed \ positive)\)</span></li></ul><p><span class="math display">\[P(A) = 0.01\%\]</span></p><p><span class="math display">\[P(B|A) = 99\%\\\]</span></p><p><span class="math display">\[P(B) = P(B|A)P(A)+P(B|\bar{A})P(\bar{A}) = 99\%*0.01\% + 0.1\%*99.99\%\]</span></p><p>可计算得到后验概率为： <span class="math display">\[P(A|B) = 9.01\%\]</span> 可见诊断后后，患者得病概率上升，原本认为得病概率为0.01%，经过一个只包含一个人的样本重新计算，更新为了9.01%</p><h3 id="硬币问题">3.2 硬币问题</h3><p>假设有100枚硬币，其中70枚为正常，30枚为缺陷，从中取出1枚投掷10次，10次中9次为正面，1次为负面。当硬币正常时，投出正面的概率为0.5，当硬币缺陷时，投出正面的概率为0.8。求硬币缺陷的概率</p><p>此类问题的特点为，先验分布是二值分布，证据分布是伯努利分布</p><ul><li>假设变量：硬币有缺陷，有是否两种可能</li><li>证据变量：投出n次正面，其概率为<span class="math inline">\(p^n*q^{1-n}\)</span></li></ul><p>令：</p><ul><li>A事件为硬币有缺陷 <span class="math inline">\(P(A) = P(H = defective)\)</span></li><li>B事件为投出9次正面<span class="math inline">\(P(B) = P(E = 9 \ head)\)</span></li></ul><p><span class="math display">\[P(A) = 0.3\]</span></p><p><span class="math display">\[P(B|A) = 0.8^9*0.2^1\\\]</span></p><p><span class="math display">\[P(B) = P(B|A)P(A)+P(B|\bar{A})P(\bar{A}) = 0.8^9*0.2^1 *0.3 + 0.5^9*0.5^1*0.7\]</span></p><p>可计算得到后验概率为： <span class="math display">\[P(A|B) = 91.2\%\]</span> 与此类似的问题有朋友说谎问题等</p><h4 id="朋友说谎问题">3.2.1 朋友说谎问题</h4><p>假设某地今天下雨的概率是<span class="math inline">\(\frac{1}{2}\)</span>，你在前往某地前向3个当地的朋友询问是否下雨，所有人都该诉你下雨了，但是每个人都有<span class="math inline">\(\frac{1}{3}\)</span>的概率说谎， 求某地今天真正下雨的概率</p><ul><li>A：某地下雨</li><li>B：三个朋友都告诉你下雨</li></ul><p><span class="math display">\[P(A) = \frac{1}{2}\]</span></p><p><span class="math display">\[P(B|A) = \frac{2}{3}^3\\\]</span></p><p><span class="math display">\[P(B) = \frac{2}{3}^3*\frac{1}{2}+\frac{1}{3}^3*\frac{1}{2}\]</span></p><h3 id="三门问题">3.3 三门问题</h3><p>有A，B，C三扇门，其中一扇背后有奖品，当玩家选择一扇门以后，主持人会打开另一扇后面是空的门，请问玩家是否要改变自己的选择？</p><p>此类问题特点为：</p><ul><li>共出现3个随机变量，都是多值分布，但是这三者之间是互斥的，如果A为1，则B，C不为1</li><li>此时先验变量为“某一变量为一特定值”，证据变量为“另一个变量被揭晓不为该值”，最后一个变量仅用于计算全概率，或者用于比较选择</li></ul><p>令：</p><ul><li>A事件为门后有奖的是A门<span class="math inline">\(P(A) = P(H = A)\)</span></li><li>D事件为B门被打开且为空<span class="math inline">\(P(B) = P(H_D= B)\)</span></li></ul><p>则： <span class="math display">\[P(A) = \frac{1}{3}\]</span></p><p>$$</p><p><span class="math display">\[当A门后有车，B，C门被打开的概率是一样的\]</span> P(D|A) =  $$ 当B门后有车，主持人不会打开B门，因此<span class="math inline">\(P(D|B) = 0\)</span></p><p>当C门后有车，主持人只能打开B门，因此<span class="math inline">\(P(D|C) = 1\)</span></p><p>因此，<span class="math inline">\(P(D) = P(A)*P(D|A)+P(B)*P(D|B)+P(C)P(D|C) = \frac{1}{3}*\frac{1}{2}+0+\frac{1}{3}*1 = \frac{1}{2}\)</span></p><p>由此可以计算得到： <span class="math display">\[P(A|D) = \frac{1}{3}\\P(C|D) = \frac{2}{3}\]</span> 因此，应该选择C门</p><h4 id="赦免问题">3.3.1 赦免问题</h4><p>有A，B，C三个囚犯，其中一个人将被赦免，另外两个将被杀死，如果有囚犯问看守，看守只能告诉他某一个人将被处死，而且看守不能告诉问他的人是否被处死。A问看守，看守回答B要被处死，求这种情况下，A和C被赦免的概率</p><p>令：</p><ul><li>A事件为被赦免的是A</li><li>D事件为看守回答B被处死</li></ul><p><span class="math display">\[P(A) = \frac{1}{3}\]</span></p><p><span class="math display">\[P(D|A) = \frac{1}{2}\\\]</span></p><p><span class="math display">\[P(D|B) = 0\]</span></p><p><span class="math display">\[P(D|C) = 1\]</span></p><p>则计算可得： <span class="math display">\[P(A|D) = \frac{1}{3}\]</span></p><p><span class="math display">\[P(C|D) = \frac{2}{3}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Interview</tag>
      
      <tag>Bayesian Inference</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evaluation for Regression</title>
    <link href="/2022/09/22/evaluation-for-regression/"/>
    <url>/2022/09/22/evaluation-for-regression/</url>
    
    <content type="html"><![CDATA[<h1 id="evaluation-for-regression-model">Evaluation for Regression Model</h1><h2 id="evalution-for-goodness-of-fitting">1. Evalution for goodness of fitting</h2><p>In most case, we can directly used the loss function's value on testing dataset as a metric to evaluate a regression model's performance</p><p>For common loss function for regression model, refer to <a href="http://zhengyuanyang.com/2022/09/22/loss-function/">here</a></p><p>here supplement several other metrics</p><p><strong>MSLE</strong></p><p>Mean Squared Log Error <span class="math display">\[MSLE = \frac{1}{n}\sum_i^n(log(1+y_i)-log(1+\hat{y)}^2\]</span></p><ul><li>Add one to avoid the appearance of log 0</li><li>MSLE is ideal when we priorly know there's a exponential relationship between Y and some X(e. g. Population). However, in many cases we would transorm data with exponential distribution in data preprocessing so that the exponential relationship would be eliminated. Thus, MSLE is not frequently used</li></ul><p><strong><span class="math inline">\(R^2\)</span></strong></p><p>Coefficient of Determination <span class="math display">\[R^2 = 1-\frac{\sum y-\hat{y}}{y-\bar{y}} = 1-\frac{RSS}{TSS}\]</span></p><ul><li><span class="math inline">\(R^2\)</span> represent how many variance of Y are interpreted by the model</li><li>If <span class="math inline">\(R^2 = 0\)</span>, TSS = RSS, which means the model equals to a model that simply predicted all samples as the average of Y, which indicates the model has a bad performance</li><li><span class="math inline">\(R^2\)</span> is not a true square number, the range of it is <span class="math inline">\((-\infty,1])\)</span></li></ul><h2 id="evaluation-for-linearity">2. Evaluation for Linearity</h2><p>Linear model can hardly capture non-linear mapping between X and Y. Thus, the linearity between X and Y need to be examined.</p><p><strong>Detection of non-Linearity</strong></p><p>For high-dimension data, we can draw a draw a <strong>Residual - Fitted</strong> Plot:</p><p><img src="/2022/09/22/evaluation-for-regression/1.png"></p><p>If the residuals randomly placed along a line, like case 1, than we can believe the data already have strong linearity.</p><p><strong>Solution for non-Linearity</strong></p><p>we should consider non-linear transformation for the features or adding non-linear terms of the features, so that the mapping relationship can be expressed by the hypothesis in a linear form</p><h2 id="evaluation-for-autocorrelation">3. Evaluation for Autocorrelation</h2><p>Autocorrelation refer to that there is a dependent mode between the randomized error of each sample . That is, the <span class="math inline">\(\epsilon\)</span> of different are correlated. This would lead to underestimate the real randomized error <span class="math inline">\(\epsilon\)</span>.</p><p>Usually, the Autocorrelation happen when there exists time-sequence factors.</p><p><strong>Detection of Autocorrelation</strong></p><p><em>1.Time-sequence Plot</em></p><p>A simple and direct way to recognized the time-sequence of <span class="math inline">\(\epsilon\)</span>. Draw a plot of <span class="math inline">\(\epsilon\)</span> sorted by time:</p><p><img src="/2022/09/22/evaluation-for-regression/2.png"></p><p>If the previous error <span class="math inline">\(e_{t-1}\)</span> would make the next error <span class="math inline">\(e_t\)</span> tend to be with opppsite sign, we call this a negative autocorrelation. Otherwise, we call it a positive autocorrelation.</p><p><em>2.DW Test</em></p><p>The DW test construct test statistics using the correction between <span class="math inline">\(e^{t-1}\)</span> and <span class="math inline">\(e^t\)</span>. Gather errors of all sample and calculate <span class="math display">\[\rho = corr(e^t,e^{t-1})\]</span> Construct <span class="math display">\[DW \approx 2(1-\rho)\]</span> according the Degree of Freedom n, we can look up in the table and find the upper bound and lower bound of the DW: <span class="math inline">\(d_U,d_L\)</span>, then we can examine which section the DW falls in to and make judgement on autocorrealtion:</p><p><img src="/2022/09/22/evaluation-for-regression/3.png"></p><p>The drawbacks of DW test is that it has two uncertain sections. Besides, it cannot detect higher derivative autocorrelation than 1.</p><p><em>3.LM Test</em></p><p>It regress on $e^t $ with all feature X and lagged error <span class="math inline">\(e = e_{t-1},e_{t-2},..e_{t-p}\)</span> with: <span class="math display">\[e_t = \alpha X + \rho e + \epsilon\]</span> Where <span class="math inline">\(\rho\)</span> is a vector, <span class="math inline">\(\rho_i\)</span> is the correlation between <span class="math inline">\(e_t\)</span> and <span class="math inline">\(e_{t-i}\)</span>. p is a hyperparameter for adjustment.</p><p>Obained the Judge coefficient <span class="math inline">\(R^2\)</span> of the model, when sample size n is big, <span class="math inline">\((n-p)R^2 \sim \chi^2(p)\)</span>, we can than to a <span class="math inline">\(\chi^2\)</span> test to examine whether there exist autocorrelation</p><p><strong>Solution for Autocorrelation</strong></p><p>The solution for autocorrelation:</p><ul><li>Consider time-sequence model</li><li>If there exists positive autocorrelation, consider addling lagged feature <span class="math inline">\(X_{t-1},X_{t-2},...\)</span> into the model</li><li>If there exist seasonal factor, adding that factor into the model as dummies variable</li><li>Generalized Difference Method: construct the model as:</li></ul><p><span class="math display">\[Y_t  = \beta_0 + \beta_1X_t + \rho_1u_1+\rho_2u_2+...\rho_pu_p+\epsilon\]</span></p><p>Where <span class="math inline">\(u_i\)</span> is the residual of the <span class="math inline">\(i^{th}\)</span> model. This method has similar ideal like boosting. In machine learning, we can split the sample set into different moments according to timestamp like hour or date and construct serial models.</p><h2 id="evaluation-of-multicollinearity">4. Evaluation of Multicollinearity</h2><p>Multicollinearity refer to the relationship that a variable's change would cause another variable's change</p><p>In regression, we do not want there to be multicollinearity among input variables.</p><p>For example suppose we have a ideal regression model that <span class="math display">\[y = 10x_1+b\]</span> Then we introduce a highly correlated variable <span class="math inline">\(x_2 = 2x_1\)</span>, in this case the modle can fit infinite possible combination of these two variables, for example, the model might end up with: <span class="math display">\[Y = -100x_1 +55x_2 +b = 10x_1+b\]</span> These will cause:</p><ul><li>coefficient might lose its interpretability. Positive coefficient might turn negative, insignificant variable might turn significant</li><li>The model might lose stability since the coefficients are enlarged. Small noises could cause big variance</li></ul><p><strong>Detection of Multicollinearity</strong></p><p>1.Variance Inflation Factor <span class="math display">\[VIF = \frac{1}{1-R^ 2}\]</span> As discussed above, <span class="math inline">\(R^2\)</span> represent the goodness of fitting. Thus we can use an input variable <span class="math inline">\(x_i\)</span> as the output variable, and fit a model with other input variable <span class="math inline">\(x_1,...x_{i-1},x_{i+ 1}...x_m\)</span> being inputs. If the <span class="math inline">\(R^2\)</span> turn out to be high, then it is possible that <span class="math inline">\(x_1\)</span> are highly correlated with other variables. In such case, the VIF would be high</p><p>In practice, if VIF &gt; 10, we can consider as there's a Multicollinearity problem. If VIF &gt; 100, we can consider as there's a serious Multicollinearity problem</p><p>2.Correlation Matrix</p><p>VIF demands large scale of calculation when the dimension of feature is high. A simpler way is to calculate the corrections matrixs using parameters like Pearson'r, Spearman'<span class="math inline">\(\rho\)</span> or Kendall's <span class="math inline">\(\tau\)</span></p><p><strong>Solution for Multicollinearity</strong></p><p>The solution for Multicollinearity includes:</p><ul><li><p>Filtering feature selection methods based on correlation</p></li><li><p>Wrapper feature engineering methods like stepwise regression</p></li><li><p>Embedded Feature Engineering methods like Lasso &amp; Ridge Regression</p></li></ul><h2 id="evaluation-for-homoskedasticity">5. Evaluation for Homoskedasticity</h2><p>Linear Regression Models require the variances of randomized to have same variance on different scale. That is, with the linearity assumption fulfilled, the variance of the model should be similar no matter how big the output variable is.</p><p><strong>Detection of Heteroskedasticity</strong></p><p>To detect heteroskedasticity, we can use <strong>Residual - Fitted</strong> Plot as well</p><p><img src="/2022/09/22/evaluation-for-regression/4.png"></p><p>In a RF plot, if the linearity assumption is satisfied, the point will place aside a horizontal line. If the model us biased, they would place around a leaned line or a curve. We call this line or curve the central line.</p><p>If the homoskedasticity is fulfilled, then the average distance from the point to the central line should be same across the axis of fitted values. If the errors has heteroskedasticity, then the points would be a shape like a spraying.</p><p><strong>Solution for Heteroskedasticity</strong></p><p>The solution for heteroskedasticity includes:</p><ul><li><a href="http://zhengyuanyang.com/2022/10/07/outlier/">Outlier detection</a>: heteroskedasticity is usually caused by outliers.</li><li>Distribution transformation on Y: heteroskedasticity can also be caused by the distribution of Y. Linear models require Y to be normally distributed. We can conduct distribution type transformation</li></ul><h2 id="evaluation-on-normality-of-epsilon">6. Evaluation on Normality of <span class="math inline">\(\epsilon\)</span></h2><p>The linear regression model requires the randomized error to be normally distributed. For normality test and normal transformation techniques, refer to <a href="http://zhengyuanyang.com/2022/10/22/distribution-type/">this article</a></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information Theory in ML</title>
    <link href="/2022/09/22/information-theory-in-ML/"/>
    <url>/2022/09/22/information-theory-in-ML/</url>
    
    <content type="html"><![CDATA[<h1 id="information-theory-basis">Information Theory Basis</h1><h2 id="information-and-entrophy">1. Information and Entrophy</h2><p><strong>self-info</strong></p><p>self-information is a measure related to the outcome of a probabilistic event. To quantify the information, we hope the measure would have following properties:</p><ul><li>A low-probability outcome contains more infomation than a high-probability event. For example, "I won't die" contains more information than "I will die". To some extent, we can regard self-info as the degree we would feel surprise about an certain outcome of an event</li><li>Information quantity must not be negative</li><li>The total imformation of several outcome should be the sum of ther individual information</li><li><span class="math inline">\(I(y=m_1,y=m_2) = I(y=m_1)+I(y=m_2)\)</span></li><li>I(y) must be continuous if P(y) is continuous</li></ul><p>It can be proved that the only expression that satisfies these properties is <span class="math inline">\(Klog(P(y))\)</span> where K is negative. Set K= 1, we define the self-information of an outcome of an event as: <span class="math display">\[I  = log(\frac{1}{P(y=c_m)}) = -log(P(y=c_m))\]</span></p><p><strong>Entrophy</strong></p><p>Entrophy is the expectation of self-info. It represent the mean information of an event related to an random variable y, the greater H is, the more uncertain event y is <span class="math display">\[H(Y) = E[I(y)] = -\sum_m^k p(y=c_m)log(p(y=c_m))\]</span></p><h2 id="entrophy-measure-for-two-variables">2. Entrophy measure for two variables</h2><h3 id="joint-entrophy">2.1 Joint Entrophy</h3><p>Joint entropy measure the uncertainty of a joint event (X, Y) <span class="math display">\[H(T,Y) =-\sum_t\sum_y p(t,y)log(p(t,y))\]</span></p><h3 id="conditional-entrophy">2.2 Conditional Entrophy</h3><p>Conditional Entrophy represent the Information recieved of one event when another event is for certain <span class="math display">\[H(Y|T) =-\sum_t\sum_y p(t,y)log(p(y|t)) = -\sum_t\sum_y p(y|t)p(t)log(p(y|t))\]</span> The higher H(Y|T) is, the less "extra information" about Y is needed when T is certain, which means the corralation between Y and T l is higher</p><p>The relationship between conditional entropy and joint entropy: <span class="math display">\[H(Y｜T) = H(Y, T) - H(T)\]</span></p><h3 id="inofrmation-gain">2.3 Inofrmation Gain</h3><p>Information gain represent the amount of uncertainty reduction of a variable when another variable is certain <span class="math display">\[IG(Y|T) = H(Y) - H(Y| T)\]</span> It is usually applied as an impurity metrics in splitting algorithm like Decision Tree</p><h3 id="mutual-information">2.4 Mutual Information</h3><p>The nature of Mutual Information and Information Gain is the same, mutual information is the shared entropy of two variable</p><p><img src="/2022/09/22/information-theory-in-ML/1.png"></p><p>First, when we consider two event together, we can rewrite H(Y) as: <span class="math display">\[H(Y) = -\sum_i^m p(y=c_i)log(p(y=c_i) \\= -\sum_j^n p(x = c_j)\sum_i^mp(y=c_i|x_=c_j)log(p(y=c_i)) \\= \sum_x\sum_yp(x,y)log(p(y))\]</span></p><p><span class="math display">\[MI = H(x,y) - H(x|y) - H(y|x) = \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)}\]</span></p><p><span class="math display">\[IG = H(y)-H(y|x) = \sum_x\sum_yp(x,y)log(p(y)) - \sum_x\sum_yp(x,y)log(p(y|x)) = MI\]</span></p><p>MI is usually mentioned when calculating the correlation of two variable, IG is mentioned when calculating the impurity reduction of one variable when splitting another variable</p><h2 id="entrophy-measure-for-two-distribution">3. Entrophy measure for two distribution</h2><h3 id="kl-divergencerelative-entrophy">3.1 KL Divergence(Relative Entrophy)</h3><p>The KL divergence is the expectation(under true distribution) of the difference between information amount under predicted and true distribution <span class="math display">\[D_{KL}(p||q) = \sum_{i=1}^mp(y_i)\log(\frac{p(y_i)}{q(y_i)})\]</span> Where:</p><ul><li>q is the estimated distribution of y</li><li>p is the real distribution of y</li></ul><p>it can be used to evaluate an estimation of a distribution with the real distribution given</p><h3 id="cross-entrophy">3.2 Cross Entrophy</h3><p>The equation KL divergence can be written as: <span class="math display">\[D_{KL}(p||q) = \sum_{i=1}^mp(y_i)\log(\frac{p(y_i)}{q(y_i)}) = -\sum_i^mp(y_i)log(q(y_i)) - (-\sum_i^mp(y_i)log(p(y_i)))\]</span> Obviously, the second term of this equation is <span class="math inline">\(H_p(x)\)</span>, which is the entrophy of the real data. In a machine learning problem, since real entrophy is fixed for a dataset, we can only minimize the first term. we call this term Cross entropy: <span class="math display">\[CH(p,q) = D_ {KL}(p||q) + H_p(X) = -\sum_i^mp(y_i)log(q(y_i))\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Information Theory</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Common Loss Function in Machine Learning</title>
    <link href="/2022/09/22/loss-function/"/>
    <url>/2022/09/22/loss-function/</url>
    
    <content type="html"><![CDATA[<h1 id="common-loss-function-in-machine-learning">Common Loss Function in Machine Learning</h1><h2 id="numerical-output">Numerical Output</h2><h3 id="msel2-lossrmse">1. MSE(L2 Loss)/RMSE</h3><p>Mean Squared Error <span class="math display">\[MSE = \frac{1}{n}\sum_i^n(y_i-\hat{y})^2\]</span></p><ul><li>MSE is differentiable any where, which makes it suitable for gradient descent optimizer</li><li>With gradient decrease, the MSE will decrease, which means MSE is effective with fixed learning rate</li><li>The squared operator gives enlarge the loss, thus MSE is sensitive ti outlier, if outlier is meant to be detected, suing MSE is fine. If the outlier is treat as part of the training data, MSE is not an ideal loss function (e. g. prediction model for sales promotion day)</li></ul><p><img src="/2022/09/22/loss-function/1.png"></p><p>RMSE is the square root of MSE <span class="math display">\[RMSE = \sqrt{MSE}\]</span></p><ul><li>RMSE is on the same scale with MSE, but it make the loss more interpretable in some case(e.g prediction on product price)</li></ul><h3 id="mae-l1-lossmape">2. MAE (L1 Loss)/MAPE</h3><p>Mean Absolute Error <span class="math display">\[MAE = \frac{1}{n}\sum_i^n|y_i-\hat{y}|\]</span></p><ul><li>MAE is not sensitive to outlier, it is less likely to cause gradient explosion</li><li>The gradient for any loss is the same, which means MAE does not converge well with fix learning rate</li><li>MAE is not differentiable at 0</li></ul><p><img src="/2022/09/22/loss-function/2.png"></p><p>MAPE is the mean percentage absolute error <span class="math display">\[MAPE = \frac{1}{n}\sum_i^n\frac{|y_i-\hat{y}|}{y_ i}\]</span></p><ul><li>It can more clearly deliver the degree of deviation rather than the scale of the error</li></ul><h3 id="smooth-l1huber-loss">3. Smooth L1(Huber Loss)</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^nz_ i\]</span></p><p><span class="math display">\[z_i = \left\{\begin{aligned}MSE \qquad&amp; |y -\hat{y}| &lt; \beta \\MAE \qquad &amp; elsewhere \\\end{aligned}\right.\]</span></p><ul><li><span class="math inline">\(\beta\)</span> is a defined threshold for error of outlier. If the error is within <span class="math inline">\(\beta\)</span>, apply MSE, otherwise, apply MAE. Huber Loss combines the advantages of L1 and L2 Loss</li><li>Huber loss is ideal for NN</li></ul><p><img src="/2022/09/22/loss-function/3.png"></p><h3 id="log-cosh-loss">4. Log-cosh Loss</h3><p><span class="math display">\[L = \sum_ i^ nlog(cosh(y_i-\hat{y_i}))\]</span></p><ul><li>Log-cosh has almost every virtue of Huber loss, the difference is, it is second-differentiable anywhere, such method is very useful when applying newton-method optimzer</li><li>However, when erro is very big, Log-cosh loss still would have gradient or hessian problem(gradient remains same when loss decrease). Just like MAE</li></ul><h2 id="categoricalprobability-output">Categorical/Probability Output</h2><h3 id="log-losscross-entropy-loss">1. Log Loss/Cross Entropy Loss</h3><p>Log Loss is basically the same concept as the cross entropy, the only difference is that it can be applied at a single sample by replaceing true probability p(x) with 1 if <span class="math inline">\(y_ {true} = m\)</span>. The average loss of all sample is the Log Loss or Cross Entropy Loss.</p><p>For Cross Entropy, refer to <a href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">here</a></p><p>When it come to a binary classification scenario, the Log Loss can be written as: <span class="math display">\[LogLoss = -\frac{1}{n}\sum_i^n [y_ilog(p(y_i))+(1-y_i)log(1-p(y_i))]\]</span></p><p>where <span class="math inline">\(y_i\)</span> is the true value of output and <span class="math inline">\(p(y_i)\)</span> is the probability predicted by the model.</p><ul><li>The gradient would decrease as Log Loss decrease, thus Log Loss can work with fixed learning rate</li><li>Sentive to outlier</li><li>GDBT usually apply Log Loss as loss function(classification)</li></ul><p>The following picture is the log loss of the prediction on a positive sample:</p><p><img src="/2022/09/22/loss-function/4.png"></p><h3 id="exponential-loss">2. Exponential Loss</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^ne^{-yf(x)}\]</span></p><ul><li>Theoretically, the optimal of Exponential Loss and Log Loss is the same(<span class="math inline">\(\frac{1}{2}log\ odds\)</span>), the advantage of exponential loss is that is is easier to calculate, thus can make optimzer update the weight with less cost</li><li>Sentive to outlier</li><li>AdaBoosting usually apply Exponential Loss as loss function(classification)</li></ul><h3 id="hinge-loss">3. Hinge Loss</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^n max(0,1-yf(x))\]</span></p><ul><li>If the model label the sample correct, the loss is 0</li><li>Less sensitive to outlier</li><li>SVM usually adopt Hinge Loss as loss function</li></ul><p><img src="/2022/09/22/loss-function/5.png"></p><h3 id="focal-loss">4. Focal Loss</h3><p>Focal Loss is a improved version of Cross Entrophy Loss <span class="math display">\[L_ f = -\frac{1}{n}\sum_i^n\sum_ j^m\alpha_ j(1-p(y ))^\gamma y log(p(y))\]</span></p><ul><li>Focal Loss add a focus factor <span class="math inline">\((1-p(y))\)</span>, so that those sample with high predicted probability, which are the ""easy samples", donate less loss. Compare to CE Loss, Focal Loss focus on those "hard samples"</li><li>Focal Loss also add a balance factor(optional), which is the percentage of a certain category of y among all samples. This make focal loss can deal with imbalanced data</li><li><span class="math inline">\(\gamma\)</span> is a influence parameter. When <span class="math inline">\(\gamma = 0\)</span>, the Focal Loss become CE Loss. In preactice, we usually set <span class="math inline">\(\gamma = 2\)</span></li></ul><h3 id="impurity">5.Impurity</h3><p>Impurity is a kind of loss functions usually applied in splitting in decision tree</p><h4 id="gini-impurity">5.1 Gini Impurity</h4><p><span class="math display">\[I_G = 1- \sum_{i}^m P(Y=C_i)^2\]</span></p><ul><li>Lower <span class="math inline">\(I_G\)</span> , better classification performance</li><li>For decision tree model, calculate <span class="math inline">\(I_G\)</span> for each split and use combined Gini Impurity(<span class="math inline">\(\sum I_G\)</span>) as loss of the spiltting</li></ul><h4 id="information-gain">5.2 Information Gain</h4><p>For details of Information Gain, refer to <a href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">here</a> <span class="math display">\[IG = H(Y) - H(Y|X)\]</span> where X is the feature the spliting based on (X&gt;c,X=1)</p><ul><li>IG is the degree that uncertainty reduce after the spilting, the greater IG is, the better a split is</li><li>We can calculate the Information Gain Rate <span class="math inline">\(IGR = \frac{IG}{H(Y)}\)</span> to present the degress of uncertainty reduction more directly</li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Loss Function</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evaluation Method for Classification</title>
    <link href="/2022/09/21/Evaluation_Classification/"/>
    <url>/2022/09/21/Evaluation_Classification/</url>
    
    <content type="html"><![CDATA[<h1 id="evaluation-method-for-classification">Evaluation Method for Classification</h1><h2 id="confusion-matrix">1. Confusion Matrix</h2><p>The confusion matrix is a table that count different cases of the predicted outcome given by a classification model. For a binary classification model, it can be given as:</p><p><img src="/2022/09/21/Evaluation_Classification/confusion-matrix.png"></p><p>For a multiple classification case, the confusion matrix can be given as:</p><p><img src="/2022/09/21/Evaluation_Classification/2.png"></p><p>For such case, we can define TP, TN, FP, FN for each category by deeming all true “not-bird” samples predicted “not-bird” as TN, etc.</p><h2 id="accuracy-precision-recall-and-f1-score">2. Accuracy, Precision, Recall and F1 score</h2><p>With confusion matrix given, we can now define the following matrix:</p><p><strong>Accuracy</strong></p><p>Accuracy = samples predicted correctly / all predicted samples <span class="math display">\[accuracy = \frac{TP+TN}{TP+FP+FN+FP} \]</span> Accuracy is a very intuitive metrics. However, it sometimes cannot directly reflect the predicting performance of the model as it cannot deal with imbalanced data. For example, suppose we have a sample set of 100 sample with 99 positive and 1 negative, even if the model simply predicted all samples as positive without any training, it would still receive an accuracy of 99%.</p><p><strong>Recall and Precision</strong></p><p>Recall = correctly predicted positive samples / all actual positive samples <span class="math display">\[racall = \frac{TP}{TP + FN}\]</span> Recall represent the ability to find positive samples among all actual postive samples. It can deal with imbalanced data. It is sensitive to FN case, thus is suitable for the business case where FN would bring significant cause(e. g Explosion recognition, vehicle safety judgement). On the other side, recall does not consider FN, a model can simply improved recall by judging all samples as positive, which is not good in some cases.</p><p>Precision = correctly predicted positive samples / all predicted positive samples <span class="math display">\[precision = \frac{TP}{TP + FP}\]</span> Precision represent the probability that a model's judgment on positive case is correct. It can partly deal with imbalanced data. It is sensitive to FP case, thus is suitable for the business case where FP would bring significant cause(e. g Crime judgment, disease diagnosis).</p><p>Recall and Precision can both deal with imbalanced data. However, there's a trade-off between these two metrics. Thus, which metric to put emphasis on idepends on specific business application. Nevertheless, in most cases, since we can just flip P/N, or 1/0, precision is more like an accompanied constraint of recall to prevent model from foucsing too much on capture the minor category samples, as FN and FP are both bad in most business application.</p><p><strong>F1 Score</strong></p><p>The F-measure is a function that balance Precision and recall <span class="math display">\[F_\alpha = \frac{(1+\alpha^2)*P*R}{(\alpha^2*P)+R}\]</span> when <span class="math inline">\(\alpha\)</span> = 1, we call this metric F1 score: <span class="math display">\[F1 = \frac{2PR}{P+R}\]</span> F1 socre combine Recall and Precision to find a balance. It is suitable for many business case where we cannot decide a clear preference.</p><h2 id="p--r-curve-roc-curve-and-auc">3. P- R Curve, ROC Curve and AUC</h2><p><strong>P- R Curve</strong></p><p>P-R Curve is a curve to depict the relationship between Precision and Recall. It's application is similar to F1 score, but lessly used as F1 score is more concise to read( The higher the better)</p><p><img src="/2022/09/21/Evaluation_Classification/3.PNG"></p><p>Usually, we can regard model B as a better model if it can completely wrap the curve of A. If that; s not the case, we can mark the point on the curve where precision equals recall(Break Event Point, or BEP). The curve with a BEP closer towards up- right direction is better.</p><p><strong>ROC Curve</strong></p><p>We define:</p><ul><li>True positive rate (sensitivity): the ratio that actual positive samples predicted correctly</li><li>True negative rate (specificity): the ratio that actual negative samples predicted correctly</li><li>False positive rate (1-specificity): the ratio that actual negative samples predicted wrongly</li></ul><p><span class="math display">\[TPR = \frac{TP}{TP+FN} = Recall\\TNR = \frac{TN}{FP+TN} = 1-TNP\\FPR = \frac{FP}{FP+TN}\]</span></p><p>From a probabilistic aspect:</p><table><tbody><tr class="odd"><td>Precision</td><td style="text-align: center;"><span class="math inline">\(P(Y=1|\hat{Y}=1)\)</span></td></tr><tr class="even"><td>Recall (sensitivity)</td><td style="text-align: center;"><span class="math inline">\(P(\hat{Y}=1|Y=1)\)</span></td></tr><tr class="odd"><td>Specificity</td><td style="text-align: center;"><span class="math inline">\(P(\hat{Y}=0|Y=0)\)</span></td></tr></tbody></table><p>From this interpreation, we found that sensitivity and specificity are condition on Y, which means The influence of P(Y) are blocked whe calculating these two metrics. Therefore, these two metrics are not influenced by the imbalance of data.</p><p>The Reciever Operating Characteristics cureve(ROC curve) take both metrics into consideration by depict the relationship between sensitivity and 1-specificity:</p><p><img src="/2022/09/21/Evaluation_Classification/4.PNG"></p><p><strong>AUC</strong></p><p>Area Under Curve(AUC) is the area beneath the ROC curve.</p><p>Suppose our model completely randomly classifies the samples, the the probability it regard an actual postive sample or an actual negative sample as a positive sample is equal, in this case, AUC would be 0.5. If AUC &gt; 0.5, it means when the model predicts a sample, <span class="math inline">\(P(\hat{Y}=1|Y=1) &gt; P(\hat{Y}=1|Y=0)\)</span> , which means the prediction is effective. Thus, the higher the AUC is, the better the model performs.</p><p>Obviously, AUC is not influenced by imbalanced data, and it's delivery information concisely. Thus, it is one of the most frequently used metrics in classification.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
      <tag>Classification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Causes and Solutions of Overfitting</title>
    <link href="/2022/09/19/overfitting/"/>
    <url>/2022/09/19/overfitting/</url>
    
    <content type="html"><![CDATA[<h1 id="overfitting-cause-and-solution">Overfitting: Cause and Solution</h1><h2 id="biasvariance-and-overfitting">1. Bias，Variance and Overfitting</h2><p>Theoretically, we consider the expected risk of the model on real data consists of two parts:</p><p><strong>Bias</strong></p><p>The model's failure to imitate the real mapping, probability distribution or other relationship of the data. For example, if we try to fit a non-linear relationship with a linear model, there would be inevitable bias. Another example is that we leave some important factors omitted</p><p><strong>Variance</strong></p><p>The model's sensitivity to changes in data. High variance occurs when the model try to seize any details. It put too many weights on unimportant feature or noise in order to reduce error in training process.</p><p>Neither high bias nor high variance is good. However, when we take measure to solve bias, like applying complex model or increase feature, the variance of the model increase accordingly. In other worlds, there is a trade-off between bias and variance. In training process, our aim would be finding a model with bias and variance acceptable.</p><p><strong>Underfitting and Overfitting</strong></p><p>When the bias is high, we cannot describe the pattern of the data correctly, we call such a scene Underfitting</p><p>When the bias is low, but the variance is high, the model is trapped in the details or noises of the data, ot fit well on the given data but it has poor generality so that it would fail on other data</p><p><img src="/2022/09/19/overfitting/1.PNG"></p><p>The performance on training and testing dataset under these scenes</p><table><thead><tr class="header"><th></th><th>training SET Erro</th><th>Testing Set ERROR</th></tr></thead><tbody><tr class="odd"><td>Underfit</td><td>high</td><td>high</td></tr><tr class="even"><td>Overfit</td><td>low</td><td>high</td></tr><tr class="odd"><td>Optimum</td><td>low</td><td>low</td></tr></tbody></table><h2 id="causes-of-overfitting">2. Causes of Overfitting</h2><p>As specified above, the core reason that cause of overfitting is the imbalanced trade-off(High variance, low bias). Specifically, we induct the reasons into:</p><ul><li>Complexity of the model: The model is too complex for the pattern we want to discover in the data</li><li>Defects of data: the samples contains so much noises that the model cannot ignore them</li><li>Overtraining: the model is trained with too much epochs that force the model to learn noises in order to converge</li><li>Improper sampling/splitting: the training set fails to represent the distribution of the real data. Or, in some other case, the real distribution itself decides that the model is hard to imitate it.</li></ul><h2 id="solutions-for-overfitting">3. Solutions for Overfitting</h2><p>According to four reasons, we can also induct the solutions into:</p><h3 id="control-complexity">3.1 <strong>Control complexity</strong></h3><p><strong>Regularization</strong></p><p>In machine learning, regularization refer to constraints on the number of feature dimensions. Usually, it is realized through adding an regular term to the loss function to penalize putting weight on too many features. This includes:</p><ul><li>Lasso regression and Ridge regression</li><li>Soft margin for SVM</li><li>Regular term in XGBoost</li><li>...</li></ul><p><strong>Feature Engineering</strong></p><p>Reduce the number of features. FIlter those redundant feature through feature engineering methods like correlation analysis and dimensionality reduction. For details, please refer to <a href>ongoing</a></p><p><strong>Simplify Structure</strong></p><p>An important rule for machine learning is to solve the task with possible simplest model. A model with simpler structure can usually solve overfitting. Specific action includes:</p><ul><li>Dropout in NN</li><li>pruning in tree-based model</li><li>Hyperparameter like hidden size, max-leaf-node</li></ul><h3 id="data-augmentation"><strong>3.2 Data augmentation </strong></h3><p>The best way to eliminate the variance caused by data defects is simply increasing more data to the training set. Since sufficient data are sometimes unavailable in real project. We can apply data augmentation to generate more training data. Data augmentation is more common in deep learning feild.</p><h3 id="early-stopping"><strong>3.3 Early stopping</strong></h3><p>Stop the optimizer earlier to prevent overtraining. For example:</p><ul><li>raise error threshold for an optimzer</li><li>set max depth for an decision tree</li><li>set max iterations for an neural network</li></ul><h3 id="sampling-and-spliting"><strong>3.4 Sampling and spliting</strong></h3><p><strong>Cross Validation</strong></p><p>Cross validation means spliting the dataset into subsets. Use some of them to estimate the distribution and other of them to evaluate the estimation. Such procedure can effectively control the varaince caused by sample selection bias. For the details of cross validation, refer to<a href>ongoing</a></p><p><strong>Sampling </strong></p><p>From an theoretical perspective, sampling itself is actually a kind of non-parameter ML model. When you do sampling, yur actual target is to imitate the distribution of the real population through getting a sample. Thus, the sample itself would have bias if it cannot represent the true distribution of the population. Training a mode using these samples would obviously cause variance.</p><p>Thus, a way might help solving overfitting is improving your sampling method. For details of sampling methods in ML, refer to <a href>ongoing</a></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
      <tag>Overfitting</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Principle</title>
    <link href="/2022/09/12/Principle/"/>
    <url>/2022/09/12/Principle/</url>
    
    <content type="html"><![CDATA[<h1 id="learning-principle">Learning Principle</h1><p>In machine learning, learning principle refer to the standard for judging whether a model is good or not</p><h2 id="loss-function-risk-function-and-objective-function">1. Loss function, Risk function and Objective function</h2><p>Loss function is a function used to evaluate the fitness of a model</p><p>For supervised learning, loss function evaluate the difference between the predicted output and true output, noted as <span class="math inline">\(L(Y,f(x, \theta))\)</span>. Ususally, loss function is applied to a single sample or a part of the samples, it cannot evaluate the overall performance of the model. To obtain that, we define: <span class="math display">\[R_{exp}(\theta) = E_P[L(Y,f(x, \theta))] = \int_{X*Y}L(Y,f(x, \theta)P(X,Y)dxdy\]</span> Where <span class="math inline">\(R_{exp}\)</span> is called <strong>risk function</strong> or **expected loss*</p><p>To convert a ML problem into an optimization problem, a ideal practice is to adopt risk function as the objective function for the optimization program. However, this require us to know the true JPD P(X, Y), which is usually unknown in real problem(we can only know the estimated JPD through observational dataset), this is called an ill-formed problem</p><p><em>Note: sometimes, instead of directly use loss function, we would use a function of the loss function to construct objective function, for example, the boosting algorithm</em></p><p>For unsupervised learning, the loss function is a totally different thing, we usually directly talked about the objective function since we do not have a <span class="math inline">\(Y_{true}\)</span> to compare with. The specific form of the objective function varies from specific type of unsupervised learning</p><h2 id="empirical-risk-minimization">2. <strong>Empirical Risk Minimization</strong></h2><p>An obvious solution for ill-formed problem is to replace ture P(X,Y) with the observed <span class="math inline">\(\hat{P(X,Y)}\)</span> on training dataset.</p><p>Suppose the weight of all sample are equivalent, we define <span class="math display">\[R_{emp}(\theta) = \frac{1}{N}\sum_{n=1}^N L(y, f(x,\theta)\]</span> Where <span class="math inline">\(R_{emp}\)</span> is called empirical risk.</p><p>When we use empirical risk as our objective function, we call the learning principle of the machine learning "ERM"</p><p>For a probability model, under some condition, we can consider ERM as equivalent to a <strong>Maximum Likehood Estimation</strong>(MLE). Refer to another article about parameter estimation</p><h2 id="structural-risk-minimization">3. Structural Risk Minimization</h2><p>When sample size is big enough, empirical risk would be close enough to the real expected risk. However, in real problem we will not have infinite samples. We would probably obtain a subset of the sample with unmeasured varaible and noise. Such situation would often lead to overfitting. In such case, we need introduce regularization: <span class="math display">\[R_{srm}(\theta) = R_{emp} + \lambda J(\theta)\]</span> where <span class="math inline">\(J(\theta)\)</span> is a function represent the complexity of the model, and <span class="math inline">\(\lambda\)</span> is a penalized parameter used the control the degree of regularization</p><p>When we use structural risk as our objective function, we call the learning principle of the machine learning "SRM"</p><p>For a probability model, under some condition, we can consider SRM as equivalent to a <strong>Maximum-A-Posterior</strong>(MAP). Refer to another article about parameter estimation</p><h2 id="objective-function-for-unsupervised-learning">4. Objective function for Unsupervised Learning</h2><p>[ongoing]</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Learning Principle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic knowledge about Machine Learning(Supervised Learning)</title>
    <link href="/2022/09/11/basic_knowledgefor_ML/"/>
    <url>/2022/09/11/basic_knowledgefor_ML/</url>
    
    <content type="html"><![CDATA[<h1 id="machine-learning">Machine Learning</h1><h2 id="about-machine-learning">1.About Machine Learning</h2><h3 id="what-is-machine-learning">1.1 What is Machine Learning</h3><p>The definition about machine learning can be expressed as:</p><p>Suppose we want a model to perform a task T, and we have a approach E to evaluate the goodness the model complete this task. If we improve E through updating an optimization algorithm O using an observational dataset D, we can call such process a machine learning process</p><h3 id="three-components-of-machine-learning">1.2 Three components of Machine Learning</h3><p><strong>Model</strong></p><p>A model is a learnner trying to solve the task we want to perform. In most cases, that task would fitting a real mapping between variables. Thus, a model can usually be noted as <span class="math inline">\(f(x,\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the parameter of the model, and the fitting task is to find the best <span class="math inline">\(\theta\)</span> according to the training dataset.</p><p>As me may not know the sepcific form of the real mapping, we need to give the algorithm a scope for leaning, we call such scope the Hypothesis Space, noted as F. For all potential fitted mapping f , <span class="math inline">\(f\in F\)</span></p><p><strong>Learning Principle</strong></p><p>To find the best <span class="math inline">\(\theta\)</span>, we need a principle to judge how good a <span class="math inline">\(\theta\)</span> is. This principle usually involve the definition and calculation of a Loss function</p><p><strong>Optimization Algorithm</strong></p><p>After confirming the training dataset, hypothesis space and learning principle, the task of finding the best parameters become a optimization problem. An optimization algorithm is a solver to solve such problem</p><h3 id="proprocess-of-the-data">1.3 Proprocess of the data</h3><h3 id="evaluation-of-the-model">1.4 Evaluation of the model</h3><h2 id="supervised-machine-learning">2. Supervised Machine Learning</h2><p>Supervised learning is a kind of machine learning where the output variable of the model is clearly labeled in the dataset. That is to say, we would have a real outcome <span class="math inline">\(Y_{true}\)</span> and a predicted outcome <span class="math inline">\(Y_{pred}\)</span></p><h3 id="joint-probability-distribution"><strong>2.1 Joint Probability Distribution</strong></h3><p>Joint probability is the probability that multiple conditions are satisfied same time, noted as <span class="math inline">\(P(X,Y)\)</span></p><p>the relationship among joint probability, conditional probability and edge probability are: <span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\\P(Y) = \sum_{i=1}^N P(X,Y)P(X)\]</span> For two independent variable: <span class="math display">\[P(X, Y) = P(X)P(Y)\\P(Y|X) = P(Y)\]</span></p><p>The fundamental hypothesis of supervised machine learning is the existence of the Joint Probability Distribution of input variable X and output variable Y</p><p>The task we want a supervised machine learning algorithm to perform is to imitate the real JPD <span class="math inline">\(P(X,Y)\)</span>, so that we can calculate <span class="math inline">\(P(Y| X)\)</span> when input X is given</p><h3 id="category-of-supervised-machine-learning">2.2 Category of Supervised Machine Learning</h3><p><strong>Probability model v.s. Non-probability model</strong></p><p>The Non-Probability model try to learn the mapping relationship f directly. Usually it confirm a hypothesis space according to prior knowledge(e.g a linear space ). For example, KNN, SVM, NN are all non-probability model</p><p>The probability model try to directly learn one or more of <span class="math inline">\(P(X,Y)\)</span> , <span class="math inline">\(P( X|Y )\)</span> and <span class="math inline">\(P(Y)\)</span></p><p>Usually, it would pre-decide the distribution form of P(Y|X) or P(Y), for example, the logistic regression suppose P(Y| X) follow a Bernoulli Distribution. Logistic Regression, Naive Bayes are probability model</p><p><strong>Parameter model v.s. Non-parameter model</strong></p><p>In statistics, a parameter estimation means an estimation with given hypothesis on the distribution of the whole population, while a non-parameter estimation does not have such hypothesis</p><p>For machine learning:</p><p>A parameter model means you have an explicit hypothesis on the mapping or the probability distribution, like liner regression, logistic regression, naive bayes(limited dimention of <span class="math inline">\(\theta\)</span>) or MLP. The advantage of such kind of models is, if the hypothesis is correct, the model can be fit with very small dataset. However, if the hypothesis is incorrect, no matter how large the dataset is, there would still be inevitable bias</p><p>A non-parameter model means you put no or limited hypothesis on the mapping or distribution, like KNN, tree-based model, SVM(non-linear). The cost of storage and calculation of non-parameter model would be bigger, but theoretically, a non-parameter can fit any complicated mapping as long as we have enough data. Usually a non-parameter model has a few hyperparameters and infinite parameters</p><p><strong>Discriminative model v.s. Generative model </strong></p><p>A discriminative model directly model on Y:</p><ul><li>All non-probability models are discriminative models</li><li>If a probability models try to directly learn <span class="math inline">\(P(Y|X)\)</span>, it is a discriminative model</li></ul><p>including most machine learning model like MLP, logistic regression, decision tree, KNN and SVM</p><p>A Generative model try to induce <span class="math inline">\(P( X| Y)\)</span> through learning P(X, Y) and P(Y), with <span class="math display">\[P(X,Y) = P(X| Y )P(Y)\]</span></p><p><span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\]</span> including naive bayes，GMM</p><h2 id="unsupervised-machine-learning">3. Unsupervised Machine Learning</h2><p>Unsupervised learning is a kind of machine learning where the output variable of the model is not labeled in the dataset. Typically we can separate unsupervised machine learning algorithm into serval types according to the task we want it to perform</p><h3 id="feature-learning">3.1 Feature learning</h3><p>Mining useful expression or combination of features in unlabeled dataset. Usually applied in dimensionality reduction or visualization, including:</p><ul><li>PCA , T- SNE, SVD</li><li>Sparse Encoding, Auto-encoder ,Denoising Autoencoder</li></ul><h3 id="probabilistic-density-estimation">3.2 Probabilistic Density Estimation</h3><p>Induce the probability density function of a variable through observational data, can be classified as:</p><ul><li>Parametric Density Estimation: have prior hypothesis on the distribution form of the variable,including MLE, MAP etc.</li><li>Non-parametric Density Estimation: do not have a prior hypothesi, including histogram, Kernel Density Estimation(KDE) etc.</li></ul><h3 id="clustering">3.3 Clustering</h3><p><strong>About Clustering</strong></p><p>Clustering is a process of splitting a sample space into some subspace, the points in a subspace shoud have similarity. A important thing to notice is that unlike supervised learning, a data point in the clustering naturally does not have a true label. This indicates clustering algorithm is not always theoretically feasible. Let <span class="math inline">\(\chi\)</span> denote the sample space, d denote a dissimilarity function measuring the dissimilarity, or we can say "distance", of two points, F denote a clustering model that map <span class="math inline">\(\chi\)</span> into a group of subspace <span class="math inline">\(C = (C_1,C_2,...C_k)\)</span> <span class="math display">\[C(C_ 1,C_2,...C_k) = F(\chi,d)\]</span> If F is a useful process, that it can segment the space into subspaces, that has inter-similarity, three assumption must be fulfilled:</p><ol type="1"><li><strong>Scale Invariance</strong>: <span class="math inline">\(F(\chi,d) = F(\chi,\alpha d)\)</span>. The similarity does not depends on the scale(unit of distance).</li><li><strong>Richness:</strong> According to different values of F should be able to output all possible split <span class="math inline">\(C = c\)</span></li><li><strong>Consistency:</strong> No matter what dissimilarity function is selected, similar points should be in the same cluster. That is, given 2 point <span class="math inline">\(x, y\)</span> put into the same cluster when applying a dissimilarity function <span class="math inline">\(d_1\)</span>, if <span class="math inline">\(d_2(x,y) \le d_1(x,y)\)</span>, then <span class="math inline">\(F(X,d_1) = F(X,d_2)\)</span></li></ol><p>Nevertheless, according to Kleinberg, there does not exist an F that make all three assumption fulfilled. Thus, a clustering model is always imperfect, this is called <strong>Clustering Paradox</strong>. Still, we can relax the constraint of Richness to k-richness, which means given the number of clusters k, F is able to output all possible split. Under such compromise, we can build a model that satisfy scale invariance, k-richness and consistency and try to find the best k.</p><p>Though different clustering models have unique methods to evaluate the fitness of model, a general way to evaluate the utility of a clustering model is: <span class="math display">\[U[C  = (C_1,C_2,...C_k)] = \frac{\sum_m^k P(C_m)\sum_i \sum_j [P(a_i=v_{i,j}|C_m)^2 -P(a_i=v_{i,j})^2]}{k}\]</span> where <span class="math inline">\(a_i\)</span> is the <span class="math inline">\(i^th\)</span> attribute, <span class="math inline">\(v_{i,j}\)</span> is the <span class="math inline">\(j^{th}\)</span> possible value of <span class="math inline">\(a_i\)</span></p><p>From a information theory perspective, through the cluster model, we should have better knowledge on the value of attributes of a data point by knowing it's cluser. <span class="math inline">\(P(a_i=v_{i,j}|C_m)^2 -P(a_i=v_{i,j})^2]\)</span> measures such gain of knowledge. <span class="math inline">\(P(C_m)\)</span> is a weight of clusters in the sample space to prevent the split to be too imbalanced. <span class="math inline">\(k\)</span> is the number of clusters, it is put here to penalize the complexity of model, as making each data point as a cluster would always obtain inner similarity.</p><p><strong>Category of Clustering Algorithm</strong></p><p>based on the mechanism of clustering, clustering algorithm can be categorized as:</p><ul><li>Distance-based clustering: K-Means, FCM</li><li>Hierarchical clustering: aggregating HC and decomposing HC</li><li>Density-based clustering: DBSCAN, CFSFDP</li><li>Probability-based: GMM</li></ul><h3 id="other-unsupervised-learning">3.4 Other Unsupervised Learning</h3><p>There are lots of emerging unsupervised learning algorithm like pred-Net, GAN etc. These algorithm would be elaborated in single section in other articles</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hypothesis Testing</title>
    <link href="/2022/03/25/Hypothesis-Testing/"/>
    <url>/2022/03/25/Hypothesis-Testing/</url>
    
    <content type="html"><![CDATA[<h1 id="hypothesis-testing">Hypothesis Testing</h1><h2 id="definition-and-terms">1. Definition and Terms</h2><p>Hypothesis testing is a kind of statistical inference method.</p><table><thead><tr class="header"><th style="text-align: center;">Term</th><th style="text-align: center;">Meaning</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Hypothesis</td><td style="text-align: center;">A claim to test</td></tr><tr class="even"><td style="text-align: center;">Null Hypothesis (<span class="math inline">\(H_0\)</span>)</td><td style="text-align: center;">Currently accepted value for a parameter(e.g diff = 0)</td></tr><tr class="odd"><td style="text-align: center;">Alternative Hypothesis(<span class="math inline">\(H_a\)</span>)</td><td style="text-align: center;">The claims to be tested. <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span> are mathematically opposites</td></tr><tr class="even"><td style="text-align: center;">Test Outcomes</td><td style="text-align: center;">Reject <span class="math inline">\(H_0\)</span> or fail to reject <span class="math inline">\(H_0\)</span></td></tr><tr class="odd"><td style="text-align: center;">Test Statistics</td><td style="text-align: center;">Statistics calculated from the data samples used to decide whether to reject <span class="math inline">\(H_0\)</span></td></tr><tr class="even"><td style="text-align: center;">Big/Small Sample</td><td style="text-align: center;">sample size -&gt; inf / sample size is fixed, usually set threshold to 30</td></tr><tr class="odd"><td style="text-align: center;">Central Limit Theorem</td><td style="text-align: center;">A theorem that indicates no matter what distribution the total population is, when sample size n is big enough, the average of a statistics of a sample <span class="math inline">\(\bar{X}\)</span> follows a normal distribution N(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\frac{\sigma^2}{n}\)</span>). This theorem allows implementation of T- test on average of continuous test statistics, like average salary of a department</td></tr><tr class="even"><td style="text-align: center;">Degree of Freedom</td><td style="text-align: center;">The number of samples(n) -1</td></tr><tr class="odd"><td style="text-align: center;">Effect Size</td><td style="text-align: center;">The degree the test statistics differ from the accepted value</td></tr><tr class="even"><td style="text-align: center;">Significance Level(1-<span class="math inline">\(\alpha\)</span>)</td><td style="text-align: center;">A decided threshold of <span class="math inline">\(\alpha\)</span>. <span class="math inline">\(\alpha\)</span> is the probability of rejecting <span class="math inline">\(H_ 0\)</span> when <span class="math inline">\(H_ 0\)</span> is ture(Type I Error)</td></tr><tr class="odd"><td style="text-align: center;">Confidence Level</td><td style="text-align: center;">How confident we are to reject <span class="math inline">\(H_0\)</span> (1-<span class="math inline">\(\alpha\)</span>)</td></tr><tr class="even"><td style="text-align: center;">p value</td><td style="text-align: center;">The probability that the observed statistics significance are caused by random factor</td></tr><tr class="odd"><td style="text-align: center;">statistical power(<span class="math inline">\(1-\beta\)</span>)</td><td style="text-align: center;"><span class="math inline">\(\beta\)</span> is the the probability of accepting <span class="math inline">\(H_ 0\)</span> when <span class="math inline">\(H_ 0\)</span> is False(Type II Error). Typically we need the power of a test to be greater than 80%</td></tr><tr class="even"><td style="text-align: center;">Confidence Interval</td><td style="text-align: center;">With <span class="math inline">\(\alpha\)</span> as significance level, <span class="math inline">\(\theta\)</span> as test statistics, if <span class="math inline">\(P\{ \theta_n &lt; \theta &lt; \theta_m \} \ge 1-\alpha\)</span>, then we call <span class="math inline">\((\theta_n , \theta_m)\)</span> the confidence interval of <span class="math inline">\(\theta\)</span> uhder the significance level <span class="math inline">\(1-\alpha\)</span></td></tr><tr class="odd"><td style="text-align: center;">One-tail Test/Two-tail Test</td><td style="text-align: center;">In one-tail test, the <span class="math inline">\(H_a\)</span> has direction, so <span class="math inline">\(H_a\)</span> would be like <span class="math inline">\(\mu &gt; x\)</span>, and we only focus on one side of the rejection area in this case. While in two tail test, <span class="math inline">\(H_a\)</span> would be like <span class="math inline">\(\mu \ne x\)</span></td></tr><tr class="even"><td style="text-align: center;">Rejection Area</td><td style="text-align: center;">Let the the area of PDF on <span class="math inline">\((-inf,z_1],[z_2,inf)] = \alpha\)</span>, then this area is called rejection area, <span class="math inline">\(b_0,b_1\)</span> are called rejection boundaries, which are the z-score when the area one the left/right side is <span class="math inline">\(\frac{\alpha}{2}\)</span>. When the test statistics fall outside the rejection boundaries, we can reject <span class="math inline">\(H_0\)</span> under the Level of Significance</td></tr><tr class="odd"><td style="text-align: center;">MDE</td><td style="text-align: center;">The minimum detectable effect size of a experiment when <span class="math inline">\(1-\alpha\)</span> and <span class="math inline">\(\beta\)</span> is given. If detected effect size <span class="math inline">\(d &lt; mde\)</span>, it might be caused by random factor, and we cannot reject <span class="math inline">\(H_ 0\)</span></td></tr><tr class="even"><td style="text-align: center;">PSB</td><td style="text-align: center;">Practical Significance Boundary</td></tr></tbody></table><p><img src="/2022/03/25/Hypothesis-Testing/img1.png"></p><p><strong>Power and Error</strong></p><p><img src="/2022/03/25/Hypothesis-Testing/img3.PNG"></p><h2 id="basic-procedure-of-hypothesis-testing">2. Basic Procedure of Hypothesis Testing</h2><h3 id="propose-hypothesis">2.1 Propose Hypothesis</h3><p>Clarify the null hypothesis and alternative hypothesis to test on</p><h3 id="test-type">2.2 Test Type</h3><p><img src="/2022/03/25/Hypothesis-Testing/5.png"></p><h3 id="construct-test-statistics">2.3 Construct Test Statistics</h3><p>For differential test, we usually want to test the difference between a parameter. In most case, we want to test the statistics based on a normal distribution. To do so, we would aggregate the measure though an additive way so that the parameter follows a normal distribution no matter what type of distribution the original measure follows. Thus, the parameter can should be concluded into 2 types: mean and ratio.</p><p>Since the parameter follows the normal distribution, we can construct the test statistics through <strong>Cohen's d</strong>: <span class="math display">\[d = \frac{(\theta_1-\hat{\theta_1})-(\theta_2-\hat{\theta_2})}{\sigma_{SE,pooled}}\]</span> where <span class="math inline">\(\sigma_{SE}\)</span> measures the variance of a parameters <span class="math inline">\(V[\theta]\)</span>.</p><p>For a mean test, <span class="math inline">\(\theta\)</span> is a mean <span class="math inline">\(\bar{ X }\)</span> <span class="math display">\[V[\bar{X}] = V[\frac{(X_1+X_2+...X_n)}{n}] = \frac{1}{n^2}\sum_i^nV[X_i]=\frac{1}{n}V[X_i] = \frac{\sigma^2_X}{n}\]</span> For a ratio test, <span class="math inline">\(\theta\)</span> is a ratio R <span class="math display">\[V[R] = V[\frac{B_1+B_2+...+B_n}{n}] = \frac{1}{n^2}\sum_i^nV[B_i]=\frac{1}{n}p(1-p)\]</span> The calculation of pooled standard error would be discussed in the following sections</p><p>For ratio test, there's also another way to construct the test statistics called <strong>Cohen's h</strong>: <span class="math display">\[h = 2(arcsin \sqrt{p1} - arcsin\sqrt{p2})\]</span> For the correlation test between categorical variable, we can use <strong>Cramer's V</strong> to construct the test statistics <span class="math display">\[V = \sqrt{\frac{\chi^2/n}{min(c- 1,r-1)}}\]</span></p><h3 id="decide-testing-parameters">2.4 Decide Testing Parameters</h3><p><strong>Significance Level</strong></p><p>Decided by the preset <span class="math inline">\(\alpha\)</span> in the testing. <span class="math inline">\(1-\alpha\)</span> is the significance level, represent the strictness degree on judging significance.</p><p><strong>Statistical Power </strong></p><p>Decided by the preset <span class="math inline">\(\beta\)</span> in the testing. Statistical Power represents the probability of rejecting <span class="math inline">\(H_0\)</span> if it is indeed false, in other word, the probability of avoiding type II error. For example, in an one-tail differential Z test, let <span class="math inline">\(\beta&#39;\)</span> denote the real-time statistical power: <span class="math display">\[\beta&#39; = P(d \le Z_{1-\alpha/2}|H_0 \ False) = P(\frac{\theta_1 - \theta_2-\Delta}{\sigma_E} \le Z_{1-\alpha/2} - \frac{\Delta}{\sigma_E})\]</span> where <span class="math inline">\(\Delta = \theta_1 - \theta_2\)</span> is the real differential that exists. Let <span class="math inline">\(Z = \frac{\Delta}{\sigma_E}\)</span> <span class="math display">\[1 - \beta&#39; = 1-\Phi(Z_{1-\alpha/2}-Z) = \Phi(Z- Z_{1-\alpha/2})\]</span> For a two-tail Z test: <span class="math display">\[1 - \beta&#39; =  \Phi(Z- Z_{1-\alpha/2}) + \Phi(-Z-Z_{1-\alpha/2})\]</span></p><p><strong>Practical Significance Boundary</strong></p><p>The preset <span class="math inline">\(\delta\)</span> decided based on business goal. It is the minimum level of difference on metrics that we cares about. The PSB serve as an initial MDE used to decided minimum sample size.</p><p><strong>Vairance of Population</strong>: The estimation on <span class="math inline">\(\sigma^2\)</span> from the historical data of the population</p><p><strong>Estimate the sample size</strong></p><p>From the definition of the statistical power we can find that the minimum sample size required is associated with the expected statistical power. Given a fixed sensitivity(MDE), the higer power we want, the more sample we need. We can deduce the formula of minimum sample size as follows:</p><p>Mean Test: <span class="math display">\[n&#39; = \frac{(\sigma_1^2+\sigma_2^2)(z_{\alpha/2}+z_{\beta})^2}{\delta^2}\]</span> Ratio Test: <span class="math display">\[n&#39;= \frac{Z_{\alpha/2}\sqrt{\frac{p_1+p_0}{2}(1-\frac{p_1+p_0}{2})}+Z_\beta\sqrt{p_1(1-p_1)+p_0(1-p_0)}}{p_1-p_0}\]</span></p><ul><li>where <span class="math inline">\(p_0\)</span> is the baseline of the ratio</li><li><span class="math inline">\(p_1\)</span> is the expected level of the ratio, <span class="math inline">\(p_1 = p_0 + \delta\)</span></li></ul><h3 id="results-analysis">2.5 Results Analysis</h3><p>After the testing duration is reached, we can decide whether there is a statistical significance. Examine the p value of the observed results. If <span class="math inline">\(p &lt; \alpha\)</span> reject the Null hypothesis.</p><p>Also, check the sensitivity(the real-time mde). If the mde is greater than the preser <span class="math inline">\(\delta\)</span>, it is likely that the testing is underpower and needs more samples.</p><p>TO calculate real-time MDE <span class="math display">\[MDE = (t_{\frac{1-\alpha}{2}} + t_ {1-\beta})\sqrt{\frac{s_1^2}{n_ 1} + \frac{s_2^2}{n_ 2}}\]</span></p><p>According to the definition of the statistical power, the sensitivity of the testing is associated with the preset statistical power.</p><p><img src="/2022/03/25/Hypothesis-Testing/8.png"></p><p>In the this figure</p><ul><li>the orange area is the preset <span class="math inline">\(\beta\)</span></li><li>the green area represent the true type II error <span class="math inline">\(\beta&#39;\)</span></li><li><span class="math inline">\(\mu_0\)</span> is the mean of the test statements under <span class="math inline">\(H_0\)</span>, which is 0</li><li><span class="math inline">\(\mu^*\)</span> is the mean where the statistical power is exactly <span class="math inline">\(1-\beta\)</span>. <span class="math inline">\(\mu^*\)</span> decides the MDE of the experiment</li><li><span class="math inline">\(\mu_1\)</span> is the true value of the statistics estimated from the sample</li></ul><p>From the figure we can find out that if the true difference is bigger than MDE, than the ture statistical power is greater then we demands, and the experiment is overpowered. On the opposite, if the true difference is smaller than the MDE, than we cannot detect the difference with enough power, and the experiment is underpowered</p><p>When the sample size remains the same, higer power, which means lower type II error, demands the right-forward shift of mean of the parameter and leads to greater mde(lower sensitivity). If we want to improve sensity while maintaining the power, we would need more samples or find a way to lower the variance of the metrics</p><h2 id="zt-test">3. Z/T-test</h2><p><strong>Z Test</strong>:</p><p>A Z-test examine a statistics assuming it follow a standard normal distribution(distribution). Thus the construction of z statistics is <span class="math inline">\(Z = \frac{X-\mu}{\sigma}\)</span></p><p><strong>T-test</strong></p><p>A T-test construct a statistics like: <span class="math inline">\(t = \frac{X}{\sqrt{Y/n}}\)</span> where <span class="math inline">\(X\sim N(0,1)\)</span> and <span class="math inline">\(Y \sim \chi^2(n)\)</span> . In a mean test <span class="math inline">\(\sqrt{Y/n}\)</span> could be the standard error. The advantage of T test is, while it have similar testing capability as Z test when n is greater than 30, it does not require the population variance <span class="math inline">\(\sigma\)</span> to be given. Meanwhile, it has stronger testing capability when n is smaller than 30</p><h3 id="one-sample-test">3.1 One Sample Test</h3><p>Objective: to test whether the accepted mean or ratio of a population is correct through a sample. In this case, we can assume <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\pi_0\)</span> is given</p><h4 id="sigma-givent-or-z">3.1.1 <span class="math inline">\(\sigma\)</span> Given(T or Z)</h4><p>In most application, the standard deviation of the whole population is unknown. But suppose we have the <span class="math inline">\(\sigma\)</span></p><p><strong>Big Sample</strong></p><p>For big sample, we conduct z-test and give hypothesis as:</p><p><span class="math inline">\(H_0: \mu = \mu_ 0\)</span> or <span class="math inline">\(\pi = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu \ne \mu_ 0\)</span> or <span class="math inline">\(\pi \ne \pi_0\)</span> for ratio test</p><p>where <span class="math inline">\(\mu_0\)</span> is the accepted value of the parameter and construct z-score as: <span class="math display">\[z = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}\]</span> for ratio test, construct z as: <span class="math display">\[z = \frac{p-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\]</span> <strong>Small Sample</strong></p><p>When the sample size is small, we can assume the sample ~ t distribution. The construction of the test statistics and reject condition are the same, the only difference is that the test statistics is now following t distribution</p><h4 id="sigma-unknownt">3.1.2 <span class="math inline">\(\sigma\)</span> Unknown(T)</h4><p>When <span class="math inline">\(\sigma\)</span> is unknown, the basic strategy is to replace population std with sample std, and apply t-test</p><p>We can conduct t-test and give hypothesis as:</p><p><span class="math inline">\(H_0: \mu = \mu_ 0\)</span> or <span class="math inline">\(\pi = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu \ne \mu_ 0\)</span> or <span class="math inline">\(\pi \ne \pi_0\)</span> for ratio test <span class="math display">\[t = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}\]</span> for ratio test, construct t as: <span class="math display">\[t = \frac{p-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\]</span></p><h3 id="two-sample-test">3.2 Two Sample Test</h3><p>Objective: to test whether a condition would effect a metric through following one group before and after experiment or comparing two groups</p><h4 id="match-test">3.2.1 Match Test</h4><p>Usually applied when testing treatment on a same group of people in different time(e. g medical treatment). In this context, we can assum the sample size and standard deviation remain as same: <span class="math inline">\(n_ 1 = n_ 2, \sigma_1 = \sigma_2\)</span></p><h5 id="sigma-givenz">3.2.1.1 <span class="math inline">\(\sigma\)</span> Given(Z)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test <span class="math display">\[z = \frac{(\bar{x_1}-\bar{x_0}) - (\mu_1-\mu_0)}{\sigma\sqrt{\frac{2}{n}}} = \frac{\bar{d}}{\sigma\sqrt{\frac{2}{n}}}\]</span> for ratio test, construct z as: <span class="math display">\[z = \frac{(p_1-p_0) - (\pi_1 - \pi_0)}{\sqrt{p(1-p)(\frac{2}{n})}}\]</span></p><p><span class="math display">\[p = \frac{p_0+p_1}{2}\]</span></p><h5 id="sigma-unkownt">3.2.1.2 <span class="math inline">\(\sigma\)</span> Unkown(T)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test <span class="math display">\[t = \frac{\sqrt{n}((\bar{x_1}-\bar{x_0}) - (\mu_1-\mu_0))}{s_d} = \frac{\sqrt{n}\bar{d}}{s_d}\]</span> where <span class="math inline">\(s_d\)</span> is the standard deviation of d</p><p>for ratio test, construct z as: <span class="math display">\[t = \frac{(p_1-p_0) - (\pi_1 - \pi_0)}{\sqrt{p(1-p)(\frac{2}{n})}}\]</span></p><p><span class="math display">\[p = \frac{p_0+p_1}{2}\]</span></p><h4 id="independent-test">3.2.2 Independent Test</h4><p>Usually applied when evaluation the effect of a treatment by comparing a experiment group and control group, which is A/B Testing</p><h5 id="sigma-givenz-1">3.2.2.1 <span class="math inline">\(\sigma\)</span> Given(Z)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test</p><p>if <span class="math inline">\(\sigma_1 = \sigma_2\)</span> <span class="math display">\[z = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{\sigma \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\]</span></p><p>if <span class="math inline">\(\sigma_1 \ne \sigma_2\)</span> <span class="math display">\[z  = \frac{(\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\]</span> for ratio test: <span class="math display">\[z = \frac{(p_1-p_2) - (\pi_1 - \pi_2)}{\sqrt{p(1-p)(\frac{1}{n_1}+\frac{1}{n_2})}}\]</span></p><p><span class="math display">\[p = \frac{p1*n1+p_2*n_2}{n_1+n_2}\]</span></p><h5 id="sigma-uknownt">3.2.2.1 <span class="math inline">\(\sigma\)</span> Uknown(T)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test</p><p>if <span class="math inline">\(\sigma_1 = \sigma_2\)</span> <span class="math display">\[t = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\]</span></p><p><span class="math display">\[s_p = \sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}}\]</span></p><p>if <span class="math inline">\(\sigma_1 \ne \sigma_2\)</span> <span class="math display">\[t = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}\]</span> for ratio test: <span class="math display">\[t = \frac{(p_1-p_2) - (\pi_1 - \pi_2)}{\sqrt{p(1-p)(\frac{1}{n_1}+\frac{1}{n_2})}}\]</span></p><p><span class="math display">\[p = \frac{p1*n1+p_2*n_2}{n_1+n_2}\]</span></p><h2 id="f-test">4. F-test</h2><p>An F-test construct the test statistics like <span class="math inline">\(\frac{U_1/d_1}{U_2/d_2}\)</span>, where <span class="math inline">\(U1 \sim \chi^2(d_1)\)</span> and <span class="math inline">\(U_2\sim \chi^2(d_2)\)</span>. In testing for Equality of Variances, the <span class="math inline">\(\frac{U}{d}\)</span> can be the standard error</p><h3 id="equality-of-variances">4.1 Equality of Variances</h3><p>Through F-test, we can examine the equality of the variances of two normally distributed varaible</p><p>Let <span class="math inline">\(X_1,X_2\)</span> be two independent variables, where: <span class="math display">\[X_1~N(\mu_1,\sigma_1^2),  X_2~N(\mu_2,\sigma_2^2)\]</span> Get a sample from each variable: <span class="math inline">\(x_1, x_2\)</span>, the sample sizes are <span class="math inline">\(n_1,n_2\)</span></p><p>Let <span class="math inline">\(\bar{x}_1,\bar{x}_2\)</span> be the mean of the samples, <span class="math inline">\(s_1,s_2\)</span> be the standard error of the samples</p><p>Set the null hypothesis and alternative hypothesis of Experiment: <span class="math display">\[H_ 0: \sigma_1^2 = \sigma_2^2\\H_ 1: \sigma_1^2 \neq \sigma_2^2\]</span> If it is a one-tail experiment: <span class="math display">\[H_ 0: \sigma_1^2 &lt; \sigma_2^2\\H_ 1: \sigma_1^2 \ge \sigma_2^2\]</span></p><p>Construct F statistics: <span class="math display">\[F(n_1-1,n_2-1) = \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}=\frac{s_ 1^2}{s_2^2}\]</span> Note: As a convention, we normally select the greater s as <span class="math inline">\(s_ 2\)</span> to let f score &lt;1</p><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span></p><p>If it is a one-tail experiment, then reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(F &lt; F_{1-\alpha}\)</span></p><h3 id="single-factor-anova">4.2 Single Factor ANOVA</h3><p>We can use f- test to examine the impact of a factor to an indicator by judging if the indicator is same when the factor is set to different value</p><p>Let the factor be Y, indicator be x</p><p>Suppose we got the following observations:</p><table><thead><tr class="header"><th>Y = Y1</th><th>Y=y2</th><th>y=y3</th><th>y=y4</th></tr></thead><tbody><tr class="odd"><td>x=1</td><td>x=2</td><td>...</td><td>...</td></tr><tr class="even"><td>x=3</td><td>x=3</td><td>...</td><td>...</td></tr><tr class="odd"><td>x=2</td><td>x=4</td><td>...</td><td>...</td></tr><tr class="even"><td>x=6</td><td>x=6</td><td>...</td><td>...</td></tr></tbody></table><p>Let k = number of groups(number of different values of Y)</p><p>Let <span class="math inline">\(n_i\)</span> = the number of samples in <span class="math inline">\(i_{th}\)</span> group</p><p>Let n = <span class="math inline">\(max(n_ k)\)</span></p><p>Construct <span class="math display">\[F = \frac{SSA/df1}{SSE/df2}\]</span> where:</p><ul><li><p>df1 = k - 1</p></li><li><p>df2 = n- k</p></li><li><p>SSA is Sum of Square Between Groups:</p><p><span class="math display">\[SSA = \sum_{i=1}^kn_i(\bar{x_i}-\bar{x})\]</span> Where:</p><ul><li><span class="math inline">\(\bar{x_i}\)</span> is the average of the <span class="math inline">\(i_{th}\)</span> group</li><li><span class="math inline">\(\bar{x}\)</span> is the average of all <span class="math inline">\(\bar{x_i}\)</span></li></ul></li><li><p>SSE is the Sum of square error <span class="math display">\[SSE = \sum_{i=1}^k(n_i-1)s_i^2\]</span> Where:</p><ul><li><span class="math inline">\(s_1^2\)</span> is the variance(square of standard error) of the <span class="math inline">\(i_{th}\)</span> group</li></ul></li></ul><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span>(the factor do has an impact)</p><h3 id="exam-on-the-significance-of-the-linear-regression">4.3 Exam on the significance of the Linear Regression</h3><p>We can use f-test to examine on whether a liner model(linear hypothesis) fit a problem well</p><p>Suppose we got the following linear hypothesis function: <span class="math display">\[y = \beta_1x_1 + \beta_2x_ 2 + \beta_ 0\]</span> Define SSR as: <span class="math display">\[SSR = ||\hat{y}-\bar{y}1_n||^2\]</span> Where:</p><ul><li><span class="math inline">\(\hat{y}\)</span> is the prediction of y</li><li><span class="math inline">\(\bar{ y }\)</span> is the average of true y</li><li>1n is a vector with all one</li></ul><p>Define SSE as: <span class="math display">\[SSR = ||y- \hat{y}||^2\]</span> Construct F statistics as: <span class="math display">\[F = \frac{SSR/p}{SSE/(n-p-1)}\]</span> where:</p><ul><li><p>n is number of samples</p></li><li><p>p is number of variables(x)</p></li></ul><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span>. In such case, the linear model is significant to the change of the x and y, thus the linear hypothesis is acceptable. Otherwise, consider a non-linear model</p><h2 id="chi2-test">5. <span class="math inline">\(\chi^2\)</span> Test</h2><p>A <span class="math inline">\(\chi^2\)</span> Test construct the test statistics as a function of a <span class="math inline">\(\chi^2\)</span> variable. The degree of freedom are usually involved in the construction of such a statistics</p><h4 id="chi-squared-test-for-independence">5.1 Chi-squared Test for Independence</h4><p>chi-squared test is a <strong>supervised</strong> hypothesis testing method to calculate the probability that 2 categorical variables are correlated</p><p>Suppose there are categorical variables X and Y, X has r possible values, with probability <span class="math inline">\((p_{x,1},...p_{x,r})\)</span> and Y has c possible values, with probability <span class="math inline">\((p_{y,1},...p_{y,c})\)</span>.</p><p>Under the null hypothesis, since X and Y are independent, the probability of observing <span class="math inline">\(X=c_{x,i}, Y = c_{y,j}\)</span> would be <span class="math inline">\(p_{i,j} = p_{x,i}p_{y,j}\)</span></p><p>In such case, let <span class="math inline">\(t_{i,j}\)</span> be the observed times that <span class="math inline">\(X=c_{x,i}, Y = c_{y,j}\)</span>, t would follow a binomial distribution <span class="math inline">\(Bin(t;n,p_{ij})\)</span></p><p>According to properties of binomial distribution, when n is big enough, <span class="math inline">\(Bin(t;n,p_{i,j})\)</span> is approximately <span class="math inline">\(N(np_{i,j},np_{i,j}(1-p_{i,j}))\)</span></p><p>According to the definition of <span class="math inline">\(\chi^2\)</span> distribution, let <span class="math display">\[\chi^2=\sum_i^r\sum_j^c \frac{(t_{i,j}-E[t_{i,j})^2]}{E[t_{i,j}]}\]</span> <span class="math inline">\(\chi^2\)</span> would follow a <span class="math inline">\(\chi^2\)</span> distribution, with the degree of freedom being <span class="math inline">\((r-1)(c-1)\)</span></p><p>We can calculate the <span class="math inline">\(\chi^2\)</span> and obtain its according p value through p value table for <span class="math inline">\(\chi^2\)</span> distribution. If p &lt; 0.05, we can reject the null hypothesis that the two variables is independent</p><p>Note that in a <span class="math inline">\(\chi^2\)</span> testm the probability <span class="math inline">\((p_{x,1},...p_{x,r})\)</span> and <span class="math inline">\((p_{y,1},...p_{y,c})\)</span> is not a hypothesis to test, it is a believed fact. In real application, it needs to be estimated from observations. When number of observations is big enough, the error of estimation can be ignored.</p><p>A example is given below:</p><ol type="1"><li><p>According to the observations, make the frequency table</p></li><li><p>calculate the row total and the column total</p></li><li><p>calculate the expectations for each cell by <span class="math inline">\(\frac{R_i*C_j}{total}\)</span> (<span class="math inline">\(\frac{125*310}{600} = 65\)</span>,etc.)</p></li><li><p>the <span class="math inline">\(\chi^2\)</span> would be <span class="math display">\[\chi_{i,j}^2 = \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\\chi^2 = \sum_{i,j}^{r,c}\chi_{i,j}^2\]</span></p><table><thead><tr class="header"><th>movie type</th><th>High POPULARITY</th><th>low POPULARITY</th><th>Row total</th></tr></thead><tbody><tr class="odd"><td>type1</td><td>50(65)</td><td>75(60)</td><td>125</td></tr><tr class="even"><td>type2</td><td>125(155)</td><td>175(145)</td><td>300</td></tr><tr class="odd"><td>type3</td><td>...</td><td>...</td><td>...</td></tr><tr class="even"><td>type4</td><td>...</td><td>...</td><td>...</td></tr><tr class="odd"><td>column total</td><td>310</td><td>290</td><td>600</td></tr></tbody></table></li><li><p>calculate the degree of freedom, <span class="math inline">\(k=(r-1)(c-1)\)</span></p></li><li><p>check the significance table of chi distribution to find the threshold of <span class="math inline">\(\chi^2\)</span> given k and <span class="math inline">\(\alpha\)</span></p></li><li><p>compare the <span class="math inline">\(\chi^2\)</span> with the rejection area, if <span class="math inline">\(\chi^2\)</span> is greater than the threshold(which means p value is smaller than <span class="math inline">\(\alpha\)</span>), then reject the null hypothesis</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest, chi2<br>X, y = data_0,target_0<br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><h3 id="chi-squared-test-for-goodness-of-fit">5.2 Chi-squared Test for Goodness of fit</h3><p>The process is basically the same.</p><table><thead><tr class="header"><th>Category</th><th>Predicted times</th><th>Observed time</th></tr></thead><tbody><tr class="odd"><td>C1</td><td>...</td><td></td></tr><tr class="even"><td>C2</td><td></td><td></td></tr><tr class="odd"><td>C3</td><td></td><td></td></tr></tbody></table><p>Construct the <span class="math inline">\(\chi^2\)</span> as: <span class="math display">\[\chi^2 = \sum_i^k\frac{O_i-P_i}{P_i}\]</span> the <span class="math inline">\(\chi^2\)</span> follows a <span class="math inline">\(\chi^2(k-1)\)</span> when number of observations is big enough</p>]]></content>
    
    
    <categories>
      
      <category>A/B Testing &amp; Case Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hypothesis Testing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Feature Selection: Statistical Method</title>
    <link href="/2022/02/09/feature-selection-stats-method/"/>
    <url>/2022/02/09/feature-selection-stats-method/</url>
    
    <content type="html"><![CDATA[<h1 id="feature-selection">Feature Selection</h1><h2 id="filter">1. Filter</h2><p>Evaluate features on divergence and correlation. set one or more thresholds and select the feature.</p><p><strong>Pro</strong>: fast, scalable, independent of the model</p><p><strong>Con</strong>: ignore the dependence of features</p><h3 id="varianceskewness">1.1 Variance/Skewness</h3><p>For most models, variables with high variance or skewness would be weighted more and deemed as more important. Thus, we can</p><ol type="1"><li>calculate the variance(not STD) of each feature</li><li>set a threshold, select all features whose variance bigger than the threshold</li></ol><p>Calculation of Skewness: <span class="math display">\[SK = \frac{n\sum(x_i-\bar{x})^3}{(n-1)(n-2)\sigma^3}\]</span> An implementation of feature selection based on variance in Python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold<br><span class="hljs-comment">#threshold 是方差的阈值</span><br><span class="hljs-comment">#返回选择后的特征</span><br>data_1 = VarianceThreshold(threshold=<span class="hljs-number">0.25</span>).fit_transform(data_0)<br></code></pre></td></tr></table></figure><h3 id="correlation">1.2 Correlation</h3><ol type="1"><li>calculate the correlation value between each feature and the target</li><li>select the features with top K biggest correlation value</li></ol><h4 id="pearson-r">1.2.1 Pearson R</h4><p>Pearson R calculates correlation based on covariance. <span class="math display">\[p_{X,Y} = \frac{cov(X,Y)}{\sigma_X \sigma_Y} = \frac{E[(X - \mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y} = \frac{E[XY]-E[X]E[Y]}{\sqrt{(E[X^2]-E[X])^2} \sqrt{(E[Y^2]-E[Y])^2}}\]</span> For a sample with n samples: <span class="math display">\[r = \frac{\sum_i^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_i^n(X_i-\bar{X})^2} \sqrt{\sum_i^n(Y_i-\bar{Y})^2}} = \frac{\sum XY - \frac{\sum X \sum Y}{n}}{\sqrt{\sum X^2 - \frac{(\sum X)^2 }{n}} \sqrt{\sum Y^2 - \frac{(\sum Y)^2 }{n}}}\]</span> The Pearson R has following properties:</p><ul><li>The range of Pearson R is [-1,1], positive numbers indicate positive correlation</li><li>The Pearson R represents the linear correlation between two variables, linear transformation of X or Y does not change Pearson R</li><li>X and Y must be numerical variables and follow normal distribution</li><li>The observations of X and Y are in pairs</li></ul><h4 id="spearman">1.2.2 Spearman</h4><p>Suppose we have samples of X and Y with n observations. Sort the observations X and Y to obtain two new set <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, where <span class="math inline">\(a_i,b_i\)</span> is the rank of <span class="math inline">\(X_i,Y_i\)</span> in X,Y</p><p>define Spearman correlation as: <span class="math display">\[\rho = \frac{6\sum_i^n(a_i-b_i)^2}{n(n^2-1)}\]</span> The Spearman correlation has following properties:</p><ul><li>The range of Spearman correlations is [-1,1], positive numbers indicate positive correlation</li><li>The Spearman correlation represents rank correlation (simply based on size relationship)</li><li>X and Y does not need to follow certain distribution. Aithough the sample must be the same, they do not need to be in pairs<br></li><li>The statistical power of Spearman is relatively lower</li></ul><p><img src="/2022/02/09/feature-selection-stats-method/1.png"></p><h4 id="kendall">1.2.3 Kendall</h4><p>Suppose we combine variables X and Y into a new element <span class="math inline">\((X,Y)\)</span>. If two elements <span class="math inline">\((X_i, Y_i)\)</span> and <span class="math inline">\(X_j,Y_j\)</span> satisfy either of these two case:</p><ul><li><span class="math inline">\(X_i &gt; X_j\)</span> and <span class="math inline">\(Y_i &gt;Y_j\)</span></li><li><span class="math inline">\(X_i &lt; X_j\)</span> and <span class="math inline">\(Y_i &lt;Y_j\)</span></li></ul><p>Then we call these two elements have consistency</p><p>if <span class="math inline">\(X_i = X_j\)</span> and <span class="math inline">\(Y_i = Y_j\)</span>, we regard these two elements as having neither consistency nor inconsistency. Otherwise, we call these two elements have inconsistency</p><p>define Kendall correlation as: <span class="math display">\[\tau = \frac{C-D}{\sqrt{N3-N1}\sqrt{N_3-N_2}{}}\]</span> where:</p><ul><li><p>C is number of paris of elements(Two element <span class="math inline">\((X _1,Y_1),(X_2,Y_ 2)\)</span> is one pair) that have consistency</p></li><li><p>D is number of paris of elements that have inconsistency</p></li><li><p><span class="math inline">\(N1 = \sum_i^s \frac{ 1}{2}U_i(U_i-1)\)</span>, where:</p><ul><li>s is the number of values in X that appears more than once</li><li><span class="math inline">\(U_i\)</span> is the number those values appears(For <span class="math inline">\(i^{th}\)</span> Values in s)</li><li>For example, for a X={1,2,2,3,3,3,4}, s=2, <span class="math inline">\(U_1 = 2, U_2 = 3\)</span></li></ul></li><li><p>N2 is calculated same way as N1 on Y</p></li><li><p><span class="math inline">\(N3 = \frac{1}{2}N(N- 1)\)</span>, where is the number of samples</p></li></ul><p>The Kendall correlations have similar conditions as Pearson R. The only difference is it represents rank correlation instead of linear correlation</p><ul><li>The range of Pearson R is [-1,1], positive numbers indicate positive correlation</li><li>X and Y must be numerical variables and follow normal distribution</li><li>The observations of X and Y are in pairs</li></ul><p><strong>An implementation of feature selection based on Pearson R in Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> pearsonr<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_pscore</span>(<span class="hljs-params">x,y</span>):</span><br>    k = np.zeros([<span class="hljs-number">4</span>,<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,x.shape[<span class="hljs-number">1</span>]):<br>        temp = pearsonr(x[:,i],y[:,<span class="hljs-number">0</span>])<br>        k[i,:] = np.array(temp)<br>    <span class="hljs-keyword">return</span> [k.T[<span class="hljs-number">0</span>],k.T[<span class="hljs-number">1</span>]]<br><span class="hljs-comment"># 下面的写法有问题理论上应该与上面的函数等价，实际上在调用np.array时，会把最内层的pearsonr系数值变成一个数组对象，导致SelectKBest无法</span><br><span class="hljs-comment"># 遍历，需要用astype把第一列转化为float，但这样很难封装进lambda函数里，不如不用，写这个的人铁nt，害我研究一个多小时</span><br><span class="hljs-comment"># get_all_pscore =  lambda X,Y:np.array(list(map(lambda x:pearsonr(x,Y),X.T))).T</span><br><br><span class="hljs-comment">#第一个参数为一个callable()，该函数以feature，target为输入，输出可以是两种：</span><br><span class="hljs-comment"># 1. 一对数组pearsonr系数-p值（可以是tuple中两个数组，也可以是ndarray，总之第一个维度须为2</span><br><span class="hljs-comment"># 2. 一个单一的数组（只有相关系数）</span><br><span class="hljs-comment"># 第二个参数k，代表选择前k个</span><br>data_1 = SelectKBest(get_all_pscore(),k=<span class="hljs-number">2</span>).fit_transform(data_0,target_0)<br></code></pre></td></tr></table></figure><h3 id="hypothesis-testing">1.3 Hypothesis Testing</h3><h4 id="chi-squared-test">1.3.1 Chi-Squared Test</h4><p>chi-squared test is a <strong>supervised</strong> hypothesis testing method to calculate the probability that 2 variables are correlated</p><ol type="1"><li><p>According to the observations, make the frequency table</p></li><li><p>calculate the row total and the column total</p></li><li><p>calculate the expectations for each cell by <span class="math inline">\(\frac{R_i*C_j}{total}\)</span> (<span class="math inline">\(\frac{125*310}{600} = 65\)</span>,etc.)</p></li><li><p>the chi2 of each cell and the total chi2 are given by <span class="math display">\[\chi_{i,j}^2 = \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\\chi^2 = \sum_{i,j}^{r,c}\chi_{i,j}^2\]</span></p></li><li><p>calculate the degree of freedom, <span class="math inline">\(k=(r-1)(c-1)\)</span></p><table><thead><tr class="header"><th>movie type</th><th>High POPULARITY</th><th>low POPULARITY</th><th>Row total</th></tr></thead><tbody><tr class="odd"><td>type1</td><td>50(65)</td><td>75(60)</td><td>125</td></tr><tr class="even"><td>type2</td><td>125(155)</td><td>175(145)</td><td>300</td></tr><tr class="odd"><td>type3</td><td>...</td><td>...</td><td>...</td></tr><tr class="even"><td>type4</td><td>...</td><td>...</td><td>...</td></tr><tr class="odd"><td>column total</td><td>310</td><td>290</td><td>600</td></tr></tbody></table></li><li><p>check the significance table of chi distribution to find the value of the random variable under given k and <span class="math inline">\(\alpha\)</span></p></li><li><p>compare the <span class="math inline">\(\chi^2\)</span>z with the rejection area, if <span class="math inline">\(\chi^2\)</span> &gt; the found value(which means p value is smaller than <span class="math inline">\(\alpha\)</span>), then reject the null hypothesis</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest, chi2<br>X, y = data_0,target_0<br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><h3 id="mutual-information">1.4 Mutual Information</h3><p>For the theoretical part about Entrophy and Mutual Information, refer to <a href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">this article</a> <span class="math display">\[MI = H(x,y) - H(x|y) - H(y|x) = \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)}\\\]</span> Suppose we have an input variable X with n samples and m unique values {<span class="math inline">\(x_1 = c_ 1,x_2 =c_ 1,...x_n = c_m\)</span>} and an output variable Y with n samples and k unique values {<span class="math inline">\(y_1 = c_ 1,y_2 =c_ 1,...y_n = c_k\)</span>}</p><p>It's easy to calculate <span class="math inline">\(P(X=c_i),P(Y=c_j),P(X=c_i,Y=c_j)\)</span></p><p>Thus the MI can be calculated. The greater MI X and Y share, the greater dependency there exists, and X is thus a more important feature.</p><p>The MI mtheod have the following properties:</p><ul><li>MI method is sometime impractical with two continuous numerical variables, since there are too many unique values</li><li>MI needs some certain metrics to map the original values in to a range(usually [0,1]), so that MI score of different variables can be compared</li></ul><p>An implementation of feature selection based on Mutual Information in Python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mutual_info_score<br>MI_matrix = np.zeros([data_and_target.shape[<span class="hljs-number">1</span>],data_and_target.shape[<span class="hljs-number">1</span>]])<br><span class="hljs-comment"># calculate the MI matrix between each two feature</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,data_and_target.shape[<span class="hljs-number">1</span>]):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,data_and_target.shape[<span class="hljs-number">1</span>]):<br>        MI_matrix[i,j] = mutual_info_score(data_and_target[i+<span class="hljs-number">1</span>],data_and_target[j+<span class="hljs-number">1</span>])<br>df = pd.DataFrame(MI_matrix,columns=data_and_target.columns,index=data_and_target.columns)<br><span class="hljs-built_in">print</span>(df)<br></code></pre></td></tr></table></figure><h2 id="wrapper">2.Wrapper</h2><p>Through the evaluation of the model, add or drop some feature each trail to obtain a subset of all features, which is the selected feature.</p><p><strong>Pro</strong>: accurate, model-relevant</p><p><strong>Con</strong>: time-consuming</p><h3 id="recursive-feature-elimination">2.1 Recursive Feature Elimination</h3><ol type="1"><li>Train model with all m features</li><li>Select k best features and to take them out(or drop k worst feature from all feature)</li><li>Train the model with the rest feature and repeat step2, until we reach the max/min number of feature we want</li><li>The taken-out/ left-in feature is the final features space we want to preserve</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFECV<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-comment"># REF works for all models that have a weight on features</span><br><span class="hljs-comment"># step: the number(or precentage) of feature to eliminate in each iteration</span><br>estimator = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>)<br>selector = RFE(estimator= estimator, n_features_to_select= <span class="hljs-number">3</span>,step=<span class="hljs-number">1</span>)<br>data_transformed = selector.fit_transform(data_0,target_0)<br><br><span class="hljs-comment"># REFCV</span><br><span class="hljs-comment"># REF with cross validation</span><br>cv = StratifiedKFold(n_splits=<span class="hljs-number">5</span>)<br>selector = RFECV(estimator= estimator, min_features_to_select= <span class="hljs-number">3</span>, cv=cv, step=<span class="hljs-number">1</span>)<br>data_transformed = selector.fit_transform(data_0,target_0)<br></code></pre></td></tr></table></figure><h3 id="step-wise-regression">2.2 Step-wise Regression</h3><h2 id="embedded">3. Embedded</h2><p>Some models, like Lasso, Ridge, and Random Forest has method embedded in the model to evaluate features. Train these model first and than make selection of feature.</p><p><strong>Pro</strong>: fast, easy to apply</p><p><strong>Con</strong>: ignore the dependence of features</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br><br>estimator = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>)<br><span class="hljs-comment"># threshold:</span><br><span class="hljs-comment"># threshold of the feature importance to drop</span><br><span class="hljs-comment"># if you apply l1 in your model(or use lasso), the threshold is 1e-5 by default</span><br><span class="hljs-comment"># otherwise, the threshold is mean by default, which means features with a importance less than mean importance will</span><br><span class="hljs-comment"># all by dropped</span><br>selecor = SelectFromModel(estimator,threshold=<span class="hljs-number">0.03</span>)<br>data_transformed = selecor.fit_transform(data_0,target_0)<br><br><br>estimator = LinearSVC(C=<span class="hljs-number">0.01</span>,penalty=<span class="hljs-string">&#x27;l1&#x27;</span>,dual=<span class="hljs-literal">False</span>).fit(data_0,target_0)<br><span class="hljs-comment"># prefit: whether the model given to the selector is already fit</span><br>selecor = SelectFromModel(estimator,threshold=<span class="hljs-number">0.03</span>,prefit=<span class="hljs-literal">True</span>)<br>data_transformed = selecor.transform(data_0)<br><span class="hljs-built_in">print</span>(data_transformed)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Feature Selection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL 基础知识</title>
    <link href="/2022/02/09/MySQL-Basic/"/>
    <url>/2022/02/09/MySQL-Basic/</url>
    
    <content type="html"><![CDATA[<h1 id="mysql基础知识">MySQL基础知识</h1><h2 id="数据库三大范式">1. 数据库三大范式</h2><ol type="1"><li><p>数据库表中，所有字段都是不可分解的原子值，互相不依赖</p></li><li><p>所有非主键字段都依赖于主键</p><p>（学号，姓名，专业名称）其中专业名称不依赖于主键，所以不满足第二范式</p></li><li><p>所有非主键属性都直接依赖于主键</p><p>（学号，姓名，专业id，专业名称）其中包含间接依赖关系 学校-&gt;专业id-&gt;专业名称，不满足第三范式</p></li></ol><h2 id="mysql的架构">2. MySQL的架构</h2><p>MySQL可以分为应用层,逻辑层,数据库引擎层,物理层。</p><p>应用层：负责和客户端，响应客户端请求，建立连接，返回数据</p><p>逻辑层：包括SQK接口，解析器，优化器，Cache与buffer</p><p>数据库引擎层：有常见的MyISAM,InnoDB等等</p><p>物理层：负责文件存储，日志等等</p><h3 id="一条sql语句执行的过程">2.1 一条SQL语句执行的过程</h3><ol type="1"><li>客户端首先通过连接器进行身份认证和权限相关</li><li>如果是执行查询语句的时候，会先查询缓存，但MySQL 8.0 版本后该步骤移除。</li><li>没有命中缓存的话，SQL 语句就会经过解析器，分析语句，包括语法检查等等</li><li>通过优化器，将用户的SQL语句按照 MySQL 认为最优的方案去执行。</li><li>执行语句，并从存储引擎返回数据</li></ol><h3 id="buffer-与-cache-的对比">2.2 buffer 与 cache 的对比</h3><ol type="1"><li>Cache(如Qcache)一般用来把固定语句对应的结果集放在内存，目的是为了提高读取速度</li><li>buffer（如buffer pool）一般用来把页面加载到内存，每次写入时，先更新buffer pool中的日志（redo log），把要修改的页记为脏页。后台进程每隔一段时间将buffer pool中的日志进行刷盘，从而提高了磁盘IO效率，降低了大量写入带来的冲击</li></ol><h2 id="mysql的引擎">3. MySQL的引擎</h2><h3 id="mysql支持的引擎">3.1 MySQL支持的引擎</h3><table><thead><tr class="header"><th>InnoDB</th><th>MyISAM</th></tr></thead><tbody><tr class="odd"><td>支持事务</td><td>不支持</td></tr><tr class="even"><td>支持外键</td><td>不支持</td></tr><tr class="odd"><td>聚集索引</td><td>非聚集索引</td></tr><tr class="even"><td>行级锁和表级锁</td><td>表级锁</td></tr></tbody></table><p>Memory 存储引擎：</p><p>Memory存储引擎将所有数据都保存在内存，不需要磁盘 IO。支持哈希索引，因此查找速度极快。Memory 表使用表级锁，因此并发写入的性能较低。</p><h3 id="innodb的索引">3.2 InnoDB的索引</h3><p>索引是一种数据结构，用于帮助存储引擎快速的找到数据，索引存储在内存上</p><h4 id="聚集索引和非聚集索引">3.2.1 聚集索引和非聚集索引</h4><p>聚集索引中，索引和数据绑定在一起，通过查找索引可以直接返回数据。主键都是聚集索引。非聚集索引中，数据和索引分离，通过一个或多个非聚集索引的查询，查到对应某一值的主键值/地址，然后再使用主键值找到数据</p><p>有时非聚集索引无法直接找到主键值，只能查到满足非聚集索引某一组值的锁欧聚集索引，此时会回到聚集索引中根据主键值继续查找数据，这一过程称为<strong>回表查询</strong></p><h4 id="联合索引和最左匹配原则"><strong>3.2.2 联合索引和最左匹配原则：</strong></h4><p>联合索引是指对表上的多个列的关键词进行索引。</p><p>对于联合索引的查询，如果精确匹配联合索引的左边连续一列或者多列，则mysql会一直向右匹配直到遇到范围查询（&gt;,&lt;,between,like）就停止匹配。where语句中，所有命中的索引被称为匹配列，没有命中但出现在索引中的成为过滤列，所有不在索引中的成为非匹配列</p><p>例如联合索引为（user, age, id), 而范围查询中的谓语为“where id = 1 and age &gt; 10 and user = wang and sex = 'M' ”,此时：</p><ol type="1"><li>在where语句语句中查找谓语user，命中，user为匹配列</li><li>在where语句语句中查找谓语age，命中，但age为范围查询，因此age后的列无法再命中</li><li>id未命中，但出现在索引中，为过滤列</li><li>sex不在索引中，为非匹配列</li></ol><p>对于user和sex，SQL将在内存中的索引（b+树）进行查询（通过explain可查看查询方式为REF）</p><p>对于id，SQL将在符合user，age条件的子树上遍历，这样的复杂度并不降低，但由于是在内存上执行，速度仍然比回表后快（通过explain可查看查询方式为INDEX）</p><p>对于sex，SQL将先回表，返回所有满足user，age，id条件的数据结果，然后再遍历获得满足sex条件的数据结果（通过explain可查看查询方式为ALL）</p><p>Mysql会对第一个索引字段数据进行排序，在第一个字段基础上，再对第二个字段排序。</p><h3 id="索引的数据结构">3.3 索引的数据结构</h3><p>InnoDB采用的是B+Tree索引</p><p>B-Tree：一种自平衡多插树。每个节点都存储key和value。因为每个节点都有数据，所以查询效率较高</p><p>B+Tree：也是自平衡多叉树，但中间节点不存放数据只存放key。只在叶节点存放数据，结构矮胖，出度更大，可以在相同的磁盘空间下容纳更多数据。且在叶节点之间链指针，所以进行范围查询时只需要遍历叶节点即可。</p><p>Hash索引：哈希索引对于每一行数据计算一个哈希码，并将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。只有 Memory 引擎显式支持哈希索引。</p><p><strong>为何不使用红黑树：</strong></p><p>红黑树是二叉查找树，出度为2，因此红黑树存储一张表，高度会比B Tree大很多，IO次数多，检索时间长</p><h2 id="mysql的事务">4. MySQL的事务</h2><h3 id="acid">4.1 ACID</h3><p>事务满足如下几个特性：</p><ul><li><p>原子性（Atomicity）: 一个事务中的所有操作要么全部完成，要么全部不完成。</p></li><li><p>一致性（Consistency）: 事务执行前后数据库的状态保存一致。</p></li><li><p>隔离性（Isolation） 多个并发事务对数据库进行操作，事务间互不干扰。</p></li><li><p>持久性（Durability） 事务执行完毕，对数据的修改是永久的，即使系统故障也不会丢失</p></li></ul><h3 id="事务的并发问题和隔离等级">4.2 事务的并发问题和隔离等级</h3><p>并发问题：</p><ul><li>丢失修改</li><li>脏读：当前事务可以查看到别的事务未提交的数据。</li><li>不可重读：在同一事务中，使用相同的查询语句，同一数据资源莫名改变了。</li><li>幻读：在同一事务中，使用相同的查询语句，莫名多出了一些之前不存在的数据，或莫名少了一些原先存在的数据，例如第二次读取前，表格被插入了新行。</li></ul><p>隔离等级</p><ul><li><p>读未提交： 一个事务还没提交，它做的变更就能被别的事务看到。</p></li><li><p>读已提交： 一个事务提交后，它做的变更才能被别的事务看到。</p></li><li><p>可重复读： 一个事务执行过程中看到的数据总是和事务启动时看到的数据是一致的。在这个级别下事务未提交，做出的变更其它事务也看不到。</p></li><li><p>串行化： 对于同一行记录进行读写会分别加读写锁，当发生读写锁冲突，后面执行的事务需等前面执行的事务完成才能继续执行。</p></li></ul><p>并发问题的解决：</p><p><img src="/2022/02/09/MySQL-Basic/1.png"></p><h2 id="mysql的锁innodb">5. MySQL的锁（InnoDB）</h2><h3 id="锁的性质">5.1 锁的性质</h3><h4 id="共享性">5.1.1 共享性</h4><p>共享锁：其他事务可以读但不能写，又称为读锁</p><p>排他锁：其他事务不能读写，又称为写锁</p><h4 id="粒度">5.1.2 粒度</h4><p><strong>表级锁:</strong> 对当前操作的整张表加锁,实现简单，加锁快，但并发能力低。</p><p><strong>行锁:</strong> 锁住某一行，如果表存在索引，那么记录锁是锁在索引上的，如果表没有索引，那么 InnoDB 会创建一个隐藏的聚簇索引加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁慢，会出现死锁。</p><p><strong>Gap 锁</strong>：锁住一个间隙以防止插入，但不包括间隙的两端。假设索引列有2, 4, 8 三个值，如果对 4 加行锁，那么也会同时对(2,4)和(4,8)这两个间隙加锁。其他事务无法插入值所对应的索引值在这两个间隙之间的记录</p><p><strong>next-key-lock：</strong>next-key lock 实际上就是 行锁+这条记录前面的 gap lock 的组合。假设有索引值10,11,13和 20,那么可能的 next-key lock 包括:</p><p>(负无穷,10],(10,11],(11,13],(13,20],(20,正无穷)</p><p>在 RR 隔离级别下，InnoDB 使用 next-key lock 主要是防止幻读问题产生。</p><h4 id="意向锁">5.1.3 意向锁</h4><p>意向锁必须是表级锁，可以有共享性和排他性。意向锁的作用在于保护不同级别的锁共享性不冲突，或者说提高检测这一冲突的效率。</p><p>假设事务A正在update表t的某一行r，并给这一行添加了排他锁，同时，事务B想要给表t上一个表级共享锁，为了防止共享性冲突，事务B必须检查表t的每一行，检查是否有A上的锁这样的排他锁，然后才能上锁</p><p>因此，A给表t中的行r上排他锁时，同时还会给表t上一个表级的排他意向锁，此时B所上的表级共享锁将会被阻塞</p><p>注意，意向锁只会阻塞表级锁，上例中，如果有另一事务C试图给另一行r2加共享锁，则A所加的排他意向锁不会对其进行阻塞</p><h4 id="死锁">5.1.4 死锁</h4><p>两个事物同时上锁进入互相等待的死循环。</p><p>如何解决死锁：</p><ol type="1"><li>合理设计索引，尽可能通过索引定位行，防止过多的表和行被锁住，减少竞争</li><li>调整SQL语句顺序，update，delete等长时间持有锁的语句放在后面</li><li>把大事物分解成小事务执行</li><li>必要时，可以使用<code>KILL</code>杀死一些进程，释放其持有的锁</li></ol><p>详细分析方法可参考<a href="https://z.itpub.net/article/detail/7B944ED17C0084CF672A47D6E938B750">这篇文章</a></p><h4 id="乐观性">5.1.5 乐观性</h4><p>乐观锁：对于数据冲突保持一种乐观态度，操作数据时不会对操作的数据进行加锁，只有到数据提交的时候才通过一种机制来验证数据是否存在冲突。</p><p>悲观锁：对于数据冲突保持一种悲观态度，在修改数据之前把数据锁住，然后再对数据进行读写，在它释放锁之前任何人都不能对其数据进行操作，直到前面一个人把锁释放后下一个人数据加锁才可对数据进行加锁，然后才可以对数据进行操作，一般数据库的锁都是悲观锁</p><h3 id="上锁方式">5.2 上锁方式</h3><h4 id="表锁">5.2.1 表锁</h4><p>隐式上锁：使用一些关键词时，自动添加自动释放的上锁。如select语句会给表上共享锁，insert，update，delete会给表上排他锁</p><p>显示上锁：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs SQL">LOCK <span class="hljs-keyword">TABLE</span> t1 READ  <span class="hljs-comment">-- 给t1表上共享锁</span><br>LOCK <span class="hljs-keyword">TABLE</span> t1 WRITE  <span class="hljs-comment">-- 给t1表上排他锁</span><br>UNLOCK <span class="hljs-keyword">TABLE</span> t1 <span class="hljs-comment">-- 解锁t1</span><br>UNLOCK TABLES <span class="hljs-comment">-- 解锁所有表</span><br></code></pre></td></tr></table></figure><h4 id="行锁">5.2.2 行锁</h4><p>隐式上锁：使用一些关键词时，自动添加自动释放的上锁。insert，update，delete会给选中的行上排他锁。注意select不会给行加锁</p><p>显示上锁：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> t1 <span class="hljs-keyword">IN</span> SHARE MODE <span class="hljs-comment">-- 给选中的行加共享锁</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> t1 <span class="hljs-keyword">FOR</span> UPDATE <span class="hljs-comment">-- 给选中的行加排他锁</span><br><span class="hljs-keyword">COMMIT</span><br><span class="hljs-keyword">ROLLBACK</span> <span class="hljs-comment">-- 事务完成或回滚时，会解除事物中所加的行锁</span><br>KILL <span class="hljs-comment">-- 杀死某一进程，会解除这一进程所加的行锁</span><br></code></pre></td></tr></table></figure><h2 id="mysql的日志">6. MySQL的日志</h2><h3 id="log种类">6.1 log种类</h3><p>redo log: 存储引擎级别的log（InnoDB有，MyISAM没有），该log关注于事务的恢复。在重启mysql服务的时候，根据redo log进行重做，从而使事务有持久性。</p><p>undo log：是存储引擎级别的log（InnoDB有，MyISAM没有）保证数据的原子性，该log保存了事务发生之前的数据的一个版本，可以用于回滚，是MVCC的重要实现方法之一。</p><p>如果需要执行事务，使用redo log，执行失败则使用undo log，这样的组合保证了事务的一致性</p><p>bin log：数据库级别的log，关注恢复数据库的数据。有关bin log和redo log的区别</p><ol type="1"><li>redo log是InnoDB引擎特有的，只记录该引擎中表的修改记录。binlog是MySQL的Server层实现的，会记录所有引擎对数据库的修改，具有崩溃修复能力。InnoDB通过redo log保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe</li><li>redo log是物理日志，记录的是在具体某个数据页上做了什么修改；binlog是逻辑日志，记录的是这个语句的原始逻辑，例如语句的增删改，并不记录数据页具体发生了什么改变，因此单独的bin log不具备崩溃修复能力。</li><li>redo log是循环写的，空间固定会用完；binlog是可以追加写入的，binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</li></ol><h3 id="wal技术">6.2 WAL技术</h3><p>WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。事务在提交写入磁盘前，会先写到redo log里面去。如果直接写入磁盘涉及磁盘的随机I/O访问，涉及磁盘随机I/O访问是非常消耗时间的一个过程，相比之下先写入redo log，后面再找合适的时机批量刷盘能提升性能。</p><h3 id="两阶段提交">6.3 两阶段提交</h3><p>为了保证binlog和redo log两份日志的逻辑一致，最终保证恢复到主备数据库的数据是一致的，采用两阶段提交的机制。</p><ol type="1"><li>执行器调用存储引擎接口，存储引擎将修改更新到内存中后，将修改操作记录redo log中，此时redo log处于prepare状态。</li><li>存储引擎告知执行器执行完毕，执行器生成这个操作对应的binlog，并把binlog写入磁盘。</li><li>执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交commit状态，更新完成</li></ol><h3 id="mysql-主从机制">6.4 MySQL 主从机制</h3><h4 id="主从配置">6.4.1主从配置</h4><p>一个服务器充当主服务器（master），其余的服务器充当从服务器（slave）。从服务器主要用来读，主服务器主要用写，从服务器定期与主服务器进行同步，复制其数据。因为复制是异步进行的，所以从服务器不需要一直连接着主服务器</p><p>主从机制的优点：</p><ol type="1"><li>数据存在多个镜像，可以防止单一主机崩溃和数据丢失，如果主机宕机可以切换到从服务器上（由于异步同步，可能数据一致性存在问题）</li><li>从服务器可以分担主服务器的读的压力</li></ol><p>注意，部分数据实时性强，经常会被更新，这类数据不适合放在从服务器上，容易引发错误</p><h4 id="主从复制">6.4.2 主从复制</h4><p>MySQL主从复制流程：</p><ol type="1"><li>在事务完成之前，主库在binlog上记录这些改变，完成binlog写入过程后，主库通知存储引擎提交事物</li><li>从库将主库的binlog复制到对应的中继日志，即开辟一个I/O工作线程，I/O线程在主库上打开一个普通的连接，然后开始binlog dump process，将这些事件写入中继日志。从主库的binlog中读取事件，如果已经读到最新了，线程进入睡眠并等待ma主库产生新的事件。</li></ol><h2 id="多版本并发控制mvcc">7. 多版本并发控制(MVCC)</h2><p><strong>多版本并发控制（MVCC）</strong> 是通过保存数据在某个时间点的快照来实现并发控制的。也就是说，不管事务执行多长时间，事务内部看到的数据是不受其它事务影响的，根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。</p><p>可以认为MVCC 是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此可以实现读写并发，并降低死锁概率，IO开销更低。</p><p>MVCC只在 可重复读（REPEATABLE READ） 和读已提交（READ COMMITTED） 两个隔离级别下工作。其他两个隔离级别都和 MVCC 不兼容</p><p>通过MVCC，不显式加锁的一般select语句使用的都是快照读，即读取数据在事务语句执行前或执行后的历史版本，而非当前数据，也即是说其他并发的事物无法影响到该快照。而显式加锁的select语句执行的是当前读，获得目标位置最新的数据</p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
      <tag>Interview Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic Statistics for Data Science</title>
    <link href="/2022/02/09/statistics-for-ds/"/>
    <url>/2022/02/09/statistics-for-ds/</url>
    
    <content type="html"><![CDATA[<h1 id="statistics-for-data-science">Statistics for Data Science</h1><h2 id="basic-concepts-for-statistics">1. Basic Concepts for Statistics</h2><p><strong>Statistics</strong>: Statistics gather, describe and analyze sample data in a numerical way to understand the whole population</p><p><strong>Target Population</strong>: A particular group of interest, the distribution of the target population is called</p><p><strong>Sample Population</strong>: A group which the sample is taken from. In most cases it equals to the target population</p><p><strong>Sample</strong>: A subset of the sample population from which data are collected, Doing statistics is a process trying to learn information of the target population through samples. Ideally, sample should be representative to sample population, the sample population should be the same or representative to target population.</p><p><img src="/2022/02/09/statistics-for-ds/1.PNG" alt style="zoom:80%;"></p><p><strong>Variable</strong>: A dimension of a sample representing a specific measure. A sample can contain multiple variables</p><p><strong>Data </strong>: the actual counts, measurements or observation about the variables that markdown with samples</p><p><strong>Data Point and Sample Size</strong>: Data point is a single record of data in the sample. Sample size is the number of data point in the sample</p><p><strong>Parameter</strong>: A numerical description of a population characteristic. Note that a parameter of target population and sample is not the same. We cannot construct statistics with unknown parameters of the whole population</p><p><strong>Sample Statistics</strong>: A function constructed from the sample. A statistics should containing no <strong>unknown</strong> parameters. Common sample statistics includes:</p><table><thead><tr class="header"><th>Sample Statistics</th><th>Format</th></tr></thead><tbody><tr class="odd"><td>mean</td><td><span class="math inline">\(\bar{X} = \frac{1}{n}\sum_i^nX_i\)</span></td></tr><tr class="even"><td>biased variance</td><td><span class="math inline">\(S_0^2 = \frac{1}{n}\sum_i^n(X_i - \bar{X})^2\)</span></td></tr><tr class="odd"><td>unbiased variance</td><td><span class="math inline">\(S^2 = \frac{1}{n-1}\sum_i^n(X_i - \bar{X})\)</span></td></tr><tr class="even"><td>standard deviation</td><td><span class="math inline">\(S = \sqrt{S^ 2}\)</span></td></tr><tr class="odd"><td>moment</td><td><span class="math inline">\(A_k = \frac{1}{n}\sum_i^nX_i^k\)</span></td></tr><tr class="even"><td>central moment</td><td><span class="math inline">\(B_ k = \frac{1}{n}\sum_i^n(X_i - \bar{X})^k\)</span></td></tr></tbody></table><h2 id="data-classification">2. Data Classification</h2><h3 id="categorical-numeric">2.1 Categorical &amp; Numeric</h3><p><strong>Categorical</strong>: consist of labels or description of traits. It's meaning less to apply quantitative calculation on it</p><p><strong>Numeric</strong>: consist of counts and measurement, it have meanings when you apply quantitative calculations</p><h3 id="discrete-continuous">2.2 Discrete &amp; Continuous</h3><p><strong>Discrete</strong>: numeric data that can take only particular values(counts, rate stars)</p><p><strong>Continuous</strong>: numeric data that can take any values in an interval</p><p><img src="/2022/02/09/statistics-for-ds/3.PNG"></p><h3 id="noir">2.3 NOIR</h3><p><strong>Nominal level data</strong>: description of categorical data that order do not matters</p><p><strong>Ordinal level data</strong>: categorical data that have a meaningful order(still not meaningful to add or divide)</p><p><strong>Interval level data</strong>: numeric data that can be arranged in a meaningful order, and the different between data entries are meaningful(timestamp, shoe size, temperature degree)</p><p><strong>Ratio level data</strong>: Interval data where zero indicates absence of something(like body height, 0 inch body high do not have a actual meaning, it means the data are not collected, where 0 degree do have an actual meaning, so it is not a ratio data.) As ratio data cannot participate calculation if data is not included, it must be non-zero in a calculation, thus it can divide other numeric data, that is why it is called ratio data</p><p><img src="/2022/02/09/statistics-for-ds/2.PNG"></p><h2 id="two-important-theorem">3. Two Important Theorem</h2><p>There are two important probability theorem for statistics:</p><h3 id="law-of-large-number">3.1 Law of Large Number</h3><p><strong>Chebyshev's inequality</strong></p><p>For a random variable X, if E[X] and V[X] both exist: <span class="math display">\[P(|X-X[x ]| \ge \epsilon ) \le \frac{D[X]}{\epsilon^2}\]</span> This inequality implies taht the probability of an observation fall far from the expectation is small. The greater <span class="math inline">\(\epsilon\)</span> is, the smaller this probability is.</p><p><strong>Chebyshev's Law of Large Number</strong></p><p>Let a sequence <span class="math inline">\(X_n \to a\)</span>: <span class="math display">\[\lim_{n\to \infty} P(|X_n - a| &lt; \epsilon) = 1 \qquad \forall \epsilon\]</span> This law implies that a statistics on a sample would approaches the same statistics on the population as the sample size is big. In other word, the sample can represent the population when n is big.</p><h3 id="central-limit-theorem">3.2 Central Limit Theorem</h3><p>If a random phenomenon is caused by numerous factors that have same distribution but are independent to each other, then the limit of thu sum of these factors, which is the phenomenon, follows a normal distribution.</p><p>The CLT can be expressed in the following format:</p><p><strong>Lindeberg–Lévy CLT</strong></p><p>Let <span class="math inline">\(X_1, X_2,..X_n\)</span> be a series of independent random variables following same distribution, <span class="math inline">\(E[X_i] = \mu, V[X_ i] = \sigma^2\)</span>, then <span class="math display">\[\lim_{n\to \infty}P(\frac{ \sum_i^n X_ i-n\mu}{\sqrt{n}\sigma } \le X) = \Phi_0(X)\]</span> where <span class="math inline">\(\Phi_0\)</span> is a standard normal distribution</p><p>This theorem implies that when n is big enough, let <span class="math inline">\(Y = \sum_i^nX_i\)</span>, we can regarding Y as following a normal distribution <span class="math inline">\(N(n\mu, n\sigma)\)</span></p><p>Such a conclusion has very important meaning to hypothesis testing. It indicates that, if a statistics is constructed through through adding up sample point, lilke mean, then this statistics should follow a normal distribution no matter what distribution each sample follows. In hypothesis test, if we want to test on the mean of a measure of the population, we can regard that measure of each sample point as a random variable, these variables are independent and same-distributed, so no matter what distribution that measure follows, the mean of it on the total should follow a normal distribution. According to law of large number, as long as n is big enough, the mean on the sample should also follow normal distribution. Thus CLT make hypothesis testing on mean statistics possible.</p><p>Let <span class="math inline">\(\mu, \sigma^2\)</span> be the mean and variance of the population, <span class="math inline">\(\bar{X},S^2\)</span> be the mean and variance of the sample. According to Law of Large Number and CLT: <span class="math display">\[E[\bar{ X}] = \mu\]</span></p><p><span class="math display">\[V[\bar{X}] = \frac{1}{n}\sigma^2\]</span></p><p><span class="math display">\[E[S^2] = \sigma^2\]</span></p><h2 id="topic-in-applied-statistics">4. Topic in Applied Statistics</h2><p>Some topics in statistics are widely applied in domains like machine learning and A/B Test</p><h3 id="sampling">1.Sampling</h3><h3 id="probability-density-estimation">2. Probability Density Estimation</h3><h3 id="statistical-learning-and-machine-learning">3. Statistical Learning and Machine Learning</h3><h3 id="experiment-and-hypothesis-testing">4. Experiment and Hypothesis Testing</h3><h3 id="observational-study-and-causal-inference">5. Observational Study and Causal Inference</h3>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Knowledge</tag>
      
      <tag>Statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MongoDB Guildbook</title>
    <link href="/2022/02/06/MongoDB-Guildline/"/>
    <url>/2022/02/06/MongoDB-Guildline/</url>
    
    <content type="html"><![CDATA[<h1 id="mongodb">MongoDB</h1><h2 id="about-mongodb">About MongoDB</h2><p>MongoDB is a open- source NoSQL database stroing data in json like documents with schema.</p><p>mongoDB do not have concepts like join.</p><p>mongoDB provides APIs for most programing language</p><h2 id="three-ways-to-access-mongodb">Three ways to access MongoDB</h2><ol type="1"><li>Community server</li><li>VS extending</li><li>MongoDB Altas</li></ol><h2 id="concepts-in-mondodb">Concepts in MondoDB:</h2><p><strong>Document:</strong> a set of K-V pairs. Every document has a unique value via key "_id". Documents have dynamic schema, documents in same collection can have different schema. They. Can hold data of any types allowed by mongodb.</p><p><strong>Collection:</strong> a group of mongodb documents, similar to "tables in other database. Unlike tables, collections does nothave any schema definition, and it cannnot be join. Usually, documents with in a collection belonging to a particular subject.</p><p><strong>Database:</strong> A database is a container of collections of data</p><p><strong>Comparison between RDBMS and MongoDB</strong></p><table><thead><tr class="header"><th style="text-align: left;">RDBMS</th><th style="text-align: left;"></th><th></th><th></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Tables</td><td style="text-align: left;">stand for entity</td><td>Collections</td><td>A set of documents representing one object</td></tr><tr class="even"><td style="text-align: left;">rows</td><td style="text-align: left;">stand for an actual record</td><td>Documents</td><td>a json objects</td></tr><tr class="odd"><td style="text-align: left;">columns</td><td style="text-align: left;">stand for attributes</td><td>Fields</td><td>The first level of the schema</td></tr></tbody></table><h2 id="start-with-mongodb">Start with MongoDB</h2><ol type="1"><li><p>Build a sever(for Mac)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mongod --config /opt/homebrew/etc/mongod.conf<br></code></pre></td></tr></table></figure><p>This create a temp mongo server. If you close the session by ctrl+c, he connecttion would be terminated</p><p>Or you can use the command</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">brew services start mongodb/brew/mongodb-community<br></code></pre></td></tr></table></figure><p>This create a back-support mongoDB server, you can still access to it after you close the terminal, to stop the back-support:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">brew services stop mongodb/brew/mongodb-community<br></code></pre></td></tr></table></figure></li><li><p>Start a connection</p><ol type="1"><li>After start the server, you can type <code>mongo</code> in bash to start a connection. Use <code>exit</code> to stop the connection</li><li>you can use mongodb compass to start a connection, this is a mongondb management UI</li><li>use Datagrip/Dataspell to build a connection</li></ol><p>Notes:</p><ul><li>The port for local host is always 27017</li><li>There's no default username and password</li></ul></li></ol><h2 id="data-types-in-mongodb">Data types in MongoDB</h2><p>Allowed data dytes in mongoDB include:</p><ul><li><p>BSON</p><p>​ A mongoDB data type, it's binary encoded json that can processed faster, it support some types(data, timestamp,object id) that are not supported by json</p></li><li><p>JSON</p></li><li><p>Integer</p></li><li><p>Boolean</p></li><li><p>Double</p></li><li><p>Arrays</p></li><li><p>Objects</p><p>​ Used to store embedded documents. If a documents A contains a K-V pair {"file":B}, where B is another document, the type of B in A's schema is Object</p></li><li><p>Null</p></li><li><p>Date</p></li><li><p>Timestemp</p></li><li><p>Object ID</p><p>​ ObjectIds are small, likely unique, fast to generate, and ordered. It's a usually used as a PK of a document. ObjectId values are 12 bytes in length, consisting of a 4-bytes timestamp value representiong the ObjectId's creation, a 5-bytes random value, a 3-byte incrementing value</p></li><li><p>Code</p><p>​ Like javascript code</p></li></ul><h2 id="create-and-drop-database">Create and Drop Database</h2><p>you can create databse by:</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">use</span> &lt;database-<span class="hljs-keyword">name</span>&gt;<br></code></pre></td></tr></table></figure><p>It would select the DB, if not exists, it create the DB</p><p>Notes: The created databse will not be visible untill you insert any data into it</p><p>To drop the database:</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">db.dropDatabase<span class="hljs-comment">()</span><br></code></pre></td></tr></table></figure><p>where <code>db</code> refer to the currently used database.</p><p>Before you drop the DB, makesure you select the DB first.</p><p>Some other commands related to create and drop</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-literal">show</span> databases -- list <span class="hljs-literal">all</span> visible DB<br>db -- <span class="hljs-literal">show</span> the <span class="hljs-literal">current</span> DB<br></code></pre></td></tr></table></figure><h2 id="create-and-drop-colletions">Create and drop colletions</h2><p>to create a collection in Database:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.create<span class="hljs-constructor">Collection(<span class="hljs-string">&quot;name&quot;</span>,<span class="hljs-params">options</span>)</span><br></code></pre></td></tr></table></figure><p>to drop a collection:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">db</span>.collection_name.<span class="hljs-keyword">drop</span>()<br></code></pre></td></tr></table></figure><p>to show collections in current DB</p><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dart"><span class="hljs-keyword">show</span> collections<br></code></pre></td></tr></table></figure><h2 id="insert-documents">Insert documents</h2><p>to create one documents:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>insert<span class="hljs-constructor">One(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;)</span>;<br></code></pre></td></tr></table></figure><p>to create many documents:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>insert<span class="hljs-constructor">Many([&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;B&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;C&quot;</span>&#125;])</span><br></code></pre></td></tr></table></figure><p>Note: the input of insertMany() should be a list[]</p><h2 id="update-documents">Update documents</h2><p>To update a documents</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.updateOne</span>(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;B&quot;</span>&#125;,&#123;<br>        <span class="hljs-variable">$set</span>:&#123;<br>            <span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;123456&quot;</span><br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><p>Where:</p><ul><li>the first parameter eplicts the documents to update</li><li>the second parameter explicts the operation to conduct</li></ul><p>Notes:This command only applies to the first document that meet the search condition(the first pararmeter)</p><p>to update many documents:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.updateMany</span>(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;123456&quot;</span>&#125;,&#123;<br>        <span class="hljs-variable">$set</span>:&#123;<br>            <span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span><br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><h2 id="read-data">Read data</h2><p>To read(and modify) data:</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment">-- find all documents in the collection</span><br>db.C1.find()<br><br><span class="hljs-comment">--find particular documents </span><br>db.C1.find(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>,<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span>&#125;) <br><br><span class="hljs-comment">-- find the first documents that fits the condition</span><br>db.C1.findOne(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and delete the first documenttaht fits the condition</span><br>db.C1.findOneAndDelete(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;D&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and replace the first documenttaht fits the condition</span><br><span class="hljs-comment">-- the second parameter gives a whole documents(like the format in insert)</span><br>db.C1.findOneAndReplace(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>,<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;23456&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and update</span><br><span class="hljs-comment">-- the second parameter gives a the operation to apply on the  documents(like the format in update)</span><br>db.C1.findOneAndUpdate(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>&#125;,&#123;<br>$<span class="hljs-keyword">set</span>:&#123;<br><span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span><br>&#125;<br>&#125;<br>   )<br>   <br><span class="hljs-comment">-- find and Modify</span><br><span class="hljs-comment">-- pass a json that tell the command to conduct multiple operaions</span><br>db.C1.findAndModify(<br>    &#123;<br>    <span class="hljs-string">&quot;query&quot;</span>: &#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>&#125;,<br>    <span class="hljs-string">&quot;update&quot;</span>:&#123;$<span class="hljs-keyword">set</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;&#125;,<br>    &#125;<br>)<br><br></code></pre></td></tr></table></figure><h2 id="delete-documents">Delete documents</h2><p>to delete documents from a collection:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">-- delete the first documetn fits the condition<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>delete<span class="hljs-constructor">One(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;)</span><br>-- delete all documents fit the condition<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>delete<span class="hljs-constructor">Many(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;)</span><br></code></pre></td></tr></table></figure><h2 id="query">Query</h2><p>The first parameter is called query condition, by passing a json map, you tell the command constraints of fields.</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>find(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>,<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;T&quot;</span>&#125;)<br></code></pre></td></tr></table></figure><p>you can also use the $and operand:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">db.C1.<span class="hljs-builtin-name">find</span>(<br>&#123;<br><span class="hljs-variable">$and</span>:[<br>&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;T&quot;</span>&#125;<br>]<br>&#125;<br>)<br></code></pre></td></tr></table></figure><p>Other operand in this formats:</p><ul><li>or</li><li>nor</li></ul><p>to query documents with quantify condition:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">db.C1.<span class="hljs-builtin-name">find</span>(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:&#123;<span class="hljs-variable">$gte</span>:<span class="hljs-string">&quot;20000&quot;</span>&#125;&#125;)<br></code></pre></td></tr></table></figure><p>Other operand in this formats:</p><ul><li>lte: less than ot equal</li><li>gt: greater than</li><li>lt: less than</li><li>eq: equal</li><li>neq: not equal</li></ul><h2 id="select-specific-fields">Select specific fields</h2><p>to select specific fields:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">var</span> pipline = [<br>    &#123;<span class="hljs-variable">$sort</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;&#125;<br>]<br><span class="hljs-keyword">db</span>.C1.aggregate(pipline)<br></code></pre></td></tr></table></figure><p>Notes:</p><ul><li>the first parameter explicts the query condition</li><li>the second parameter explicts the fields you want or do not want(Projection)</li><li><strong>You can not mix inclusion and exclusion</strong> in the second parameter like {"name":0,"mobile":1}</li><li>**The only field that can be mixed is "_id"** of documents. {"name":1,"_id":0}</li></ul><h2 id="projection">Projection</h2><p>Projection is a mechanism allowing you to select specific fieds, like slice of an array.</p><h2 id="aggregation">Aggregation</h2><p>To perform aggregation on collections:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">var</span> pipline = [<br>    &#123;<span class="hljs-variable">$sort</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;&#125;,<br>    &#123;<span class="hljs-variable">$limit</span>:4&#125;<br>]<br><span class="hljs-keyword">db</span>.C1.aggregate(pipline)<br></code></pre></td></tr></table></figure><p>where pip line is list of operations,</p><ul><li>$count</li><li>$group</li><li>$limit</li><li>$lookup</li><li>$match</li><li>$merge</li><li>$sort</li><li>project</li><li>unwind</li><li>unset</li></ul><h2 id="limit-and-skip">Limit and skip</h2><p>Limit the results returned by:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.limit</span>(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>showing the first there results</p><p>Skip the first 2 results and show the rest:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.skip</span>(<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="sort">Sort</h2><p>to sort results:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.sort</span>(&#123;<span class="hljs-string">&quot;name&quot;</span>:-<span class="hljs-number">1</span>&#125;)<br></code></pre></td></tr></table></figure><p>where the json map in sort() specify the sorting depending on which fields. 1 for ascending and -1 for descending. If you pass multiple fields, it sort the next fields in the groups of previous fields.</p><h2 id="create-and-drop-index">Create and Drop index</h2><p>Create indexes:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>create<span class="hljs-constructor">Index(&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;)</span><br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>create<span class="hljs-constructor">Indexes([&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;,&#123;<span class="hljs-string">&quot;mobile&quot;</span>:1&#125;])</span><br></code></pre></td></tr></table></figure><p>Drop index:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>drop<span class="hljs-constructor">Index(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:1&#125;)</span><br><br>-- drop all indexes except _id<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>drop<span class="hljs-constructor">Indexes()</span><br></code></pre></td></tr></table></figure><p>Group by:</p><p>Group by operand:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.aggregate</span>(<br>    &#123;<br>        <span class="hljs-variable">$group</span>:&#123;<br>            _id:<span class="hljs-string">&quot;$name&quot;</span>,<br>            <span class="hljs-string">&quot;count&quot;</span>:&#123;<span class="hljs-variable">$sum</span>:<span class="hljs-number">1</span>&#125;<br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><p>Where:</p><ul><li>_id is an essential arguments, it explain group by which field</li><li>put a $ before the field to group by, it's necessary</li><li>if you want to group by multiple levels, use _id:["$name","$mobile"]</li><li>the "count" is an alias defined by user</li><li>{$sum:1} = count(*)</li><li>{$sum:"$field"} = sum(field)</li><li>other operans includes: $avg, $min, $max</li><li>$push: push all values of the given field in the group into one array</li><li>$addToSet: same as $push, but return a unique set</li><li>$first, $last: return the first/last value</li><li>if _id:null, then return all documents in one group</li></ul><h2 id="back-up-restore">Back up &amp; Restore</h2><p>to back up all databases</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">-- dump the current database<br>mongodump<br></code></pre></td></tr></table></figure><p>To restore all databases</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mongorestore<br></code></pre></td></tr></table></figure><h2 id="transaction">Transaction</h2><p>For transation realted content in MongoDB, please refer to <a href="https://zhuanlan.zhihu.com/p/71679945">this link</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NoSQL</tag>
      
      <tag>MongoDB</tag>
      
      <tag>Guidebook</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
