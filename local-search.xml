<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Sampling Methods for Machine Learning</title>
    <link href="/2022/12/05/sampling/"/>
    <url>/2022/12/05/sampling/</url>
    
    <content type="html"><![CDATA[<h1 id="sampling-methods-for-machine-learning">Sampling Methods for Machine Learning</h1><h2 id="about-sampling">1. About Sampling</h2>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Sampling</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic of Probability Theory for Data Science</title>
    <link href="/2022/12/02/Basic-prob/"/>
    <url>/2022/12/02/Basic-prob/</url>
    
    <content type="html"><![CDATA[<h1 id="basic-of-probability-theory-for-data-science">Basic of Probability Theory for Data Science</h1><h2 id="basic-concepts-for-probability">1. Basic Concepts for Probability</h2><p><strong>Random Experiment</strong>:</p><ul><li>A trail that can have more than one possible outcome.</li><li>The trail should be replicable under fixed conditions</li><li>The outcome of the trail is unpredictable</li></ul><p><strong>Event </strong>: A specific outcome of a random experiment(e.g X=1)</p><p><strong>Fundamental Event</strong>: The minimum grain of event defined according to the objective of the random experiment(Not possible or necessary to split into smaller grain). For example, for a throw of dice, the fundamental events would be face = 1,2,...6.</p><p><strong>Compound Event</strong>: A event consists of multiple fundamental events(e. g, for a throw of dice: Face &lt; 5)</p><p><strong>Sample Space</strong>: A collection consists of all possible fundamental events. (e. g, for two flips of a coin <span class="math inline">\(\Omega = \{(face,face),(back,face),(face,back),(back,back)\}\)</span>)</p><p><strong>Random Variable</strong>: A function that map each sample point <span class="math inline">\(\omega\)</span> in the sample space <span class="math inline">\(\Omega\)</span> into a real number <span class="math inline">\(X = X(\omega)\)</span> . strictly, the definition of a event is a collection <span class="math inline">\(\{\omega | X(\omega = a)\}\)</span>, but in most real implementation, just understand event as an outcome.</p><h2 id="interpretation-of-probability">2. Interpretation of Probability</h2><p>Probability describe how likely a event would happen. In the probability theory, the following axiom are give: <span class="math display">\[0 \le P(A) \le 1\]</span></p><p><span class="math display">\[P(\Omega) = 1\]</span></p><p><span class="math display">\[P( A_1+A_2) = P(A_1)+P(A_ 2)\]</span></p><p>Where <span class="math inline">\(\Omega\)</span> is a definite event(containing all fundamental events), <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are exclusive</p><h3 id="classical-model-of-probability">2.1 Classical Model of Probability</h3><p>In Classical Interpretation of probability, two assumptions are considered satisfied:</p><ol type="1"><li>The sample space contains finite fundamental events</li><li>The happening of each fundamental event are equally likely</li></ol><p>Under such assumptions, the probability of an event A can be defined as <span class="math display">\[P(A) = \frac{n_a}{n_s}\]</span> where <span class="math inline">\(n_ s\)</span> is the number of fundamental events in the sample space, and <span class="math inline">\(n_ a\)</span> is the number of fundamental events in event A.</p><p>For most classical probability case, <span class="math inline">\(n_s\)</span> and <span class="math inline">\(n_ a\)</span> can be calculated from permutation and combination: <span class="math display">\[P_n^m = \frac{n! }{(n- m)!}\]</span></p><p><span class="math display">\[C_m^n = \frac{n! }{m!(n- m)!}\]</span></p><h3 id="geometric-model-of-probability">2.2 Geometric Model of Probability</h3><p>Define a geometric measure of a event(e. g length of line segment, area) <span class="math display">\[P(A) = \frac{M(A)}{M(S)}\]</span></p><h3 id="frequency-and-statistical-probability">2.3 Frequency and Statistical Probability</h3><p>Suppose n times of random experiment are conducted, and event A happened m times, the define the frequency of event A as: <span class="math display">\[\omega(A) = \frac{m}{n}\]</span> Then the Statistical Probability of event A is: <span class="math display">\[P(A) = \lim_{n\to\infty}w(A)\]</span> Note that probability is a inner properties of a random variable. Statistical Probability is a mathematic approximation of th real probability</p><h2 id="baisc-theroms-in-probability-theory">3. Baisc Theroms in Probability Theory</h2><p><span class="math display">\[P(A) = 1-P(\bar{A})\]</span></p><p><span class="math display">\[P(A-B) = P(A) - P(A\cap B)\]</span></p><p><span class="math display">\[P(A+B) = P(A) +P(B) - P(A\cap B)\]</span></p><h2 id="conditional-probability-joint-probability-and-independency">4. Conditional Probability, Joint Probability and Independency</h2><h3 id="conditional-probability">4.1 Conditional Probability</h3><p>Let A, B be two events in sample space <span class="math inline">\(\Omega\)</span>, the probability of the A given the condition that B has happened is called conditional probability, denoted as <span class="math inline">\(P(A| B)\)</span></p><p>The sample sapce of <span class="math inline">\(P(A| B)\)</span> is B, not <span class="math inline">\(\Omega\)</span>. Conditioning means "Compression" on the sample spcae. According to the axiom of probability theory, the probability of the whole sample space is 1. Thus, let A be the random variable X = a: <span class="math display">\[\sum_a P(A|B) = 1\]</span> A and B can be two events of a same variable, or each be a event for a separate variable.</p><h3 id="law-of-total-probability">4.2 Law of Total Probability</h3><p>Let <span class="math inline">\(B_1,B_ 2,...,B_n\)</span> be a series of collectively exhaustive events, A be another event: <span class="math display">\[P(A) = \sum_i^n P(B_i)P(A|B_i)\]</span></p><h3 id="joint-probability">4.3 Joint Probability</h3><p>Assume a two-dimension sample space is determined by two random experiment, which means we have two random variables X and Y for a sample space. Let A, B be a certain outcome of variable X and Y respectively, the probability taht events A and B both happen is called the joint probability of A and B, denoted as <span class="math inline">\(P(A, B)\)</span>. The joint probability has the following properties: <span class="math display">\[\sum_x\sum_y P(X=x,Y =y) = 1\]</span> If X and Y are independent: <span class="math display">\[P(X,Y) = P(X)P(Y)\]</span> For such a sample sapce: <span class="math display">\[P(A| B ) = \frac{P(A,B)}{P(B)}\]</span> associated with the Law of Total Probability: <span class="math display">\[P(A)= \sum_i^n P(B_i)P(A|B_i) = \sum_i^n P(A,B_i)\]</span></p><h3 id="bayesian-law">4.4 Bayesian Law</h3><p>Let <span class="math inline">\(A_1,A_ 2,...,A_n\)</span> be a series of collectively exhaustive events, B be another event: <span class="math display">\[P(A_i|B) = \frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B|A_i)}{\sum_i^nP(A_i)P(B|A_i)}\]</span> where we call:</p><ul><li><span class="math inline">\(A_i\)</span>: hypothesis event, an event we want to attest its probability distribution through observations on evidence</li><li>B: evidence, an event used to update knowledge on the hypothesis event</li><li><span class="math inline">\(P(A)\)</span>: prior probability, representing the knowledge before the evidence emerge</li><li><span class="math inline">\(P(B|A)\)</span>: likehood, representing the probability of B under events A</li><li><span class="math inline">\(P( A_i|B)\)</span>: posterior probability, representing the updated knowledge after evidence emerge</li></ul><p>Specific examples of bayesian inference can be referred via this <a href="http://zhengyuanyang.com/2022/09/29/bayes-inference/#%E5%B8%B8%E8%A7%81%E9%A2%98%E5%9E%8B%E5%92%8C%E4%BE%8B%E9%A2%98%E8%A7%A3%E6%9E%90">article</a></p><h3 id="independency-of-events">4.5 Independency of Events</h3><p>If the probability of A is not affected by whether event B happen, then A is independent to B. In conditional probability form: <span class="math display">\[P(A) = P(A|B)\]</span></p><p><span class="math display">\[P(A,B) = P(A)P(B)\]</span></p><p>Note that <span class="math inline">\(A \perp \!\!\! \perp B\)</span> and <span class="math inline">\(A\cap B\)</span> cannot be both true</p><h2 id="probability-distribution-probability-density-function">5. Probability Distribution &amp; Probability Density Function</h2><h3 id="discrete-random-variable-and-probability-distribution">5.1 Discrete Random Variable and Probability Distribution</h3><p>If the possible value of a random variable is countable, then it is a discrete random variable. The probability distribution of a discrete random variable X is defined as a <strong>function</strong>: <span class="math display">\[P(X) = P(X=x_k)\]</span> The PDF has the following properties: <span class="math display">\[P(X) \ge 0\]</span></p><p><span class="math display">\[\sum_k P(X) = 1\]</span></p><h3 id="continuous-random-variable-and-probability-density">5.2 Continuous Random Variable and Probability Density</h3><p>If a randome variable can be any value on a range <span class="math inline">\([a , b]\)</span> and there exists a integratable function <span class="math inline">\(f( x)\)</span>, so that: <span class="math display">\[P( a&lt; X \le b  ) = \int_a^ b f(x)dx\]</span> Then X is called a continuous random variable, <span class="math inline">\(f(x )\)</span> is called the probability density function of X.</p><p>For a continuous random variable, the probability of each single sample point would be 0. Instead of an actual probability, the distribution of a continuous random variable is described by the probability density of each data point. The value of <span class="math inline">\(f(x)\)</span> on a specific point represents the probability density of that sample point: <span class="math display">\[f(x= a) = \lim_{\Delta \to \infty } P(a&lt;X \le a+ \Delta  )\]</span></p><h3 id="distribution-type">5.3 Distribution Type</h3><p>For details about distribution type, refer to <a href="http://zhengyuanyang.com/2022/11/04/distribution/">here</a></p><h2 id="expectation-and-variance">6. Expectation and Variance</h2><h3 id="expectation">6.1 Expectation</h3><p>For discrete variable: <span class="math display">\[E[X] = \sum_i^n x_iP(X=x_i)\]</span></p><p>For discrete variable: <span class="math display">\[E[X] = \int_{-\infty}^\infty xf(x)dx\]</span> if E[X] can converge</p><p>Properties of Expectation: <span class="math display">\[E[X+C] = E[X]+C\]</span></p><p><span class="math display">\[E[CX] = CE[X]\]</span></p><p><span class="math display">\[E[X + Y] = E[X]+E[Y] \qquad \forall X, Y\]</span></p><p><span class="math display">\[E[XY] = E[X]E[Y] \qquad if \ X \perp \! \! \perp Y\]</span></p><p><span class="math display">\[E[g(X)] = \sum_i^n g(x_i)P(X=x_i) \qquad or \qquad \int_{-\infty}^\infty g(x)f(x)dx\]</span></p><p><span class="math display">\[E[X|Y=y] = \sum_i^nx_iP(X=x_i|Y=y) \qquad or \qquad \int_{-\infty}^\infty xf(x|y)dx\]</span></p><h3 id="variance">6.2 Variance</h3><p><span class="math display">\[V[X] = E[(X-E[X])^2] = E[ X^ 2]-(E[X])^2\]</span></p><p>Properties of Variance: <span class="math display">\[V[C] = 0\]</span></p><p><span class="math display">\[V[X+C] = V[X]\]</span></p><p><span class="math display">\[V[CX] = C^2V[X]\]</span></p><p><span class="math display">\[V[X\pm Y] = V[X]+V[Y] \pm 2Cov[X, Y]\]</span></p><p><span class="math display">\[V[g(X)] = g&#39;(E[X])^2V[X]\]</span></p><h3 id="covariance">6.3 Covariance</h3><p><span class="math display">\[Cov[X, Y] = E[(X-E[X])(Y- E[Y])] = E[XY]-E[X]E[Y]\]</span></p><p>Covariance is a measure of the joint variability of two random variables, it represent thedegree that two variables variate samely in directions. If two variable are independent: <span class="math display">\[Cov[X,Y] = 0 \qquad if \ X \perp \! \! \perp Y\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Probability</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Structural Causal Model</title>
    <link href="/2022/11/10/structual-causal-model/"/>
    <url>/2022/11/10/structual-causal-model/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-of-structual-causal-model">Introduction of Structual Causal Model</h1>]]></content>
    
    
    <categories>
      
      <category>Causal Inference</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Structural Causal Model</tag>
      
      <tag>Introduction</tag>
      
      <tag>Causal Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Potential Outcome Framework</title>
    <link href="/2022/11/07/potential-outcome/"/>
    <url>/2022/11/07/potential-outcome/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-of-potential-outcome-framework">Introduction of Potential Outcome Framework</h1><h2 id="potential-outcomes-framework">1. Potential Outcomes Framework</h2><p>The Potential Outcomes framework is a series of notation widely used in causal inference.</p><h3 id="conditioning-v.s.-intervening">1.1 Conditioning v.s. Intervening</h3><p>In statistics, conditioning and intervening is two different concepts.</p><p><strong>Conditioning</strong>: Segmenting a dataset into subset bu applying conditions on variables</p><p><strong>Intervening:</strong> Intervene a process with certain treatment, so that every unit is affected by treatment variable</p><p><img src="/2022/11/07/potential-outcome/1.PNG"></p><p>we use the <strong>do</strong> operands the represent that we intervene a process. Let Y donate the outcome statistics of the process, the probability of Y = y under a intervention such that T=t is noted as <span class="math inline">\(P(Y=y|do(T=t))\)</span> or <span class="math inline">\(P(y|do(t))\)</span></p><p>When there exists confounder in the process, the conditional probability does not equal the interventional probability: <span class="math display">\[P (Y|do(t),X) = E_X[P(Y|T=t,X)] \ne  P(Y|T=t,X)\]</span></p><h3 id="potential-outcomes">1.2 Potential Outcomes</h3><p>Suppose we want to evaluate the cause effect of a pill on a single person.</p><p><img src="/2022/11/07/potential-outcome/2.PNG"></p><p>Let T denotes whether this person take a pill, Y denotes the degree that his headache is relieved after taking the pill. The Cause Effect can be noted as: <span class="math display">\[Y|do(T=1) \ - Y|do( T =0) = Y(1)-Y(0)\]</span> The cause effect of a treatment on a single unit is called <strong>Individual Treatment Effect</strong>(ITE)</p><p>A problem of evaluating the effect this way is that for a single person we can only observe Y for once. If we intervene the treatment so that T = 1, we won't obtain any observations about Y(0).</p><p>In such case, we call Y(1),Y(0) the potential outcomes under T. A potential outcome is the outcome of the statistics under our potential treatment.</p><p>For a specific unit, suppose we applied treatment T=t on it, we call the observed outcome <span class="math inline">\(Y^F= Y(T=t)\)</span> the factual outcome. The potential outcome that not applying T=t, <span class="math inline">\(Y^{CF} = Y(T\ne t)\)</span> is called the counterfactual outcome. <span class="math inline">\(Y^{CF }(T =1) = Y^F(T=0)\)</span></p><h3 id="average-treatment-effect">1.3 Average Treatment Effect</h3><p>Due to the problem discussed in 1.2, we usually cannot get the ITE of a treatment directly. However, if we do not put specific emphasis on the effect of individual, we can obtain the overall effect size by calculating the <strong>Average Treatment Effect(</strong>ATE) of a treatment on a population: <span class="math display">\[ATE = \Epsilon[ITE] =E[Y^F(T=1)] - E[Y^{CF}(T=1)] = E[Y(1)] - E[Y(0)]\]</span> Note that when there are any confounders X so that <span class="math inline">\((Y(1),Y(0)) \not \! \perp \!\!\! \perp T\)</span>: <span class="math display">\[E[Y(1)] = E_X[E[Y|T=1,X=x]] \ne E[Y|T=1]\]</span> In such case, the ATE is now: <span class="math display">\[ATE =E_X[E[Y|T=1,X=x]-E[Y|T=0,X=x]]=E_X[E[Y|T=1,X=x]] - E_X[E[Y|T=0,X=x]]\]</span> In most real application cases, there are multiple confounders in the causal inference process. To estimate the ATE, we need to "block" the effects of confounder. We call the treatment effect under a group condidsa</p><h3 id="basic-assumptions-about-potential-outcomes">1.4 Basic Assumptions about Potential Outcomes</h3><p><strong>Stable Unit Treatment Value Assumption(SUTVA)</strong>: The potential outcome of any unit won't be changed by treatment on other units. This assumption emphasize the independency of units</p><p><strong>Positivity Assumption</strong>: For units of any possible values of background variable, any assigned treatment is possible. <span class="math display">\[P(T=t|X=x) \in (0,1) \quad \forall t\ and \ x\]</span> <strong>Consistency Assumption </strong>: If T=t is applied on a group of unit, then the outcomes of all units in that group would be Y(T=t). <span class="math display">\[T = t \Rightarrow Y=Y(t)\]</span> The consistency assumption make sure we have $Y(t) = Y|T=t $ when there's no confounders</p><p><strong>Ignorability Assumption</strong>: The determination of the treatment T is independent to the potential outcomes: <span class="math display">\[(Y(1),Y(0)) \perp \!\!\! \perp T\]</span> Under the Ignorability Assumption: <span class="math display">\[E[Y(1)] = E[Y(1)|T=1] = E[Y| T=1]\]</span> The Ignorability Assumption is usually considered satisfied when we conduct a random control trail. If we randomized the determination of the treatment, this assumption implies the influence of the confounders are blocked. However, in a non-experimental context, since T is not randomized, the influence of the confounder still exists, and the Ignorability Assumption is not satisfied. In such scenarios, we would weaken Ignorability Assumption into the following assumption.</p><p><strong>Conditional Ignorability Assumption</strong>: Given background variables X, the determination of the treatment T is independent to the potential outcomes <span class="math display">\[(Y(1),Y(0))|X \perp \!\!\! \perp T\]</span> Under the conditional Conditional Ignorability Assumption: <span class="math display">\[E[Y(1)|X=x_i] = E[Y(1)|T=1,X=x_i] = E[Y| T=1,X=x_i]\]</span> With these assumption, we can make the following statement for a causal inference process:</p><ul><li><p>The SUTVA, Positivity and Consistency should always be satisfied</p></li><li><p>If a RCT is conducted, we can regard as <span class="math inline">\((Y(1),Y(0)) \perp \!\!\! \perp T\)</span>, and The ignorability assumption can be considered satisfied. Thus a RCT is a best solution for causal inference</p></li><li><p>If RCT is not implementable, we can condition on the confounder to satisfy the conditional ignorability assumption and estimate the CATE, then we may estimate the distribution of the confounders and calculate the exception of the CATE to obtain ATE</p><p><img src="/2022/11/07/potential-outcome/3.PNG"></p></li></ul><h3 id="randomized-control-trail">1.5 Randomized Control Trail</h3><p>A Randomized Control Trail is a trail that whether to apply a treatment on a unit is decided only by a coin flip(A random event). Since the determination of each unit is randomized, we can state that Ignorability Assumption is satisfied if there are no omitted confounders, and <span class="math inline">\((Y(1),Y(0)) \perp \!\!\! \perp T\)</span></p><p>Under such assumption: <span class="math display">\[E[Y(1)] = E[Y(1)|T=1] = E[Y^F |T=1]\]</span></p><p><span class="math display">\[E[Y^{CF}(T= 1)] = E[ Y^F (T= 0)] = E[Y|T=0]\]</span></p><p><span class="math display">\[ATE = E[Y(1)] - E[Y(0)] = E[Y|T=1] - E[Y|T=0]\]</span></p><p>Thus, we can estimate the ATE through a RCT. An RCT make the determination of the treatment randomized and blocked the influence of the confounders</p>]]></content>
    
    
    <categories>
      
      <category>Causal Inference</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distribution</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Introduction of Causal Inference</title>
    <link href="/2022/11/07/basic-causal-inference/"/>
    <url>/2022/11/07/basic-causal-inference/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction-of-causal-inference">Introduction of Causal Inference</h1><h2 id="section">1.</h2><h2 id="why-causal-inference">2. Why Causal Inference</h2>]]></content>
    
    
    <categories>
      
      <category>Causal Inference</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distribution</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Common Distribution Types in Machine Learning</title>
    <link href="/2022/11/04/distribution/"/>
    <url>/2022/11/04/distribution/</url>
    
    <content type="html"><![CDATA[<h1 id="common-distribution-types">Common Distribution Types</h1><h2 id="random-variable-and-probability-distribution">1. Random Variable and Probability Distribution</h2><p><strong>Random Variable</strong></p><p>A random variable is a quantity whose values depends on the outcome of a random event. Random variable is terminology for numerical data. The probability that the value of a random variable X equals a certain outcome can be given as <span class="math inline">\(P(X = x_1)\)</span></p><p><strong>Distribution</strong></p><p>The probability distribution function is a function <span class="math inline">\(P = f(x)\)</span>, where:</p><ul><li>x donates the possible values of random variable</li><li>y donates the probability when X = x</li></ul><p>Note that the integrate of PDF is 1.</p><p><strong>Parameter</strong></p><p>A parameter is used to determine the shape of a type of PDF. Different type of PDF has different kinds of parameters. <span class="math display">\[P = f_X(x,\theta)\]</span></p><h2 id="discrete-distribution">2. Discrete Distribution</h2><h3 id="bernoulli-distribution">2.1 Bernoulli Distribution</h3><p>A trial is performed with probability p of success, and X is a random variable indicates success or not. In such case, X has 2 possible values(1 means success), and it follows a bernoulli distribution. <span class="math inline">\(p\)</span> is the only parameter for a Bernoulli Distribution.</p><p>The PDF of a bernoulli distribution: <span class="math display">\[f(x) = p^ x (1-p)^{1-x}\]</span> The expectation of a bernoulli distribution is: <span class="math display">\[E[X] = \sum_i x_i f( x) = 0+p =p\]</span></p><p>The variance of a bernoulli distribution is: <span class="math display">\[Var[X] = \sum_i(x_i-E[x]^2)f(x) = p(1-p)\]</span></p><h3 id="binomial-distribution">2.2 Binomial Distribution</h3><p>Let randome variable X donates the number of success we achieved in n independent bernoulli trials. Each trail has a same probability p of success. The parameters of a Binomial Distribution includes n and p</p><p>The PDF of a binomial distribution: <span class="math display">\[f_X(X = k,n,p) = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\]</span> where k = 0,1,2...n</p><p><img src="/2022/11/04/distribution/1.png"></p><p>The expectation of a binomial distribution is: <span class="math display">\[E[X] = np\]</span></p><p>The variance of a binomial distribution is: <span class="math display">\[Var[X] = np(1-p)\]</span> Some properties of binomial distribution:</p><ul><li>if <span class="math inline">\(X \sim bin( n, p), Y \sim bin(m,p)\)</span>, then <span class="math inline">\(X+Y \sim bin(n+m,p)\)</span></li><li>If p is small, <span class="math inline">\(Bin(n,p)\)</span> is approximately <span class="math inline">\(Pois(\lambda)\)</span></li><li>if n is large and p is not near 0 and 1, <span class="math inline">\(Bin(n,p)\)</span> is approximately <span class="math inline">\(N(np, np(1-p)\)</span></li></ul><h3 id="geometric-distribution">2.3 Geometric Distribution</h3><p>A Geometric Distribution basically have same story like a binomial distribution, except now the X donates times of trails before first success. The only parameter of a Geometric Distribution is p.</p><p>The PDF of a Geometric Distribution: <span class="math display">\[f(X=k) = (1-p)^{k-1}p\]</span> <img src="/2022/11/04/distribution/6.PNG"></p><p>The expectation of a binomial distribution is: <span class="math display">\[E[X] = \frac{1}{p}\]</span></p><p>The variance of a binomial distribution is: <span class="math display">\[Var[X] = \frac{1-p}{p^2}\]</span> Property of binomial distribution:</p><ul><li>Memoryless: if <span class="math inline">\(x \sim Geom(p)\)</span>, then <span class="math inline">\(P(T&gt;t+s|T&gt;t) = P(T&gt;s)\)</span></li></ul><h3 id="section"></h3><h3 id="possion-distribution">2.4 Possion Distribution</h3><p>Suppose an event(X = x) happens with a certain probability(usually low probability), let X be the times that event happens in a unit time, then X follows a Possion Distribution. <span class="math inline">\(\lambda\)</span> is the only parameter for a Possion Distrbution, which is the expectation of X.</p><p>The PDF of a binomial distribution: <span class="math display">\[f_X(X = k,\lambda) = \frac{e^ {-\lambda}\lambda^k}{k!}\]</span> where k = 0,1,2...<span class="math inline">\(\infty\)</span></p><p><img src="/2022/11/04/distribution/2.png"></p><p>The expectation of a poisson function is: <span class="math display">\[E[X] = \lambda\]</span></p><p>The variance of a poisson function is: <span class="math display">\[Var[X] = \lambda\]</span> Some properties of poisson distribution:</p><ul><li>if <span class="math inline">\(X \sim pois(\lambda_1), Y \sim pois(\lambda_2)\)</span>, then <span class="math inline">\(X+Y \sim pois(\lambda_1+\lambda_2)\)</span></li><li><span class="math inline">\(X|(X+Y=n) \sim Bin(n,\frac{\lambda_1}{\lambda_1+\lambda_2})\)</span></li><li>if n is large and p is not near 0 and 1, <span class="math inline">\(Bin(n,p)\)</span> is approximately <span class="math inline">\(N(np, np(1-p)\)</span></li></ul><h2 id="continuous-distribution">3. Continuous Distribution</h2><h3 id="normal-distribution">3.1 Normal Distribution</h3><p>If how the certain value of a continuous random variable X is unknown, we can assume it follows a normal distribution. The parameters of a normal distribution include <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></p><p>The PDF of a normal distribution: <span class="math display">\[f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span> The expectation of a normal distribution is: <span class="math display">\[E[X] = \mu\]</span></p><p>The variance of a normal distribution is: <span class="math display">\[Var[X] = \sigma^2\]</span> Some properties of normal distribution:</p><ul><li>if <span class="math inline">\(X \sim N(\mu_1,\sigma_1^2), Y \sim N(\mu_2,\sigma_2^2)\)</span>, then <span class="math inline">\(X+Y \sim N(\mu_1+\mu_2,\sigma_1^2 + \sigma_2^2)\)</span></li><li>if <span class="math inline">\(X \sim N(\mu_1,\sigma_1^2)\)</span>, then $aX+b N(a+ b, (a)^2) $</li><li>if <span class="math inline">\(X_1,X_2,..X_n\)</span> all follows normal distribution, than <span class="math inline">\(X_1^2+X_2^2+...X_n^2\)</span> follows <span class="math inline">\(\chi^2(n)\)</span></li></ul><h3 id="exponential-distribution">3.2 Exponential Distribution</h3><p>The Exponential Distribution describe same story as poisson distribution, exception the random variable X now donates the time interval between two occurrence of the events. The only parameter for exponential distribution is <span class="math inline">\(\beta\)</span>, where <span class="math inline">\(\beta = \frac{1}{\lambda}\)</span>, representing the probability of occurrence of the event in a unit time.</p><p>The PDF of a exponential distribution: <span class="math display">\[f(x) = f(x) = \left\{             \begin{array}{lr}             \lambda e^{-\lambda x} &amp; x \ge 0 \\             0 &amp; x&lt;0\\             \end{array}\right.\]</span></p><p><span class="math display">\[\lambda = \frac{1}{\beta}\]</span></p><p><img src="/2022/11/04/distribution/3.png"></p><p>The expectation of a normal distribution is: <span class="math display">\[E[X] = \beta\]</span></p><p>The variance of a normal distribution is: <span class="math display">\[Var[X] = \beta^2\]</span> Some properties of normal distribution:</p><ul><li>Memoryless: if <span class="math inline">\(x \sim Expo(\beta)\)</span>, then <span class="math inline">\(P(T&gt;t+s|T&gt;t) = P(T&gt;s)\)</span></li><li>if <span class="math inline">\(X \sim Expo(\lambda)\)</span>, then <span class="math inline">\(\lambda X \sim Expo(1)\)</span></li><li>if we have independent <span class="math inline">\(X_i \sim Expo( \lambda_ i)\)</span>, then <span class="math inline">\(min(X_1,X_2,..X_k) \sim Expo(\lambda_1+\lambda_2+...+\lambda_k)\)</span></li><li>if we have independent <span class="math inline">\(X_i \sim Expo( \lambda)\)</span>, then <span class="math inline">\(max(X_1,X_2,..X_k) \sim Expo(\lambda)+Expo(2\lambda)+...+Expo(k\lambda)\)</span></li></ul><h3 id="section-1"></h3><h3 id="gamma-distribution">3.3 Gamma Distribution</h3><p>Suppose in the exponential distribution scenario, you want to observe the event for <span class="math inline">\(\alpha\)</span> times before you stop, then the time you need to do that would follows a Gamma distribution <span class="math display">\[f(x) = \frac{x^{\alpha-1}\lambda^\alpha e^{-\lambda x}}{\Gamma(\alpha) }\]</span></p><p><span class="math display">\[\Gamma(\alpha) = (\alpha-1)! \qquad \alpha \ is \ Z\]</span></p><p><span class="math display">\[\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1) \qquad \alpha \ is \ R\]</span></p><p><span class="math display">\[\Gamma(\frac{1}{2}) = \sqrt{\pi}\]</span></p><p>The expectation of a gamma distribution is: <span class="math display">\[E[X] = \alpha \beta\]</span></p><p>The variance of a gamma distribution is: <span class="math display">\[Var[X] = \alpha \beta^2\]</span> Some properties of gamma distribution:</p><ul><li><p>if <span class="math inline">\(X \sim \Gamma(\alpha_1,\beta), Y \sim \Gamma(\alpha_2,\beta)\)</span>, then <span class="math inline">\(X+Y \sim \Gamma(\alpha_1+\alpha_2, \beta)\)</span></p></li><li><p>When <span class="math inline">\(\alpha = 1\)</span>, a gamma distribution is equivalent to a exponential distribution</p></li></ul><h3 id="beta-distribution">3.4 Beta Distribution</h3><p>The beta distribution is the conjugate prior distribution of a binomial distribution in a Bayesian Inference Context. Thus, it can be interpreted as the PDF of the parameter of a binomial distribution, which is the probability to success in the trail. In other world, it is a likelihood function.</p><p>The PDF of a exponential distribution: <span class="math display">\[f(x) = \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\]</span> where:</p><ul><li><p><span class="math inline">\(\alpha\)</span> is the number of observations of success</p></li><li><p><span class="math inline">\(\beta\)</span> is the number of observations of failure</p></li><li><p><span class="math inline">\(B(\alpha, \beta)\)</span> is a standard B function. It is applied to make the integrate of the PDF one</p><p><img src="/2022/11/04/distribution/4.png"></p></li></ul><p>The expectation of a beta distribution is: <span class="math display">\[E[X] = \frac{\alpha}{\alpha + \beta}\]</span></p><p>The variance of a gamma distribution is: <span class="math display">\[Var[X] = \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\]</span> Some properties of beta distribution:</p><ul><li><p>if <span class="math inline">\(X \sim \Gamma(a,\lambda), Y \sim \Gamma(b,\lambda)\)</span>, X and Y are independent, then <span class="math inline">\(\frac{X}{X+Y} \sim B(a,b)\)</span></p></li><li><p>After n trial, if x success is observed, the distribution of p pf a trail is updated to <span class="math inline">\(B(\alpha+x,\beta+n-x)\)</span></p></li></ul><h3 id="chi2-distribution">3.5 <span class="math inline">\(\chi^2\)</span> Distribution</h3><p>Suppose k independent random variable <span class="math inline">\(Z_1,..Z_ k\)</span> all follow standard normal distribution, then define random variable X as <span class="math inline">\(\sum_iZ_i^2\)</span>, then X follows a <span class="math inline">\(\chi^2\)</span> distribution. k is the only parameter of <span class="math inline">\(\chi^2\)</span> distribution, representing the number of independent variable(degree of freedom)</p><p>The PDF of a <span class="math inline">\(\chi^2\)</span> distribution: <span class="math display">\[f(x) = \frac{1}{2^{\frac{k}{2}}\Gamma(\frac{k}{2})}x^{\frac{k}{2}-1}e^{\frac{-x}{2}}\]</span> <img src="/2022/11/04/distribution/5.png"></p><p>The expectation of a <span class="math inline">\(\chi^2\)</span> distribution is: <span class="math display">\[E[X] = k\]</span></p><p>The variance of a <span class="math inline">\(\chi^2\)</span> distribution is: <span class="math display">\[Var[X] = 2k\]</span> Some properties of <span class="math inline">\(\chi ^2\)</span> distribution:</p><ul><li><p>If <span class="math inline">\(\chi^2(k_1)\)</span> and <span class="math inline">\(\chi^2(k_2)\)</span> are independent, then <span class="math inline">\(\chi^2(k_1) + \chi^2(k_2) \sim \chi^2(k_1+k_2)\)</span></p></li><li><p>When k is large, a <span class="math inline">\(\chi^2\)</span> distribution is similar to a normal distribution</p></li></ul><h2 id="multivariate-distribution">4. Multivariate Distribution</h2><p>A Multivariate Distribution is the joint PDF of a vector of variables</p><h3 id="multinomial-distribution">4.1 Multinomial Distribution</h3><p>Suppose there are n items fall into k buckets, where the probabilities each items fall into the k buckets is <span class="math inline">\(\vec{p} = (p_1,p_2,...p_k)\)</span>, let <span class="math inline">\(X_i\)</span> donates the number of items fall into the <span class="math inline">\(i^th\)</span> bucket, then a vector <span class="math inline">\(\vec{X} = (X_1,X_2,...X_k)\)</span> follows a Multinomial Distribution MulNm(n,<span class="math inline">\(\vec{p}\)</span> )</p><p>The Joint PDF of a Multinomial Distribution is: <span class="math display">\[P(\vec{X} = \vec{n})=\frac{n!}{n_1!n_2!...n_k!}p_1^{n_1}p_2^{n_2}...p_ k^ {n_k}\]</span> where <span class="math inline">\(\vec{n}\)</span> is a specifc combination of numbers of items fall into each buckets:<span class="math inline">\(\vec{n} = (n_1,n_2...n_k)\)</span>, and $n_ 1+n _2+...+n_k = n $</p><p>The Expectation Vector <span class="math inline">\(\vec{ E }\)</span> would be <span class="math inline">\((np_1,np_2,...np_k)\)</span></p><p>The Variance vector <span class="math inline">\(\vec{ V }\)</span> would be <span class="math inline">\((np_1(1-p_ 1),np_2(1-p_2),...np_k(1-p_k))\)</span></p><p>The Covariance for <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> would be <span class="math inline">\(Cov(X_i,X_j) = -np_ip_j\)</span></p><h3 id="multivariate-normal-distribution">4.2 Multivariate Normal Distribution</h3><p>For a vector <span class="math inline">\(\vec{X} = (X_1,X_2,...X_k)\)</span> , if every linear combination of this vector is normally distributed, then <span class="math inline">\(\vec{ X }\)</span> follows a Multivariate Normal Distribution MulNorm(<span class="math inline">\(\vec{\mu},\vec{\sigma^2}\)</span>).</p><p>The Joint PDF of a MVN Distribution is: <span class="math display">\[f(\vec{X}) = \frac{1}{\sqrt{(2\pi)^k|\Sigma|}} e^{-\frac{1}{2}(X-\mu)^T\Sigma(X-\mu)}\]</span> Where <span class="math inline">\(|\Sigma|\)</span> is the determinant of the Covariance Matrix</p><p>The Covariance for <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_ j\)</span> is calculated by: <span class="math display">\[Cov(X_i,X_j) = E[(X_i-\mu_{X_i})(X_ j- \mu_{X_j})] = E[X_iX_j]-E[X_i]E[X_j]\]</span></p><h2 id="distribution-transformation">5. Distribution Transformation</h2><p>Suppose the PDF of a random variable x is <span class="math inline">\(f_X(x)\)</span>, define <span class="math inline">\(y = t(x)\)</span>, then: <span class="math display">\[x = t^-{1}(y)\]</span> let the CDF of X and Y be <span class="math inline">\(F_X,F_Y\)</span> <span class="math display">\[P(Y&lt;y) = P(X&lt;x)\]</span></p><p><span class="math display">\[F_Y(y) = F_X(t^{-1}(y))\]</span></p><p><span class="math display">\[f_Y(y) = F&#39;_Y(y) = (t^{-1}(y))&#39;f_X(t^{-1}(y))\]</span></p><p>With Such rules：</p><table style="width:100%;"><thead><tr class="header"><th>From</th><th>Transformation</th><th>to</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(Expo(\lambda)\)</span></td><td><span class="math inline">\(Y = 1-e^{\lambda x}\)</span></td><td>U(0,1)</td></tr><tr class="even"><td>U(0,1)</td><td><span class="math inline">\(Y=\sqrt{2}h^{-1}(2X-1)\)</span>, <span class="math inline">\(h(x) = \frac{2}{\sqrt{\pi}}\int_0^xe^{-t^ 2}dt\)</span></td><td>N(0,1)</td></tr></tbody></table><p>In some case, we use a transformation to let <span class="math inline">\(f_Y(y)\)</span> have an approximate formulation as the target distribution. Then we define an error function to measure the loss of y and E[y] under target distribution, and train the parameters of the tranformation</p><table><thead><tr class="header"><th>From</th><th>Transformation</th><th>to</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(Expo(\lambda)\)</span></td><td><span class="math inline">\(Y =\frac{X^\theta-1}{\theta}\)</span></td><td><span class="math inline">\(N(\mu,\sigma^2)\)</span></td></tr><tr class="even"><td><span class="math inline">\(Expo(\lambda)\)</span></td><td><span class="math inline">\(Y=log_\theta（1+X）\)</span></td><td><span class="math inline">\(N(\mu,\sigma^2)\)</span></td></tr></tbody></table><p>There are many improved transformation algorithm like Box-cox, Yeo-johnson and Box-Muller. For more transformation methods for data distribution in machine learning, refer to <a href>ongoing</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distribution</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Encoding Methods for Categorical Variable</title>
    <link href="/2022/10/22/encoding/"/>
    <url>/2022/10/22/encoding/</url>
    
    <content type="html"><![CDATA[<h1 id="encoding-methods-for-categorical-variable">Encoding Methods for Categorical Variable</h1><h2 id="about-encoding">1. About Encoding</h2><p>Encoding is a process that map categorical variable to numerical variable. For nost models, untransformed categorical data is not accepted as inputs. Five types of encoder introduced below.</p><h2 id="label-encoder">2. Label Encoder</h2><p>Label Encoder is one of the simplest encoding methods. It gives a label to each category</p><p>A categorical variable before encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Dog</td></tr><tr class="odd"><td>Bird</td></tr></tbody></table><p>after encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>0</td></tr><tr class="even"><td>1</td></tr><tr class="odd"><td>2</td></tr></tbody></table><p>A Label Encoder has following properties:</p><ul><li>It changes a categorical variable into a multivalued discrete variable</li><li>It does not generate extra variable, thus is memory-saving</li><li>The encoded data has a magnitude relationship, thus Label encoder should be applied to ordinal variable instead of nominal variable</li><li>It does not change the number of categories</li></ul><h2 id="one-hot-encoder">3. One-hot Encoder</h2><p>One-hot encoder expand the categorical variable into c variables, where c is number of categories. For categories, these variables are exclusive.</p><p>A categorical variable before encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Dog</td></tr><tr class="odd"><td>Bird</td></tr></tbody></table><p>after encoding:</p><table><thead><tr class="header"><th>Cat</th><th>Dog</th><th>Bird</th></tr></thead><tbody><tr class="odd"><td>1</td><td>0</td><td>0</td></tr><tr class="even"><td>0</td><td>1</td><td>0</td></tr><tr class="odd"><td>0</td><td>0</td><td>1</td></tr></tbody></table><p>For some kinds of data, the generated variables could also be non-exclusive</p><table><thead><tr class="header"><th>Color</th><th>R</th><th>G</th><th>B</th></tr></thead><tbody><tr class="odd"><td>White</td><td>0</td><td>0</td><td>0</td></tr><tr class="even"><td>Purple</td><td>255</td><td>0</td><td>100</td></tr><tr class="odd"><td>Orange</td><td>100</td><td>120</td><td>10</td></tr></tbody></table><p>A One-hot Encoder has following properties:</p><ul><li>It changes a categorical variable into c multivalued/binary discrete variables</li><li>It generates c-1 extra variable, thus is memory-costing when c is big. It suits variable with few categories</li><li>It can be applied to both ordinal and nominal variable</li><li>It does not change the number of categories</li><li>Dummy-variables and True-False encoder are very similar to one-hot encoder, only small differences exist</li></ul><h2 id="target-encoder">4. Target-Encoder</h2><p>Target encoder transform the categorical variables according to the output variable.</p><p>For numerical output, the target encoder replace categorical variable with the mean of samples under each category</p><table><thead><tr class="header"><th>X(species)</th><th>Y(Weight)</th><th>X'</th></tr></thead><tbody><tr class="odd"><td>cat</td><td>10</td><td>12.5</td></tr><tr class="even"><td>cat</td><td>15</td><td>12.5</td></tr><tr class="odd"><td>dog</td><td>20</td><td>25</td></tr><tr class="even"><td>dog</td><td>30</td><td>25</td></tr></tbody></table><p>For categorical output, the target encoder replace categorical variable with <span class="math inline">\(P(y= y_i|x=x_i)\)</span></p><table><thead><tr class="header"><th>X(species)</th><th>Y(Size)</th><th>X1(size-Small)</th><th>X2(size-medium)</th><th>X3(size-BIG)</th></tr></thead><tbody><tr class="odd"><td>cat</td><td>big</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="even"><td>cat</td><td>big</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="odd"><td>cat</td><td>medium</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="even"><td>cat</td><td>small</td><td>0.25</td><td>0.25</td><td>0.5</td></tr><tr class="odd"><td>dog</td><td>big</td><td>0.33</td><td>0</td><td>0.66</td></tr><tr class="even"><td>dog</td><td>big</td><td>0.33</td><td>0</td><td>0.66</td></tr><tr class="odd"><td>dog</td><td>small</td><td>0.33</td><td>0</td><td>0.66</td></tr></tbody></table><p>A Target Encoder has following properties:</p><ul><li>It changes a categorical variable into some continuous variables</li><li>For continuous and binary outputs, it does not generate extra variables, for multivalued categorical output, it generate k variables, where k is the number of categories of output variable. When k &lt; c, the target encoder can be more memory-saving than one-hot encoder</li><li>It can be applied to both ordinal and nominal variable</li><li>It does not change the number of categories</li><li>There several improved target encoders like smooth target encoder and bayesian target encoder</li></ul><h2 id="frequency-encoder">5.Frequency Encoder</h2><p>The frequency encoder convert categorical variable into discrete variables by counting each category's frequency in training dataset:</p><p>A categorical variable before encoding:</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Cat</td></tr><tr class="odd"><td>Dog</td></tr><tr class="even"><td>Bird</td></tr></tbody></table><p>after encoding:</p><table><thead><tr class="header"><th>Species</th><th>X'</th></tr></thead><tbody><tr class="odd"><td>Cat</td><td>2</td></tr><tr class="even"><td>Dog</td><td>1</td></tr><tr class="odd"><td>Bird</td><td>1</td></tr></tbody></table><p>A Frequency Encoder has following properties:</p><ul><li>It changes a categorical variable into a discrete variable</li><li>It does not generate extra variables, thus is memory-saving</li><li>The might be collision of variables, and change the number of categories, thus this endocing method does not fit small dataset</li><li>There would be magnitude in transformed variables</li></ul><h2 id="binary-encoder">6. Binary Encoder</h2><p>Binary Encoder use $log_2N $ variables to express the original variable with N categories</p><table><thead><tr class="header"><th>Species</th></tr></thead><tbody><tr class="odd"><td>Cat</td></tr><tr class="even"><td>Dog</td></tr><tr class="odd"><td>Bird</td></tr><tr class="even"><td>Snake</td></tr></tbody></table><p>A variable with four categories can be expressed in a 2-dimension vector</p><table><thead><tr class="header"><th>Species</th><th>X1</th><th>X2</th></tr></thead><tbody><tr class="odd"><td>Cat</td><td>0</td><td>0</td></tr><tr class="even"><td>Dog</td><td>0</td><td>1</td></tr><tr class="odd"><td>Bird</td><td>1</td><td>0</td></tr><tr class="even"><td>Snake</td><td>1</td><td>1</td></tr></tbody></table><p>The binary encoder has similar properties as One-hot Encoder, but:</p><ul><li>It saves more memory</li><li>The generated variables is less interoperable</li></ul><h2 id="hash-encoder">7. Hash Encoder</h2><p>The Hash Enocder map the original variable into a low-dimension space, and use the length of hash bin as transformed values. It is usually applied in a text processing scenario.</p><p>A text variable before encoding:</p><table><thead><tr class="header"><th>Message</th></tr></thead><tbody><tr class="odd"><td>I love python python is good</td></tr><tr class="even"><td>I dont like python</td></tr><tr class="odd"><td></td></tr></tbody></table><p>A text variable after encoding:</p><table><thead><tr class="header"><th>text</th><th>I</th><th>love</th><th>Python</th><th>is</th><th>good</th><th>dont</th><th>like</th></tr></thead><tbody><tr class="odd"><td>I love python python is good</td><td>1</td><td>1</td><td>2</td><td>1</td><td>1</td><td>0</td><td>0</td></tr><tr class="even"><td>I dont like python</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr class="odd"><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>A Hash Encoder has following properties:</p><ul><li>It changes a categorical variable into some discrete variables</li><li>Comparing to One-hot Encoder, it saves memory when the original variable is complex and repeatable, like text and graph</li><li>The might be collision of variables, and change the number of categories, thus this endocing method does not fit small dataset</li></ul><h2 id="embedding-encoder">8. Embedding Encoder</h2><p>Embedding methods is a techniques transforms the original categorical variable into a vector that reflect the similarity of the original categories. It is more frequently used in deep learning scenarios like NLP. Generally speaking, it can be regraded as a kind of encoding methods.</p><p>[ongoing]</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Encoding</tag>
      
      <tag>Feature Engineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distribution Type Transformation in Machine Learning</title>
    <link href="/2022/10/22/distribution-type/"/>
    <url>/2022/10/22/distribution-type/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Scaler in Machine Learning</title>
    <link href="/2022/10/21/scaler/"/>
    <url>/2022/10/21/scaler/</url>
    
    <content type="html"><![CDATA[<h1 id="scaler-in-machine-learning">Scaler in Machine Learning</h1><h2 id="about-scaling">1. About Scaling</h2><p>When adopting multiple variables, many models would be effected by the difference of scales of variables. For example, distance-based clustering would enlarge the impact of variables with larger scale. Another example is that Gradient D Descending is harder to converge on unscaled data. Data Scaling can significantly improve the performance and efficiency of some models, while on other models, like tree model or Naive Bayes, data scaling does not make an influence.</p><p>​</p><h2 id="scaler">2. Scaler</h2><p>Scaler is those algorithm used to scaling data. They usually do not change the type of the data distribution. Four types of sclaer are listed as follows:</p><h2 id="standard-scaler">2.1 Standard Scaler</h2><p>The Standard Scaler does the following transformation <span class="math display">\[x&#39; = \frac{x-\bar{x}}{s}\]</span> where</p><ul><li><p><span class="math inline">\(\bar{x}\)</span> is the average of the original data</p></li><li><p>s is the standard deviation of the original data</p></li><li><p>x' is the Z-score of thr original data.</p></li></ul><p>The Standarad Scaler has following properties:</p><ul><li><p>The original data should fit a normal distribution. This is not a necessary hypothesis but the scaler <strong>works better if the original data has strong normality.</strong> After the transformation, the data follows N(0,1)</p></li><li><p>If the original data is not normally distributed, the standard sclater will not change it's type of distribution. In such case, consider distribution transformation methods before applying standard scaler.</p><p><img src="/2022/10/21/scaler/1.PNG"></p></li><li><p>The <strong>range of transformed data is not for fixed</strong>. But most data points should lie in [-3,3].</p></li><li><p>The <strong>Outlier has an significant influence on the scaling</strong>. Consider applying outlier detection method before scaling</p></li></ul><h3 id="minmax-scalermean-normalization">2.2 MinMax Scaler/Mean Normalization</h3><p>The MinMax Scaler does the following transformation <span class="math display">\[x&#39; = \frac{x-x_{min}}{x_{max}-x_{min}}\]</span> The MinMax Scaler has following properties:</p><ul><li>The MinMax scaler is a linear transformation, it does not change the type of distribution, and <strong>it does not have hypothesis on data distribution</strong></li><li>The <strong>range of data after transformation would be [0,1]</strong></li><li>The Outlier has an significant influence on the scaling. Consider applying outlier detection method before scaling</li></ul><p>A variant of MinMax Scaler is the Mean Normalization, which is given as: <span class="math display">\[x&#39; = \frac{x-\bar{x}}{x_{max}-x_{min}}\]</span> The Mean Normalization has similar properties as MinMax Scaler, but the range of transformed data would be [-1,1]</p><h3 id="robust-scaler">2.3 Robust Scaler</h3><p>The Robust Scaler does the following transformation <span class="math display">\[x&#39; = \frac{x-x_{median}}{IQR}\]</span> where IQR is the difference of 75% percentile and 25% percentile</p><p>The MinMax Scaler has following properties:</p><ul><li>The Robust Scaler does not change the type of distribution, and <strong>it does not have hypothesis on data distribution</strong></li><li>The Robust Scaler reduce the influences of the outlier. If the outlier cannot be eliminated due to some reasons, we can consider this scaling method</li><li>The range of the transformed data is not fixed</li></ul><h3 id="l1l2-scaler">2.4 L1/L2 Scaler</h3><p>The L1/L2 Scaler does the following transformation <span class="math display">\[x_i&#39; = \frac{x_i}{|x|}\]</span></p><p><span class="math display">\[x_i&#39; = \frac{x_i }{||x ||}\]</span> where</p><ul><li>x is a feature vector: {<span class="math inline">\(x_1,x_ 2,...x_ n\)</span>}</li><li><span class="math inline">\(|x|,||x||\)</span> is the L1, L2 norm of the feature vector</li></ul><p>The L1/L2 Scaler has following properties:</p><ul><li>The L /L2 Scaler is a sample-level transomation, it can be applied on a single sample</li><li>The range of the transformed data is [-1,1]</li><li>The parameters of the distribution of the transformed data is hard to predict, since the calculated norm of each sample is different</li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Scaler</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Distance in Machine Learning</title>
    <link href="/2022/10/08/distance/"/>
    <url>/2022/10/08/distance/</url>
    
    <content type="html"><![CDATA[<h1 id="distance-in-machine-learning">Distance in Machine Learning</h1><h2 id="euclidean-distance">1. Euclidean Distance</h2><p>The Euclidean distance between two data point X and Y is given as: <span class="math display">\[d = \sqrt{\sum(x_i-y_i)^2}\]</span></p><ul><li><p>E Distance is one of the most frequently used distances in machine learning, for its direct and intuitive demonstration.</p></li><li><p>E Distance treat all dimensions as same, thus would be effected by scale of variable. To eliminate that, we can used standard euclidean distance, by replace data values x with: <span class="math display">\[x&#39; = \frac{x-\bar{x}}{s}\]</span></p></li></ul><h2 id="manhattan-distance">2. Manhattan Distance</h2><p><img src="/2022/10/08/distance/MD.png"></p><p>In this demo, the euclidean distance between the beginning and the end in a district shown as the is the green line. However, as we cannot walk through the building straightforward, we must move vertically or horizontally along the blocks. This distance is called Manhattan distance.</p><p>The manhattan distance is the sum of <strong>axis difference</strong> between two data points. It is given as: <span class="math display">\[d = \sum_i^m |x_i-y_i|\]</span></p><ul><li>The manhatten distance only involves summing calculation. So it is faster then float calculation, and it does not generate accuracy lost.</li></ul><h2 id="chebyshev-distance">3. Chebyshev Distance</h2><p><img src="/2022/10/08/distance/2.png"></p><p>The manhattan distance is the maxnium of <strong>axis difference</strong> between two data points. It is given as: <span class="math display">\[D = max |x_i-y_i|\]</span></p><h2 id="minkowski-distance">4.Minkowski Distance</h2><p>Minkowski Distance is the distance between two points in a p norm space <span class="math display">\[d = (\sum|x_i-y_i|^p)^\frac{1}{p}\]</span></p><ul><li>when p = 1, MK distance equals Manhattan Distance</li><li>when p = 2, MK distance equals Euclidean Distance</li><li>when p = <span class="math inline">\(\infty\)</span>, MK distance equals Chebyshev Distance</li></ul><p>The norm p is a hyper-parameter can be adjusted. It is more flexible, but choosing p can be difficult</p><h2 id="mahalanobis-distance">5. Mahalanobis Distance</h2><p>Mahalanobis Distance is the Euclidean distance in a standard principal component space(Space after PC decomposition). <span class="math display">\[D(X) = \sqrt{(X-\mu)^TS^{-1}(X-\mu)}\]</span> where:</p><ul><li>X is the feature vector of a sample</li><li>S is the covariance matrix of all features</li><li><span class="math inline">\(\mu\)</span> is the mean vector of all samples</li></ul><p>The M distance between two data point can be given as: <span class="math display">\[D(X,Y) = \sqrt{(X-Y)^TS^{-1}(X-Y)}\]</span></p><ul><li>The M distance is not effcted my scale of data, and it can avoid the correlation among features. Thus it can be a better metrics for measuring distance between data point and central point</li><li>The M distance is usually applied in outlier detection, combined with Grubb's Test</li><li>when the covariance matrix is a identity matrix, which means the distributions of features are independent to each other, the M distance equals to the E distance</li></ul><p><img src="/2022/10/08/distance/4.png"></p><p><img src="/2022/10/08/distance/5.png"></p><h2 id="cosine-distance">6. Cosine Distance</h2><p>The Cosine Distance uses cos of the inner angle of two vectors to represent their distance <span class="math display">\[d = 1- cos(\theta) = 1- \frac{a\cdot b}{|a|*|b|}\]</span></p><ul><li><p>The rule of cosine similarity of two vector is "according -&gt; 1, Orthogonal -&gt;, inverse -&gt; -1". Such rule remains same even when the dimension is extremely high. Since the Euclidean distance could be effected by the number of dimensions, consine distance can be a better measure when it comes to high-dimension data. FOr example, a long sentence and a short sentence share a same meaning can have large euclidean distance, but small cosine distance</p></li><li><p>cosine distance is not a strictly defined distance for mathematics, as it does not satisfy triangle inequality</p></li></ul><h2 id="jaccard-distance">7. Jaccard Distance</h2><p>The Jaccard Distance uses Jaccard similarity to represent distance: <span class="math display">\[d = 1-J(A,B) = 1-\frac{|A \cap B|}{|A \cup B|}\]</span> Where J(A,B) is the ratio of two set's intersection and their union</p><ul><li>Jaccard distance can represent the similarity of two set instead of to points. For example, it can measure the similarity of two customer's shopping records. It can be applied in user segmentation and recommendation</li></ul><h2 id="string-related-distance">8. String Related Distance</h2><p><strong>Hamming Distance:</strong> For two string s1 and s2, the hamming distance is the minimun times needed to transform s1 to s2 by replacing a single character</p><p><strong>Edit Distance</strong>：For two string s1 and s2, the edit distance is the minimun times needed to transform s1 to s2 by adding,deleting or replacing a single character</p><h2 id="other-measure-similar-to-distance">9. Other Measure similar to Distance</h2><p>Some measures, just like cosine distance, are not strictly defined distance by mathematics, but can take effects in a similar way, including:</p><ul><li>Correlation Distance(for two variables)</li><li>KL divergence(for two distributions)</li><li>Mutual Information(for two variables)</li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Distance</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Outlier Detection in Feature Engineering</title>
    <link href="/2022/10/07/outlier/"/>
    <url>/2022/10/07/outlier/</url>
    
    <content type="html"><![CDATA[<h1 id="outlier-detection-and-processing">Outlier Detection and Processing</h1><h2 id="about-outlier">1. About Outlier</h2><p><strong>What is an outlier</strong></p><p>In statistics, an outlier is a data point that significantly differs from other observations.</p><p><strong>Why we need outlier detection? </strong></p><p>Outliers can lead to bias in machine learning. Many models, especially those have strong hypothesis on data distribution(LR,K- Means) would be influenced, or even be unable to converge if outliers exist in training data. Some business and engineering problem, like fraud detection and quality control, are also a outlier detection problem in nature. For this article, we only look to the outlier detection for feature engineering.</p><h2 id="statistical-method">2. Statistical Method</h2><h3 id="one-dimension-method">2.1 One-dimension Method</h3><h4 id="six-sigma-method">2.1.1 Six <span class="math inline">\(\sigma\)</span> Method</h4><p>Given a <strong>one-dimension</strong> variable X that satisfies <strong>Gaussian Distribution</strong>: <span class="math inline">\(X~N(\mu,\sigma)\)</span>, where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> is the mean and standard error of the sample:</p><p>If a point <span class="math inline">\(x \notin (\mu-3\sigma,\mu+3\sigma)\)</span>, then x can be flagged as an outlier</p><h4 id="grubbs-test">2.1.2 Grubb's Test</h4><p>Grubbs' Test is a kind of hypothesis testing that based ib the following hypothesis:</p><p><span class="math inline">\(H_0\)</span>: There are no outlier in the observations</p><p><span class="math inline">\(H_1\)</span>: There are only one outlier in the observations</p><p>There are several constraints for Grubb's Test:</p><ul><li>The Grubb's test can detect <strong>one outlier in a single test</strong>. If we want to test all data point, we can replace G statistics with z-score to test all samples. If we belive there are a large number of outliers exsits, we should consider other method.</li><li>To apply G test, the variable must follow a <strong>Gaussian Distribution</strong></li><li>The test variable must be <strong>univariate</strong>, if we want to apply the test on multivariate vector, we can calculate the distance between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\bar{x}\)</span>, and use the distance d as the test variable</li></ul><p>Define statistics G as: <span class="math display">\[G = \frac{\max|X_i - \bar{X}|}{s}\]</span> which is the z-score of the farthest data point</p><p>If: <span class="math display">\[G &gt; \frac{N- 1}{\sqrt{N}}\sqrt{\frac{t_{\frac{\alpha}{2N},N-2}^2}{N-2+t_{\frac{\alpha}{2N},N-2}^2}}\]</span></p><p>Where:</p><ul><li>N is the sample size</li><li>t represents the t score, suppose the orignal dataset can be trasformed to a t distribution</li><li><span class="math inline">\(\frac{\alpha}{2N}\)</span> is the significance level (If it's a single-side test, then <span class="math inline">\(\frac{\alpha}{N}\)</span>)</li><li>N-2 is the degree of freedom</li></ul><p>Then maxnium/minimum of these data points can be flagged as an outlier</p><h4 id="boxplot">2.1.3 BoxPlot</h4><p>Boxplot is depicted as follows:</p><p><img src="/2022/10/07/outlier/2.png"></p><p>It shows the maximun, 75 percentile, median, 25 percentile and minimum of a dataset.</p><p>Note that the maxinum and minimum are not the original values of the data, instead it's calculated as: <span class="math display">\[Max = \min(M,Q_3+1.5IQR )\]</span></p><p><span class="math display">\[Min = \max(m,Q1-1.5IQR)\]</span></p><p>where：</p><ul><li>M,m is the actual maximum, minimum of the dataset</li><li>Q1,Q3 is the 25,75 percentile of the dataset</li><li>IQR is the intermediate quantiles range of the data, <span class="math inline">\(IQR = Q3-Q1\)</span>, which is the length of the box</li><li>if the actual maximum is smaller than the calculated maximum, then plot with the actual maximun, and there are no outliers. The same apply to minimun</li><li>The data points outside the range of calculated minimum and maximum would be labeled as outliers</li></ul><p><img src="/2022/10/07/outlier/3.png"></p><h3 id="high-dimension-method">2.2 High-dimension method</h3><h4 id="chi2-test">2.2.1 <span class="math inline">\(\chi^2\)</span> Test</h4><p>[ongoing]</p><h4 id="gaussian-distribution-detection">2.2.2 Gaussian Distribution Detection</h4><p>The P value for a high-dimenstion data point is quiet difficult to calculate. We can instead set a threshold <span class="math inline">\(\epsilon\)</span>, suppose a data set <span class="math inline">\(D = \{ x^{(i)} : 0\le i \le m \}\)</span> ,let <span class="math display">\[\mu = \frac{1}{m}\sum_i^m x^{(i)},\sum=\frac{1}{m}\sum(x^{(i)}-\mu)(x^{(i)}-\mu)^T\]</span></p><p><span class="math display">\[p(x) = \frac{1}{2\pi^\frac{n}{2}|\sum|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\]</span></p><p>if <span class="math inline">\(p(x) &lt; \epsilon\)</span>, the data point can be regard as an outlier</p><h2 id="distance-based-method">3. Distance-based Method</h2><p>For details of distance metrics, refer to <a href="http://zhengyuanyang.com/2022/10/08/distance/">this article</a></p><h3 id="angle-based-method">3.1 Angle-based Method</h3><p><img src="/2022/10/07/outlier/1.png"></p><p>From this graph it is obvious to discover that the inner angle consists of an outlier and any two other points have almost same size. Thus we can define the Angle-based outlier factor as: <span class="math display">\[ABOF(o) = VAR_{x, y \ne o}[\frac{\vec{ox}\cdot \vec{oy}}{dist(o,x)^2dist(o,y)^2}]\]</span> where:</p><ul><li>o is any points in the dataset</li><li>x, y is all other point in the dataset</li><li>dist is the distance metirc, usually euclidean distance</li></ul><p>THe ABOF of an point is the variance of the consine of it and all data pairs consists of other data points. The smaller ABOF a data point has, the more likely it would be an outlier</p><p>The complexity of ABOF is <span class="math inline">\(O(n^ 3)\)</span>. Besides, when applying euclidean distance, the scale of data would have an effect</p><h3 id="nearest-neighbor-method">3.2 Nearest Neighbor Method</h3><p>The steps of Nearest neighbor method is given as follow</p><ul><li>Calculate the KNN of a data point</li><li>Add up the distances of the KNN</li><li>Repeat for every points and sort the sum of distance in descending order</li><li>The points with larger sum of distance is more likely to be outliers</li></ul><p>The tranditional KNN is not suitable for sparse dataset. To improve this, we can apply the following metrics to make judgement:</p><p><strong>Local Outlier Factor(LOF) </strong>: The ratio of the local average density of the KNN of the data point and the local average density of the point itself, the greater LOF a point has, the more likely it would be an outlier</p><p><strong>Connectivity Oulier Factor(COF) </strong>: Outlier are points p where average chaining distance of p is higer than average chaining distanc of p's neighbors. Points with higher COF has sparser surrounding than its neighbors</p><h3 id="distance-based-test">3.3 Distance-based Test</h3><p>As mentioned in 2.1.2, for high dimension data, we can apply tests on the distance of X and <span class="math inline">\(\bar{X}\)</span> instead of the original vector.</p><h2 id="model-based-method">4. Model-based Method</h2><h3 id="clustering-model">4.1 Clustering Model</h3><p>Clustering model can be used as outlier detection methods.</p><ul><li>For distance-based clustering, like K-Means, outliers are points have largest distance to their cluster centroids</li><li>For density-based clustering, like DB-SCAN, outliers would be labeled by the model</li></ul><p>For details of clustering algorithm, refer to <a href>ongoing</a></p><h3 id="one-class-svm">4.2 One-class SVM</h3><h3 id="isolation-forest">4.3 Isolation Forest</h3><h3 id="pca">4.4 PCA</h3><p>PCA can also be utilized as an outlier detection method. In PCA, we would decompose the covariance matrix, the <span class="math inline">\(m^{th}\)</span> eigenvector <span class="math inline">\(\vec{e_m}\)</span> represent present the direction of the <span class="math inline">\(m^{th}\)</span> principle component, and the eigenvalue <span class="math inline">\(\lambda_m\)</span> is the variance on that direction, which also indicates the importance of principal component. Therefore, <span class="math inline">\(\vec{x}\cdot \vec{e_m}\)</span> represents mapping all original features to the direction of <span class="math inline">\(m^{th}\)</span> principal component and adding up all decomposed vectors on that direction, as <span class="math inline">\(\vec{x}\)</span> is standardized.</p><p>Thus, we can define the deviation extent of an sample on a certain principle component direction: <span class="math display">\[d_m = \frac{\vec{x_i}\cdot \vec{e_m}}{\lambda_m}\]</span> The role of <span class="math inline">\(\lambda\)</span> is to make deviation extent more comparable, as the size of variance can be different to some degree.</p><p>With such definition, we might discover the deviation extent of an outlier could probably larger on each direction are generally higer, as shown below:</p><p><img src="/2022/10/07/outlier/4.png"></p><p>we can define the Anomaly score of a data point as: <span class="math display">\[Score(x_i) = \sum_m d_m\]</span> The higher this score is, the more likely a data point would be an outlier.</p><p>This score is equalvalent to the Mahalanobis Distance between the data sample and the mean, as the M distance is the euclidean distance in standard principal components space: <span class="math display">\[D = \sqrt{(X-\mu)^TS^{-1}(X-\mu)}=\sqrt{X^TE\lambda^{-1}E^{-1}X} = \sqrt{\frac{(EX)^2}{\lambda}} = \sqrt{d}\]</span> In this equation:</p><ul><li>X is standardized vector, <span class="math inline">\(\mu\)</span> is thus 0</li><li><span class="math inline">\(S^{-1}\)</span> is the inverse of the covariance matrix</li><li><span class="math inline">\(\lambda\)</span> is the eigenvalues matrix, which is a diagnal matrix, <span class="math inline">\(\lambda^{-1} = \frac{1}{\lambda}\)</span></li><li>E is the eigenvalues matrix, which is a orthonormal matrix, <span class="math inline">\(E^{-1} = E^T\)</span></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Outlier</tag>
      
      <tag>Data Preprocess</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>笔试中较难SQL考题总结</title>
    <link href="/2022/09/30/SQL-problems/"/>
    <url>/2022/09/30/SQL-problems/</url>
    
    <content type="html"><![CDATA[<h1 id="笔试常见sql难题">笔试常见SQL难题</h1><h2 id="用户连续登录">1. 用户连续登录</h2><p><img src="/2022/09/30/SQL-problems/1.PNG"></p><p><strong>思路</strong>：</p><ol type="1"><li><p>以用户为组，日期为序，每一行记录加上一个行号</p><p>注意，一个用户一天可以登录多次，如果全部排号，这个方法就失效了，所以加一个group by，但是不聚合，这相当于选择了distinct（user_id sales_date）的组合</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <br>    user_id,<br>    sales_date,<br>    <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> user_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> sales_date) rn<br><span class="hljs-keyword">from</span> sales_tb<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id,<span class="hljs-type">date</span>()<br></code></pre></td></tr></table></figure></li><li><p>将上表作为cte，用user_id, date_sub(current_date, interval rn day) group by， 这样相当于把连续的天数聚合到了同一个天（1/3 -1， 1/4 -2， 1/6 - 3 中断）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">with</span> cte <span class="hljs-keyword">as</span> (<br><span class="hljs-keyword">select</span> <br>    user_id,<br>    sales_date,<br>    <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> user_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> sales_date) rn<br><span class="hljs-keyword">from</span> sales_tb<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id,sales_date<br>)<br><span class="hljs-keyword">select</span> <br><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <br><span class="hljs-keyword">from</span> cte <br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> user_id, date_sub(sales_date,<span class="hljs-type">interval</span> rn <span class="hljs-keyword">day</span>) <br><br></code></pre></td></tr></table></figure></li></ol><h3 id="变体连续签到领取金币">变体：连续签到领取金币</h3><p><img src="/2022/09/30/SQL-problems/4.PNG"></p><p>思路：</p><ol type="1"><li>与上题一样，将uid和登录日期combine-distinct后，加上一列行号</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SELECT</span><br>uid,<br><span class="hljs-type">date</span>(in_time) <span class="hljs-keyword">as</span> login_date,<br><span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span>  uid <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> <span class="hljs-type">date</span>(in_time)) <span class="hljs-keyword">as</span> rn<br><span class="hljs-keyword">from</span> <br>tb_user_log<br><span class="hljs-keyword">where</span> <span class="hljs-type">date</span>(in_time) <span class="hljs-operator">&gt;=</span> <span class="hljs-string">&#x27;2021-07-07&#x27;</span> <span class="hljs-keyword">and</span> <span class="hljs-type">date</span>(in_time) <span class="hljs-operator">&lt;</span> <span class="hljs-string">&#x27;2021-11-01&#x27;</span><br>             <span class="hljs-keyword">and</span> sign_in <span class="hljs-operator">=</span> <span class="hljs-number">1</span><br>            <span class="hljs-keyword">and</span> artical_id <span class="hljs-operator">=</span> <span class="hljs-number">0</span><br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> uid, <span class="hljs-type">date</span>(in_time)<br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>将上表作为cte，同样通过减去行号的方式，将连续的签到天数聚合到一天（past day）</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>uid,<br>date_sub(login_date, <span class="hljs-type">interval</span> rn <span class="hljs-keyword">DAY</span>) ad,<br><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>) <span class="hljs-keyword">as</span> days,<br><span class="hljs-built_in">FLOOR</span>(<span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>)<span class="hljs-operator">/</span><span class="hljs-number">7</span>) <span class="hljs-keyword">as</span> fac,<br><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>)<span class="hljs-operator">%</span><span class="hljs-number">7</span> <span class="hljs-keyword">as</span> res<br><span class="hljs-keyword">from</span> cte <br><span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">by</span> uid, date_sub(login_date, <span class="hljs-type">interval</span> rn <span class="hljs-keyword">DAY</span>)<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>将上表作为cte2， 增加两列<ol type="1"><li>将past day + lag（day counts）+ 1， 这样相当于找到了同uid下上一次连续登录了几天，由于当前行的past day 是由其所代表的连续签到天数的一天减掉其行号所得到的，而这个行号比上一行的最大连续签到天数再大1，所以是past + lag + 1</li><li>根据天数计算及金币数量</li></ol></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    uid,<br>    date_add(ad, <span class="hljs-type">interval</span> (ifnull(<span class="hljs-built_in">lag</span>(days) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> uid <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> ad),<span class="hljs-number">0</span>)<span class="hljs-operator">+</span><span class="hljs-number">1</span>) <span class="hljs-keyword">DAY</span>) cd,<br>    <span class="hljs-keyword">case</span><br>    <span class="hljs-keyword">when</span> res <span class="hljs-operator">&lt;</span> <span class="hljs-number">3</span> <span class="hljs-keyword">then</span> fac<span class="hljs-operator">*</span><span class="hljs-number">15</span> <span class="hljs-operator">+</span> res<br>    <span class="hljs-keyword">when</span> res <span class="hljs-operator">&gt;=</span> <span class="hljs-number">3</span> <span class="hljs-keyword">then</span> fac<span class="hljs-operator">*</span><span class="hljs-number">15</span> <span class="hljs-operator">+</span> res<span class="hljs-operator">+</span><span class="hljs-number">2</span><br>    <span class="hljs-keyword">end</span> coin<br><span class="hljs-keyword">from</span> cte2<br></code></pre></td></tr></table></figure><ol start="4" type="1"><li>以上表为cte3，根据uid，月份聚合，得到结果，注意时间格式抽取函数的运用</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    uid,<br>    date_format(cd,&quot;%Y%m&quot;),<br>    <span class="hljs-built_in">sum</span>(coin)<br><span class="hljs-keyword">from</span> cte3<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> uid, date_format(cd,&quot;%Y%m&quot;)<br></code></pre></td></tr></table></figure><h3 id="技巧mysql-时间格式化函数">技巧：MySQL 时间格式化函数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <br>date_format(target_datetime, <span class="hljs-string">&#x27;%Y-%m-...&#x27;</span>)<br><span class="hljs-keyword">from</span><br>tb_time<br></code></pre></td></tr></table></figure><p>格式具体代码如下</p><p><img src="/2022/09/30/SQL-problems/2.PNG"></p><p><img src="/2022/09/30/SQL-problems/3.PNG"></p><h2 id="同一时间最大观看人数">2. 同一时间最大观看人数</h2><p><img src="/2022/09/30/SQL-problems/5.PNG"></p><p>思路：</p><ol type="1"><li>将进入和登出union起来，并把进入记为1，退出记为-1</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    artical_id,<br>    uid,<br>    in_time <span class="hljs-keyword">as</span> tn,<br>    <span class="hljs-number">1</span> person<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">where</span> artical_id <span class="hljs-operator">!=</span> <span class="hljs-number">0</span><br><span class="hljs-keyword">UNION</span><br><span class="hljs-keyword">select</span> <br>    artical_id,<br>    uid,<br>    out_time <span class="hljs-keyword">as</span> tn,<br>    <span class="hljs-number">-1</span> person<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">where</span> artical_id <span class="hljs-operator">!=</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>以上表为cte，以文章为组，根据文章，时间点，人数排序，用sum记录当前实时人数 running total</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span><br><span class="hljs-operator">*</span>,<br><span class="hljs-built_in">sum</span>(person) <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> artical_id <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> artical_id, tn <span class="hljs-keyword">asc</span>, person <span class="hljs-keyword">DESC</span> <span class="hljs-keyword">rows</span> <span class="hljs-keyword">BETWEEN</span> UNBOUNDED PRECEDING <span class="hljs-keyword">and</span> <span class="hljs-keyword">CURRENT</span> <span class="hljs-type">ROW</span> ) <span class="hljs-keyword">as</span> p_count<br><span class="hljs-keyword">from</span> <br>cte1<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>以上表为cte，选取各文章组别组别中实时人数最大的时间节点</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> artical_id, <span class="hljs-built_in">max</span>(p_count) <span class="hljs-keyword">as</span> max_read<br><span class="hljs-keyword">from</span> cte2 <br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> artical_id<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> max_read <span class="hljs-keyword">desc</span><br></code></pre></td></tr></table></figure><h2 id="新用户的次日留存率">3. 新用户的次日留存率</h2><p><img src="/2022/09/30/SQL-problems/6.PNG"></p><ol type="1"><li>由于跨天登入登出算都活跃，所以将in和out union起来，然后用group by uid，date(time) 来去除一天内重复的活跃记录（相当于只记录每天有没有活跃）</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>    uid,<span class="hljs-type">date</span>(in_time) <span class="hljs-keyword">as</span> tn<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">union</span><br><span class="hljs-keyword">select</span> <br>    uid, <span class="hljs-type">date</span>(out_time) <span class="hljs-keyword">as</span> tn<br><span class="hljs-keyword">from</span> tb_user_log<br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> uid,<span class="hljs-type">date</span>(tn)<br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>以上图为cte<ol type="1"><li>增加一列判断该用户本次登录时是否为新用户（以用户为组，时间为序，给每一次登录一个row number， 新用户row_numbe为1）</li><li>left join自身 on （1.day+1 = 2.day），增加一列判断该用户在后一天是否活跃（left join自身，如果后一天未活跃，则会得到null）</li></ol></li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> a1.<span class="hljs-operator">*</span>, <br>    <span class="hljs-keyword">case</span><br>    <span class="hljs-keyword">when</span> <span class="hljs-built_in">row_number</span>() <span class="hljs-keyword">over</span>(<span class="hljs-keyword">partition</span> <span class="hljs-keyword">by</span> a1.uid <span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> a1.tn) <span class="hljs-operator">=</span> <span class="hljs-number">1</span> <span class="hljs-keyword">then</span> <span class="hljs-string">&#x27;NEW&#x27;</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;OLD&#x27;</span><br>    <span class="hljs-keyword">end</span> user_status,<br>    <span class="hljs-keyword">case</span><br>    <span class="hljs-keyword">when</span> a2.uid <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">null</span> <span class="hljs-keyword">then</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>    <span class="hljs-keyword">end</span> next_day<br><span class="hljs-keyword">from</span> active a1<br><span class="hljs-keyword">left</span> <span class="hljs-keyword">join</span> active a2 <span class="hljs-keyword">ON</span><br>date_add(a1.tn,<span class="hljs-type">interval</span> <span class="hljs-number">1</span> <span class="hljs-keyword">day</span>) <span class="hljs-operator">=</span> a2.tn<br><span class="hljs-keyword">and</span><br>a1.uid <span class="hljs-operator">=</span> a2.uid<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>以上表为cte，在where语句中加入只计算新用户，月份为11等条件，group by聚合，计算每天的的今日新用户次日留存数/今日新用户总数</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <br>tn,<br>round(<span class="hljs-built_in">sum</span>(next_day)<span class="hljs-operator">/</span><span class="hljs-built_in">count</span>(<span class="hljs-operator">*</span>),<span class="hljs-number">2</span>) <span class="hljs-keyword">as</span> uv_left_rate<br><span class="hljs-keyword">from</span> cte <span class="hljs-keyword">where</span> user_status  <span class="hljs-operator">=</span> <span class="hljs-string">&#x27;NEW&#x27;</span><br> <span class="hljs-keyword">and</span> <span class="hljs-keyword">month</span>(tn) <span class="hljs-operator">=</span> <span class="hljs-number">11</span><br><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> tn<br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> tn <br></code></pre></td></tr></table></figure><h2 id="行列置换">4.行列置换</h2><p>要对每一列的值进行聚合，在把其作为一行输出</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;column_1&#x27;</span>,<span class="hljs-built_in">sum</span>(column_1), <span class="hljs-built_in">count</span>(column_1) <br><span class="hljs-keyword">from</span> orders<br><span class="hljs-keyword">union</span><br><span class="hljs-keyword">select</span> <span class="hljs-string">&#x27;column_2&#x27;</span>,<span class="hljs-built_in">sum</span>(column_2), <span class="hljs-built_in">count</span>(column_2) <br><span class="hljs-keyword">from</span> orders<br></code></pre></td></tr></table></figure><h3 id="技巧从csv导入文件">技巧：从csv导入文件</h3><p>标准的导入语法为</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL">LOAD DATA INFILE <span class="hljs-string">&#x27;path&#x27;</span><br><span class="hljs-keyword">INTO</span> <span class="hljs-keyword">TABLE</span> tb<br>FIELDS TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;,&#x27;</span><br>ENCLOSED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;&quot;&#x27;</span><br>LINES TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;\n&#x27;</span><br>IGNORE <span class="hljs-number">1</span> LINES<br></code></pre></td></tr></table></figure><p>注意：</p><ol type="1"><li><p>文件必须放在MYSQL指定的安全文件夹中，才能导入，可以使用语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SHOW</span> VARIABLES <span class="hljs-keyword">LIKE</span> <span class="hljs-string">&#x27;secure_file_priv&#x27;</span><br></code></pre></td></tr></table></figure><p>来查看</p></li><li><p>文件路径中的反斜杠/</p></li><li><p>若在windows文件夹下，行截止符应为 ''</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs SQL">LINES TERMINATED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;\r\n&#x27;</span><br></code></pre></td></tr></table></figure></li><li><p>IGNORE 用于跳过表头行</p></li></ol><h2 id="选取组内最小的另一种思路">5. 选取组内最小的另一种思路</h2><p><img src="/2022/09/30/SQL-problems/7.PNG"></p><p>思路：</p><p>这里不使用窗口函数和子语句，因为那样不方便查找min（date）所对应的device</p><ol type="1"><li>以所有该用户的event_date为子查询，选取组内小于等于任何event_date的 event date，得到的就是组内最小值，且可以简单的得到最小值对应行的其他数据</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SELECT</span> player_id, device_id <span class="hljs-keyword">from</span> Activity a1<br><span class="hljs-keyword">WHERE</span> event_date <span class="hljs-operator">&lt;=</span> <span class="hljs-keyword">ALL</span>(<span class="hljs-keyword">SELECT</span> event_date <span class="hljs-keyword">FROM</span> Activity a2 <span class="hljs-keyword">WHERE</span> a1.player_id <span class="hljs-operator">=</span> a2.player_id );<br></code></pre></td></tr></table></figure><h2 id="选取第二高的薪水">6. 选取第二高的薪水</h2><p><img src="/2022/09/30/SQL-problems/8.PNG"></p><p>思路：</p><p>可以使用dense_rank进行排序，但这里使用另一种方式</p><ol type="1"><li>选出distinct的salary，降序排列，然后使用limit offset进行筛选</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">DISTINCT</span><br>    Salary <span class="hljs-keyword">AS</span> SecondHighestSalary<br><span class="hljs-keyword">FROM</span><br>    Employee<br><span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> Salary <span class="hljs-keyword">DESC</span><br>LIMIT <span class="hljs-number">1</span> <span class="hljs-keyword">OFFSET</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>加上ifnull</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span><br>    IFNULL(<br>      (<span class="hljs-keyword">SELECT</span> <span class="hljs-keyword">DISTINCT</span> Salary<br>       <span class="hljs-keyword">FROM</span> Employee<br>       <span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> Salary <span class="hljs-keyword">DESC</span><br>        LIMIT <span class="hljs-number">1</span> <span class="hljs-keyword">OFFSET</span> <span class="hljs-number">1</span>),<br>    <span class="hljs-keyword">NULL</span>) <span class="hljs-keyword">AS</span> SecondHighestSalary<br></code></pre></td></tr></table></figure><h3 id="技巧有关limit和offset的使用">技巧：有关limit和offset的使用</h3><ol type="1"><li><p>当limit后跟一个参数时，表示从头开始选取若干行</p><p><code>select * from tb limit 3</code></p></li><li><p>当limit后跟两个参数时，表示从头开始跳过a行，再选取之后的b行</p><p><code>select * from tb limit 2,3 --跳过1-2行，选取3-5行</code></p></li><li><p>limit offset 组合使用时，limit后只跟一个参数表示选取多少行，offset后一个参数，表示跳过多少行</p><p><code>select * from tb limit 3 offset 2 --效果同 2.</code></p></li></ol><h2 id="技巧从json导入数据">7.技巧：从json导入数据</h2><p>似乎有专门的导入工具，暂时可以使用<a href="https://data.page/json/csv">此网址</a>,输入json格式的数据（不用输入表头)，将其转化为csv文件，在参考<a href="#技巧：从csv导入文件">从csv导入文件</a></p><h2 id="选择连续且为空的座位">8. 选择连续且为空的座位</h2><p><img src="/2022/09/30/SQL-problems/9.PNG"></p><p>思路：</p><p>此题应该与<a href="#1.%20用户连续登录">用户连续登录</a>区分开来，此问题需要输出所有连续的行号，而不需要聚合，所以用row number判断并不恰当</p><ol type="1"><li>将表格和自身join起来，on<ol type="1"><li>abs（1.seat - 2.seat）= 1</li><li>1.free = 2.free = 1</li></ol></li><li>这样出现在表格中的每一行都一定有一个neighbor， 且neighbour也为空座，最后再选择distinct</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs SQL"><span class="hljs-keyword">select</span> <span class="hljs-keyword">distinct</span> a.seat_id<br><span class="hljs-keyword">from</span> cinema a <span class="hljs-keyword">join</span> cinema b<br>  <span class="hljs-keyword">on</span> <span class="hljs-built_in">abs</span>(a.seat_id <span class="hljs-operator">-</span> b.seat_id) <span class="hljs-operator">=</span> <span class="hljs-number">1</span><br>  <span class="hljs-keyword">and</span> a.free <span class="hljs-operator">=</span> <span class="hljs-literal">true</span> <span class="hljs-keyword">and</span> b.free <span class="hljs-operator">=</span> <span class="hljs-literal">true</span><br><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> a.seat_id<br></code></pre></td></tr></table></figure><h2 id="技巧使用concat">9.技巧：使用concat（）</h2><p>concat()的作用是把两个列连接成一个列，当要使用类似如下语句时</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">select</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">where</span> concat(a,b) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> (<span class="hljs-keyword">select</span> concat(c,d) <span class="hljs-keyword">from</span> cte)<br></code></pre></td></tr></table></figure><p>此语句逻辑为：只要a,b不同时等于某些特定组合，就可以选择</p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SQL</tag>
      
      <tag>Interview</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面试中的贝叶斯推断问题</title>
    <link href="/2022/09/29/bayes-inference/"/>
    <url>/2022/09/29/bayes-inference/</url>
    
    <content type="html"><![CDATA[<h1 id="面试中的贝叶斯推断问题">面试中的贝叶斯推断问题</h1><h2 id="有关贝叶斯推断">1. 有关贝叶斯推断</h2><p>贝叶斯推断是一项统计技术，基于贝叶斯定理，通过观察证据来更新假设的概率。贝叶斯推断将参数看作一个变量，因此可以用于参数估计，但是在数据岗位面试中，一般以计算后验概率的方式考察</p><p>定义：</p><ul><li>H：假设变量，代表某一事件各种可能结果，服从特定分布，其概率密度函数为<span class="math inline">\(f_H\)</span>（先验概率分布）</li><li>E：证据变量，用于更新先验概率，服从特定分布，其概率密度函数为<span class="math inline">\(f_E\)</span>（边际似然函数，证据分布）</li><li><span class="math inline">\(f_{E|H}\)</span>： 似然函数，即在固定H的前提下，E的概率密度函数</li></ul><p>根据贝叶斯定理及全概率公式： <span class="math display">\[f_{post}(H=\hat{H}|E=\hat{E}) = \frac{f_{E|H=\hat{H}}(E=\hat{E})f_H(H=\hat{H})}{f_E(E=\hat{E})} = \frac{f_{E|H=\hat{H}}(E=\hat{E})f_H(H=\hat{H})}{\int_{-\infty}^\infty f_{E|H}(E=\hat{E})f_H(H)\mathrm{d}H}\]</span> 其中：</p><ul><li><span class="math inline">\(f_{post}\)</span> 称作后验概率分布，是基于证据更新后的先验概率分布</li><li><span class="math inline">\(\hat{H}\)</span> 为假设的某一特定取值</li><li><span class="math inline">\(\hat{E}\)</span> 为证据的观测值</li></ul><p>式（1）可以简写为： <span class="math display">\[P(H|E)= \frac{P(E|H)P(H)}{P(E)} = \frac{P(E|H)P(H)}{\sum_i^m P(E|H_i)P(H_i)} = \frac{P(E|H)P(H)}{P(E|H_i)P(H_i)+P(E|\bar{H_i})P(\bar{H_i})}\]</span></p><h2 id="后验概率计算的基本框架">2. 后验概率计算的基本框架</h2><p>数据分析及数据科学面试中，大多数的贝叶斯推断问题都是具体取值下的后验概率计算问题，且一般假设变量的样本空间有限，先验分布是离散的，因此其基本做题框架可以归结如下：</p><ol type="1"><li>明确问题中的假设变量和证据变量分别是什么, 对于假设变量，确定一个完备事件组</li><li>根据要求的结果，确定关心的假设事件<span class="math inline">\(A_ k\)</span>和证据事件B, 并确定先验概率P(A)</li><li>对于完备事件组中每一个假设事件<span class="math inline">\(A_ i\)</span>, 计算相应的似然性<span class="math inline">\(P(B| A_i)\)</span></li><li>根据贝叶斯定理及全概率公式计算后验概率<span class="math inline">\(P(A_i| B)\)</span>， 如果有多轮迭代，把前一轮的后验概率当作后一轮的先验概率</li></ol><h2 id="常见题型和例题解析">3. 常见题型和例题解析</h2><h3 id="检测问题">3.1 检测问题</h3><p><strong>例题</strong>：已知某肺炎的患病率为0.01%。现在需要做检测，如果被测者患病则被检测为阳性的概率为99%。如果被测者没有病则被检测为阴性的概率为99.9%。现在一个人检查结果是阳性。问真正得病的概率是多少？</p><p>此类问题的特点为：先验分布和证据分布都为二值/多值分布</p><ul><li>假设变量：患者真正得病，有得病和不得病两种结果</li><li>证据变量：患者被检测出得病，有检测出得病和检测出不得病两种结果</li></ul><p>令：</p><ul><li>A事件为患者真正得病 <span class="math inline">\(P(A) = P(H=true\ positive)\)</span></li><li>B事件为患者诊断得病<span class="math inline">\(P(B) = P(E = diagnosed \ positive)\)</span></li></ul><p><span class="math display">\[P(A) = 0.01\%\]</span></p><p><span class="math display">\[P(B|A) = 99\%\\\]</span></p><p><span class="math display">\[P(B) = P(B|A)P(A)+P(B|\bar{A})P(\bar{A}) = 99\%*0.01\% + 0.1\%*99.99\%\]</span></p><p>可计算得到后验概率为： <span class="math display">\[P(A|B) = 9.01\%\]</span> 可见诊断后后，患者得病概率上升，原本认为得病概率为0.01%，经过一个只包含一个人的样本重新计算，更新为了9.01%</p><h3 id="硬币问题">3.2 硬币问题</h3><p>假设有100枚硬币，其中70枚为正常，30枚为缺陷，从中取出1枚投掷10次，10次中9次为正面，1次为负面。当硬币正常时，投出正面的概率为0.5，当硬币缺陷时，投出正面的概率为0.8。求硬币缺陷的概率</p><p>此类问题的特点为，先验分布是二值分布，证据分布是伯努利分布</p><ul><li>假设变量：硬币有缺陷，有是否两种可能</li><li>证据变量：投出n次正面，其概率为<span class="math inline">\(p^n*q^{1-n}\)</span></li></ul><p>令：</p><ul><li>A事件为硬币有缺陷 <span class="math inline">\(P(A) = P(H = defective)\)</span></li><li>B事件为投出9次正面<span class="math inline">\(P(B) = P(E = 9 \ head)\)</span></li></ul><p><span class="math display">\[P(A) = 0.3\]</span></p><p><span class="math display">\[P(B|A) = 0.8^9*0.2^1\\\]</span></p><p><span class="math display">\[P(B) = P(B|A)P(A)+P(B|\bar{A})P(\bar{A}) = 0.8^9*0.2^1 *0.3 + 0.5^9*0.5^1*0.7\]</span></p><p>可计算得到后验概率为： <span class="math display">\[P(A|B) = 91.2\%\]</span> 与此类似的问题有朋友说谎问题等</p><h4 id="朋友说谎问题">3.2.1 朋友说谎问题</h4><p>假设某地今天下雨的概率是<span class="math inline">\(\frac{1}{2}\)</span>，你在前往某地前向3个当地的朋友询问是否下雨，所有人都该诉你下雨了，但是每个人都有<span class="math inline">\(\frac{1}{3}\)</span>的概率说谎， 求某地今天真正下雨的概率</p><ul><li>A：某地下雨</li><li>B：三个朋友都告诉你下雨</li></ul><p><span class="math display">\[P(A) = \frac{1}{2}\]</span></p><p><span class="math display">\[P(B|A) = \frac{2}{3}^3\\\]</span></p><p><span class="math display">\[P(B) = \frac{2}{3}^3*\frac{1}{2}+\frac{1}{3}^3*\frac{1}{2}\]</span></p><h3 id="三门问题">3.3 三门问题</h3><p>有A，B，C三扇门，其中一扇背后有奖品，当玩家选择一扇门以后，主持人会打开另一扇后面是空的门，请问玩家是否要改变自己的选择？</p><p>此类问题特点为：</p><ul><li>共出现3个随机变量，都是多值分布，但是这三者之间是互斥的，如果A为1，则B，C不为1</li><li>此时先验变量为“某一变量为一特定值”，证据变量为“另一个变量被揭晓不为该值”，最后一个变量仅用于计算全概率，或者用于比较选择</li></ul><p>令：</p><ul><li>A事件为门后有奖的是A门<span class="math inline">\(P(A) = P(H = A)\)</span></li><li>D事件为B门被打开且为空<span class="math inline">\(P(B) = P(H_D= B)\)</span></li></ul><p>则： <span class="math display">\[P(A) = \frac{1}{3}\]</span></p><p>$$</p><p><span class="math display">\[当A门后有车，B，C门被打开的概率是一样的\]</span> P(D|A) =  $$ 当B门后有车，主持人不会打开B门，因此<span class="math inline">\(P(D|B) = 0\)</span></p><p>当C门后有车，主持人只能打开B门，因此<span class="math inline">\(P(D|C) = 1\)</span></p><p>因此，<span class="math inline">\(P(D) = P(A)*P(D|A)+P(B)*P(D|B)+P(C)P(D|C) = \frac{1}{3}*\frac{1}{2}+0+\frac{1}{3}*1 = \frac{1}{2}\)</span></p><p>由此可以计算得到： <span class="math display">\[P(A|D) = \frac{1}{3}\\P(C|D) = \frac{2}{3}\]</span> 因此，应该选择C门</p><h4 id="赦免问题">3.3.1 赦免问题</h4><p>有A，B，C三个囚犯，其中一个人将被赦免，另外两个将被杀死，如果有囚犯问看守，看守只能告诉他某一个人将被处死，而且看守不能告诉问他的人是否被处死。A问看守，看守回答B要被处死，求这种情况下，A和C被赦免的概率</p><p>令：</p><ul><li>A事件为被赦免的是A</li><li>D事件为看守回答B被处死</li></ul><p><span class="math display">\[P(A) = \frac{1}{3}\]</span></p><p><span class="math display">\[P(D|A) = \frac{1}{2}\\\]</span></p><p><span class="math display">\[P(D|B) = 0\]</span></p><p><span class="math display">\[P(D|C) = 1\]</span></p><p>则计算可得： <span class="math display">\[P(A|D) = \frac{1}{3}\]</span></p><p><span class="math display">\[P(C|D) = \frac{2}{3}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Interview</tag>
      
      <tag>Bayesian Inference</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evaluation for Regression</title>
    <link href="/2022/09/22/evaluation-for-regression/"/>
    <url>/2022/09/22/evaluation-for-regression/</url>
    
    <content type="html"><![CDATA[<h1 id="evaluation-for-regression-model">Evaluation for Regression Model</h1><h2 id="evalution-for-goodness-of-fitting">1. Evalution for goodness of fitting</h2><p>In most case, we can directly used the loss function's value on testing dataset as a metric to evaluate a regression model's performance</p><p>For common loss function for regression model, refer to <a href="http://zhengyuanyang.com/2022/09/22/loss-function/">here</a></p><p>here supplement several other metrics</p><p><strong>MSLE</strong></p><p>Mean Squared Log Error <span class="math display">\[MSLE = \frac{1}{n}\sum_i^n(log(1+y_i)-log(1+\hat{y)}^2\]</span></p><ul><li>Add one to avoid the appearance of log 0</li><li>MSLE is ideal when we priorly know there's a exponential relationship between Y and some X(e. g. Population). However, in many cases we would transorm data with exponential distribution in data preprocessing so that the exponential relationship would be eliminated. Thus, MSLE is not frequently used</li></ul><p><strong><span class="math inline">\(R^2\)</span></strong></p><p>Coefficient of Determination <span class="math display">\[R^2 = 1-\frac{\sum y-\hat{y}}{y-\bar{y}} = 1-\frac{RSS}{TSS}\]</span></p><ul><li><span class="math inline">\(R^2\)</span> represent how many variance of Y are interpreted by the model</li><li>If <span class="math inline">\(R^2 = 0\)</span>, TSS = RSS, which means the model equals to a model that simply predicted all samples as the average of Y, which indicates the model has a bad performance</li><li><span class="math inline">\(R^2\)</span> is not a true square number, the range of it is <span class="math inline">\((-\infty,1])\)</span></li></ul><h2 id="evaluation-of-multicollinearity">2. Evaluation of Multicollinearity</h2><p>Multicollinearity refer to the relationship that a variable's change would cause another variable's change</p><p>In regression, we do not want there to be multicollinearity among input variables.</p><p>For example suppose we have a ideal regression model that <span class="math display">\[y = 10x_1+b\]</span> Then we introduce a highly correlated variable <span class="math inline">\(x_2 = 2x_1\)</span>, in this case the modle can fit infinite possible combination of these two variables, for example, the model might end up with: <span class="math display">\[Y = -100x_1 +55x_2 +b = 10x_1+b\]</span> These will cause:</p><ul><li>coefficient might lose its interpretability. Positive coefficient might turn negative, insignificant variable might turn significant</li><li>The model might lose stability since the coefficients are enlarged. Small noises could cause big variance</li></ul><p><strong>VIF</strong></p><p>Variance Inflation Factor <span class="math display">\[VIF = \frac{1}{1-R^ 2}\]</span> As discussed above, <span class="math inline">\(R^2\)</span> represent the goodness of fitting. Thus we can use an input variable <span class="math inline">\(x_i\)</span> as the output variable, and fit a model with other input variable <span class="math inline">\(x_1,...x_{i-1},x_{i+ 1}...x_m\)</span> being inputs. If the <span class="math inline">\(R^2\)</span> turn out to be high, then it is possible that <span class="math inline">\(x_1\)</span> are highly correlated with other variables. In such case, the VIF would be high</p><p>In practice, if VIF &gt; 10, we can consider as there's a Multicollinearity problem. If VIF &gt; 100, we can consider as there's a serious Multicollinearity problem</p><p>The solution Multicollinearity includes:</p><ul><li><p>Filtering feature selection methods based on correlation</p></li><li><p>Wrapper feature engineering methods like stepwise regression</p></li><li><p>Embedded Feature Engineering methods like Lasso &amp; Ridge Regression</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Information Theory in ML</title>
    <link href="/2022/09/22/information-theory-in-ML/"/>
    <url>/2022/09/22/information-theory-in-ML/</url>
    
    <content type="html"><![CDATA[<h1 id="information-theory-in-machine-learning">Information Theory in Machine Learning</h1><h2 id="information-and-entrophy">1. Information and Entrophy</h2><p><strong>self-info</strong></p><p>self-information is a measure related to the outcome of a probabilistic event. To quantify the information, we hope the measure would have following properties:</p><ul><li>A low-probability outcome contains more infomation than a high-probability event. For example, "I won't die" contains more information than "I will die". To some extent, we can regard self-info as the degree we would feel surprise about an certain outcome of an event</li><li>Information quantity must not be negative</li><li>The total imformation of several outcome should be the sum of ther individual information</li><li><span class="math inline">\(I(y=m_1,y=m_2) = I(y=m_1)+I(y=m_2)\)</span></li><li>I(y) must be continuous if P(y) is continuous</li></ul><p>It can be proved that the only expression that satisfies these properties is <span class="math inline">\(Klog(P(y))\)</span> where K is negative. Set K= 1, we define the self-information of an outcome of an event as: <span class="math display">\[I  = log(\frac{1}{P(y=c_m)}) = -log(P(y=c_m))\]</span></p><p><strong>Entrophy</strong></p><p>Entrophy is the expectation of self-info. It represent the mean information of an event related to an random variable y, the greater H is, the more uncertain event y is <span class="math display">\[H(Y) = E[I(y)] = -\sum_i^m p(y=c_m)log(p(y=c_m))\]</span></p><h2 id="entrophy-measure-for-two-variables">2. Entrophy measure for two variables</h2><h3 id="joint-entrophy">2.1 Joint Entrophy</h3><p>Joint entropy measure the uncertainty of a joint event (X, Y) <span class="math display">\[H(T,Y) =-\sum_t\sum_y p(t,y)log(p(t,y))\]</span></p><h3 id="conditional-entrophy">2.2 Conditional Entrophy</h3><p>Conditional Entrophy represent the Information recieved of one event when another event is for certain <span class="math display">\[H(Y|T) =-\sum_t\sum_y p(t,y)log(p(y|t)) = -\sum_t\sum_y p(y|t)p(t)log(p(y|t))\]</span> The higher H(Y|T) is, the less "extra information" about Y is needed when T is certain, which means the corralation between Y and T l is higher</p><p>The relationship between conditional entropy and joint entropy: <span class="math display">\[H(Y｜T) = H(Y, T) - H(T)\]</span></p><h3 id="inofrmation-gain">2.3 Inofrmation Gain</h3><p>Information gain represent the amount of uncertainty reduction of a variable when another variable is certain <span class="math display">\[IG(Y|T) = H(Y) - H(Y| T)\]</span> It is usually applied as an impurity metrics in splitting algorithm like Decision Tree</p><h3 id="mutual-information">2.4 Mutual Information</h3><p>The nature of Mutual Information and Information Gain is the same, mutual information is the shared entropy of two variable</p><p><img src="/2022/09/22/information-theory-in-ML/1.png"></p><p>First, when we consider two event together, we can rewrite H(Y) as: <span class="math display">\[H(Y) = -\sum_i^m p(y=c_i)log(p(y=c_i) \\= -\sum_j^n p(x = c_j)\sum_i^mp(y=c_i|x_=c_j)log(p(y=c_i)) \\= \sum_x\sum_yp(x,y)log(p(y))\]</span></p><p><span class="math display">\[MI = H(x,y) - H(x|y) - H(y|x) = \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)}\]</span></p><p><span class="math display">\[IG = H(y)-H(y|x) = \sum_x\sum_yp(x,y)log(p(y)) - \sum_x\sum_yp(x,y)log(p(y|x)) = MI\]</span></p><p>MI is usually mentioned when calculating the correlation of two variable, IG is mentioned when calculating the impurity reduction of one variable when splitting another variable</p><h2 id="entrophy-measure-for-two-distribution">3. Entrophy measure for two distribution</h2><h3 id="kl-divergencerelative-entrophy">3.1 KL Divergence(Relative Entrophy)</h3><p>The KL divergence is the expectation(under true distribution) of the difference between information amount under predicted and true distribution <span class="math display">\[D_{KL}(p||q) = \sum_{i=1}^mp(y_i)\log(\frac{p(y_i)}{q(y_i)})\]</span> Where:</p><ul><li>q is the estimated distribution of y</li><li>p is the real distribution of y</li></ul><p>it can be used to evaluate an estimation of a distribution with the real distribution given</p><h3 id="cross-entrophy">3.2 Cross Entrophy</h3><p>The equation KL divergence can be written as: <span class="math display">\[D_{KL}(p||q) = \sum_{i=1}^mp(y_i)\log(\frac{p(y_i)}{q(y_i)}) = -\sum_i^mp(y_i)log(q(y_i)) - (-\sum_i^mp(y_i)log(p(y_i)))\]</span> Obviously, the second term of this equation is <span class="math inline">\(H_p(x)\)</span>, which is the entrophy of the real data. In a machine learning problem, since real entrophy is fixed for a dataset, we can only minimize the first term. we call this term Cross entropy: <span class="math display">\[CH(p,q) = D_ {KL}(p||q) + H_p(X) = -\sum_i^mp(y_i)log(q(y_i))\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Loss Function</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Common Loss Function in Machine Learning</title>
    <link href="/2022/09/22/loss-function/"/>
    <url>/2022/09/22/loss-function/</url>
    
    <content type="html"><![CDATA[<h1 id="common-loss-function-in-machine-learning">Common Loss Function in Machine Learning</h1><h2 id="numerical-output">Numerical Output</h2><h3 id="msel2-lossrmse">1. MSE(L2 Loss)/RMSE</h3><p>Mean Squared Error <span class="math display">\[MSE = \frac{1}{n}\sum_i^n(y_i-\hat{y})^2\]</span></p><ul><li>MSE is differentiable any where, which makes it suitable for gradient descent optimizer</li><li>With gradient decrease, the MSE will decrease, which means MSE is effective with fixed learning rate</li><li>The squared operator gives enlarge the loss, thus MSE is sensitive ti outlier, if outlier is meant to be detected, suing MSE is fine. If the outlier is treat as part of the training data, MSE is not an ideal loss function (e. g. prediction model for sales promotion day)</li></ul><p><img src="/2022/09/22/loss-function/1.png"></p><p>RMSE is the square root of MSE <span class="math display">\[RMSE = \sqrt{MSE}\]</span></p><ul><li>RMSE is on the same scale with MSE, but it make the loss more interpretable in some case(e.g prediction on product price)</li></ul><h3 id="mae-l1-lossmape">2. MAE (L1 Loss)/MAPE</h3><p>Mean Absolute Error <span class="math display">\[MAE = \frac{1}{n}\sum_i^n|y_i-\hat{y}|\]</span></p><ul><li>MAE is not sensitive to outlier, it is less likely to cause gradient explosion</li><li>The gradient for any loss is the same, which means MAE does not converge well with fix learning rate</li><li>MAE is not differentiable at 0</li></ul><p><img src="/2022/09/22/loss-function/2.png"></p><p>MAPE is the mean percentage absolute error <span class="math display">\[MAPE = \frac{1}{n}\sum_i^n\frac{|y_i-\hat{y}|}{y_ i}\]</span></p><ul><li>It can more clearly deliver the degree of deviation rather than the scale of the error</li></ul><h3 id="smooth-l1huber-loss">3. Smooth L1(Huber Loss)</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^nz_ i\\z_i = \left\{\begin{aligned}MSE \qquad&amp; |y -\hat{y}| &lt; \beta \\MAE \qquad &amp; elsewhere \\\end{aligned}\right.\]</span></p><ul><li><span class="math inline">\(\beta\)</span> is a defined threshold for error of outlier. If the error is within <span class="math inline">\(\beta\)</span>, apply MSE, otherwise, apply MAE. Huber Loss combines the advantages of L1 and L2 Loss</li><li>Huber loss is ideal for NN</li></ul><p><img src="/2022/09/22/loss-function/3.png"></p><h3 id="log-cosh-loss">4. Log-cosh Loss</h3><p><span class="math display">\[L = \sum_ i^ nlog(cosh(y_i-\hat{y_i}))\]</span></p><ul><li>Log-cosh has almost every virtue of Huber loss, the difference is, it is second-differentiable anywhere, such method is very useful when applying newton-method optimzer</li><li>However, when erro is very big, Log-cosh loss still would have gradient or hessian problem(gradient remains same when loss decrease). Just like MAE</li></ul><h2 id="categoricalprobability-output">Categorical/Probability Output</h2><h3 id="log-losscross-entropy-loss">1. Log Loss/Cross Entropy Loss</h3><p>Log Loss is basically the same concept as the cross entropy, the only difference is that it can be applied at a single sample by replaceing true probability p(x) with 1 if <span class="math inline">\(y_ {true} = m\)</span>. The average loss of all sample is the Log Loss or Cross Entropy Loss.</p><p>For Cross Entropy, refer to <a href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">here</a></p><p>When it come to a binary classification scenario, the Log Loss can be written as: <span class="math display">\[LogLoss = -\frac{1}{n}\sum_i^n [y_ilog(p(y_i))+(1-y_i)log(1-p(y_i))]\]</span></p><ul><li>The gradient would decrease as Log Loss decrease, thus Log Loss can work with fixed learning rate</li><li>Sentive to outlier</li><li>GDBT usually apply Log Loss as loss function(classification)</li></ul><p>The following picture is the log loss of the prediction on a positive sample:</p><p><img src="/2022/09/22/loss-function/4.png"></p><h3 id="exponential-loss">2. Exponential Loss</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^ne^{-yf(x)}\]</span></p><ul><li>Theoretically, the optimal of Exponential Loss and Log Loss is the same(<span class="math inline">\(\frac{1}{2}log\ odds\)</span>), the advantage of exponential loss is that is is easier to calculate, thus can make optimzer update the weight with less cost</li><li>Sentive to outlier</li><li>AdaBoosting usually apply Exponential Loss as loss function(classification)</li></ul><h3 id="hinge-loss">3. Hinge Loss</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^n max(0,1-yf(x))\]</span></p><ul><li>If the model label the sample correct, the loss is 0</li><li>Less sensitive to outlier</li><li>SVM usually adopt Hinge Loss as loss function</li></ul><p><img src="/2022/09/22/loss-function/5.png"></p><h3 id="focal-loss">4. Focal Loss</h3><p>Focal Loss is a improved version of Cross Entrophy Loss <span class="math display">\[L_ f = -\frac{1}{n}\sum_i^n\sum_ j^m\alpha_ j(1-p(y ))^\gamma y log(p(y))\]</span></p><ul><li>Focal Loss add a focus factor <span class="math inline">\((1-p(y))\)</span>, so that those sample with high predicted probability, which are the ""easy samples", donate less loss. Compare to CE Loss, Focal Loss focus on those "hard samples"</li><li>Focal Loss also add a balance factor(optional), which is the percentage of a certain category of y among all samples. This make focal loss can deal with imbalanced data</li><li><span class="math inline">\(\gamma\)</span> is a influence parameter. When <span class="math inline">\(\gamma = 0\)</span>, the Focal Loss become CE Loss. In preactice, we usually set <span class="math inline">\(\gamma = 2\)</span></li></ul><h3 id="impurity">5.Impurity</h3><p>Impurity is a kind of loss functions usually applied in splitting in decision tree</p><h4 id="gini-impurity">5.1 Gini Impurity</h4><p><span class="math display">\[I_G = 1- \sum_{i}^m P(Y=C_i)\]</span></p><ul><li>Lower <span class="math inline">\(I_G\)</span> , better classification performance</li><li>For decision tree model, calculate <span class="math inline">\(I_G\)</span> for each split and use combined Gini Impurity(<span class="math inline">\(\sum I_G\)</span>) as loss of the spiltting</li></ul><h4 id="information-gain">5.2 Information Gain</h4><p>For details of Information Gain, refer to <a href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">here</a> <span class="math display">\[IG = H(Y) - H(Y|X)\]</span> where X is the feature the spliting based on (X&gt;c,X=1)</p><ul><li>IG is the degree that uncertainty reduce after the spilting, the greater IG is, the better a split is</li><li>We can calculate the Information Gain Rate <span class="math inline">\(IGR = \frac{IG}{H(Y)}\)</span> to present the degress of uncertainty reduction more directly</li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Loss Function</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evaluation Method for Classification</title>
    <link href="/2022/09/21/Evaluation_Classification/"/>
    <url>/2022/09/21/Evaluation_Classification/</url>
    
    <content type="html"><![CDATA[<h1 id="evaluation-method-for-classification">Evaluation Method for Classification</h1><h2 id="confusion-matrix">1. Confusion Matrix</h2><p>The confusion matrix is a table that count different cases of the predicted outcome given by a classification model. For a binary classification model, it can be given as:</p><p><img src="/2022/09/21/Evaluation_Classification/confusion-matrix.png"></p><p>For a multiple classification case, the confusion matrix can be given as:</p><p><img src="/2022/09/21/Evaluation_Classification/2.png"></p><p>For such case, we can define TP, TN, FP, FN for each category by deeming all true “not-bird” samples predicted “not-bird” as TN, etc.</p><h2 id="accuracy-precision-recall-and-f1-score">2. Accuracy, Precision, Recall and F1 score</h2><p>With confusion matrix given, we can now define the following matrix:</p><p><strong>Accuracy</strong></p><p>Accuracy = samples predicted correctly / all predicted samples <span class="math display">\[accuracy = \frac{TP+TN}{TP+FP+FN+FP} \]</span> Accuracy is a very intuitive metrics. However, it sometimes cannot directly reflect the predicting performance of the model as it cannot deal with imbalanced data. For example, suppose we have a sample set of 100 sample with 99 positive and 1 negative, even if the model simply predicted all samples as positive without any training, it would still receive an accuracy of 99%.</p><p><strong>Recall and Precision</strong></p><p>Recall = correctly predicted positive samples / all actual positive samples <span class="math display">\[racall = \frac{TP}{TP + FN}\]</span> Recall represent the ability to find positive samples among all actual postive samples. It can deal with imbalanced data. It is sensitive to FN case, thus is suitable for the business case where FN would bring significant cause(e. g Explosion recognition, vehicle safety judgement). On the other side, recall does not consider FN, a model can simply improved recall by judging all samples as positive, which is not good in some cases.</p><p>Precision = correctly predicted positive samples / all predicted positive samples <span class="math display">\[precision = \frac{TP}{TP + FP}\]</span> Precision represent the probability that a model's judgment on positive case is correct. It can partly deal with imbalanced data. It is sensitive to FP case, thus is suitable for the business case where FP would bring significant cause(e. g Crime judgment, disease diagnosis).</p><p>Recall and Precision can both deal with imbalanced data. However, there's a trade-off between these two metrics. Thus, which metric to put emphasis on idepends on specific business application. Nevertheless, in most cases, since we can just flip P/N, or 1/0, precision is more like an accompanied constraint of recall to prevent model from foucsing too much on capture the minor category samples, as FN and FP are both bad in most business application.</p><p><strong>F1 Score</strong></p><p>The F-measure is a function that balance Precision and recall <span class="math display">\[F_\alpha = \frac{(1+\alpha^2)*P*R}{(\alpha^2*P)+R}\]</span> when <span class="math inline">\(\alpha\)</span> = 1, we call this metric F1 score: <span class="math display">\[F1 = \frac{2PR}{P+R}\]</span> F1 socre combine Recall and Precision to find a balance. It is suitable for many business case where we cannot decide a clear preference.</p><h2 id="p--r-curve-roc-curve-and-auc">3. P- R Curve, ROC Curve and AUC</h2><p><strong>P- R Curve</strong></p><p>P-R Curve is a curve to depict the relationship between Precision and Recall. It's application is similar to F1 score, but lessly used as F1 score is more concise to read( The higher the better)</p><p><img src="/2022/09/21/Evaluation_Classification/3.PNG"></p><p>Usually, we can regard model B as a better model if it can completely wrap the curve of A. If that; s not the case, we can mark the point on the curve where precision equals recall(Break Event Point, or BEP). The curve with a BEP closer towards up- right direction is better.</p><p><strong>ROC Curve</strong></p><p>We define:</p><ul><li>True positive rate (sensitivity): the ratio that actual positive samples predicted correctly</li><li>True negative rate (specificity): the ratio that actual negative samples predicted correctly</li><li>False positive rate (1-specificity): the ratio that actual negative samples predicted wrongly</li></ul><p><span class="math display">\[TPR = \frac{TP}{TP+FN} = Recall\\TNR = \frac{TN}{FP+TN} = 1-TNP\\FPR = \frac{FP}{FP+TN}\]</span></p><p>From a probabilistic aspect:</p><table><tbody><tr class="odd"><td>Precision</td><td style="text-align: center;"><span class="math inline">\(P(Y=1|\hat{Y}=1)\)</span></td></tr><tr class="even"><td>Recall (sensitivity)</td><td style="text-align: center;"><span class="math inline">\(P(\hat{Y}=1|Y=1)\)</span></td></tr><tr class="odd"><td>Specificity</td><td style="text-align: center;"><span class="math inline">\(P(\hat{Y}=0|Y=0)\)</span></td></tr></tbody></table><p>From this interpreation, we found that sensitivity and specificity are condition on Y, which means The influence of P(Y) are blocked whe calculating these two metrics. Therefore, these two metrics are not influenced by the imbalance of data.</p><p>The Reciever Operating Characteristics cureve(ROC curve) take both metrics into consideration by depict the relationship between sensitivity and 1-specificity:</p><p><img src="/2022/09/21/Evaluation_Classification/4.PNG"></p><p><strong>AUC</strong></p><p>Area Under Curve(AUC) is the area beneath the ROC curve.</p><p>Suppose our model completely randomly classifies the samples, the the probability it regard an actual postive sample or an actual negative sample as a positive sample is equal, in this case, AUC would be 0.5. If AUC &gt; 0.5, it means when the model predicts a sample, <span class="math inline">\(P(\hat{Y}=1|Y=1) &gt; P(\hat{Y}=1|Y=0)\)</span> , which means the prediction is effective. Thus, the higher the AUC is, the better the model performs.</p><p>Obviously, AUC is not influenced by imbalanced data, and it's delivery information concisely. Thus, it is one of the most frequently used metrics in classification.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
      <tag>Classification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Causes and Solutions of Overfitting</title>
    <link href="/2022/09/19/overfitting/"/>
    <url>/2022/09/19/overfitting/</url>
    
    <content type="html"><![CDATA[<h1 id="overfitting-cause-and-solution">Overfitting: Cause and Solution</h1><h2 id="biasvariance-and-overfitting">1. Bias，Variance and Overfitting</h2><p>Theoretically, we consider the expected risk of the model on real data consists of two parts:</p><p><strong>Bias</strong></p><p>The model's failure to imitate the real mapping, probability distribution or other relationship of the data. For example, if we try to fit a non-linear relationship with a linear model, there would be inevitable bias. Another example is that we leave some important factors omitted</p><p><strong>Variance</strong></p><p>The model's sensitivity to changes in data. High variance occurs when the model try to seize any details. It put too many weights on unimportant feature or noise in order to reduce error in training process.</p><p>Neither high bias nor high variance is good. However, when we take measure to solve bias, like applying complex model or increase feature, the variance of the model increase accordingly. In other worlds, there is a trade-off between bias and variance. In training process, our aim would be finding a model with bias and variance acceptable.</p><p><strong>Underfitting and Overfitting</strong></p><p>When the bias is high, we cannot describe the pattern of the data correctly, we call such a scene Underfitting</p><p>When the bias is low, but the variance is high, the model is trapped in the details or noises of the data, ot fit well on the given data but it has poor generality so that it would fail on other data</p><p><img src="/2022/09/19/overfitting/1.PNG"></p><p>The performance on training and testing dataset under these scenes</p><table><thead><tr class="header"><th></th><th>training SET Erro</th><th>Testing Set ERROR</th></tr></thead><tbody><tr class="odd"><td>Underfit</td><td>high</td><td>high</td></tr><tr class="even"><td>Overfit</td><td>low</td><td>high</td></tr><tr class="odd"><td>Optimum</td><td>low</td><td>low</td></tr></tbody></table><h2 id="causes-of-overfitting">2. Causes of Overfitting</h2><p>As specified above, the core reason that cause of overfitting is the imbalanced trade-off(High variance, low bias). Specifically, we induct the reasons into:</p><ul><li>Complexity of the model: The model is too complex for the pattern we want to discover in the data</li><li>Defects of data: the samples contains so much noises that the model cannot ignore them</li><li>Overtraining: the model is trained with too much epochs that force the model to learn noises in order to converge</li><li>Improper sampling/splitting: the training set fails to represent the distribution of the real data. Or, in some other case, the real distribution itself decides that the model is hard to imitate it.</li></ul><h2 id="solutions-for-overfitting">3. Solutions for Overfitting</h2><p>According to four reasons, we can also induct the solutions into:</p><h3 id="control-complexity">3.1 <strong>Control complexity</strong></h3><p><strong>Regularization</strong></p><p>In machine learning, regularization refer to constraints on the number of feature dimensions. Usually, it is realized through adding an regular term to the loss function to penalize putting weight on too many features. This includes:</p><ul><li>Lasso regression and Ridge regression</li><li>Soft margin for SVM</li><li>Regular term in XGBoost</li><li>...</li></ul><p><strong>Feature Engineering</strong></p><p>Reduce the number of features. FIlter those redundant feature through feature engineering methods like correlation analysis and dimensionality reduction. For details, please refer to <a href>ongoing</a></p><p><strong>Simplify Structure</strong></p><p>An important rule for machine learning is to solve the task with possible simplest model. A model with simpler structure can usually solve overfitting. Specific action includes:</p><ul><li>Dropout in NN</li><li>pruning in tree-based model</li><li>Hyperparameter like hidden size, max-leaf-node</li></ul><h3 id="data-augmentation"><strong>3.2 Data augmentation </strong></h3><p>The best way to eliminate the variance caused by data defects is simply increasing more data to the training set. Since sufficient data are sometimes unavailable in real project. We can apply data augmentation to generate more training data. Data augmentation is more common in deep learning feild.</p><h3 id="early-stopping"><strong>3.3 Early stopping</strong></h3><p>Stop the optimizer earlier to prevent overtraining. For example:</p><ul><li>raise error threshold for an optimzer</li><li>set max depth for an decision tree</li><li>set max iterations for an neural network</li></ul><h3 id="sampling-and-spliting"><strong>3.4 Sampling and spliting</strong></h3><p><strong>Cross Validation</strong></p><p>Cross validation means spliting the dataset into subsets. Use some of them to estimate the distribution and other of them to evaluate the estimation. Such procedure can effectively control the varaince caused by sample selection bias. For the details of cross validation, refer to<a href>ongoing</a></p><p><strong>Sampling </strong></p><p>From an theoretical perspective, sampling itself is actually a kind of non-parameter ML model. When you do sampling, yur actual target is to imitate the distribution of the real population through getting a sample. Thus, the sample itself would have bias if it cannot represent the true distribution of the population. Training a mode using these samples would obviously cause variance.</p><p>Thus, a way might help solving overfitting is improving your sampling method. For details of sampling methods in ML, refer to <a href>ongoing</a></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
      <tag>Overfitting</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Principle</title>
    <link href="/2022/09/12/Principle/"/>
    <url>/2022/09/12/Principle/</url>
    
    <content type="html"><![CDATA[<h1 id="learning-principle">Learning Principle</h1><p>In machine learning, learning principle refer to the standard for judging whether a model is good or not</p><h2 id="loss-function-risk-function-and-objective-function">1. Loss function, Risk function and Objective function</h2><p>Loss function is a function used to evaluate the fitness of a model</p><p>For supervised learning, loss function evaluate the difference between the predicted output and true output, noted as <span class="math inline">\(L(Y,f(x, \theta))\)</span>. Ususally, loss function is applied to a single sample or a part of the samples, it cannot evaluate the overall performance of the model. To obtain that, we define: <span class="math display">\[R_{exp}(\theta) = E_P[L(Y,f(x, \theta))] = \int_{X*Y}L(Y,f(x, \theta)P(X,Y)dxdy\]</span> Where <span class="math inline">\(R_{exp}\)</span> is called <strong>risk function</strong> or **expected loss*</p><p>To convert a ML problem into an optimization problem, a ideal practice is to adopt risk function as the objective function for the optimization program. However, this require us to know the true JPD P(X, Y), which is usually unknown in real problem(we can only know the estimated JPD through observational dataset), this is called an ill-formed problem</p><p><em>Note: sometimes, instead of directly use loss function, we would use a function of the loss function to construct objective function, for example, the boosting algorithm</em></p><p>For unsupervised learning, the loss function is a totally different thing, we usually directly talked about the objective function since we do not have a <span class="math inline">\(Y_{true}\)</span> to compare with. The specific form of the objective function varies from specific type of unsupervised learning</p><h2 id="empirical-risk-minimization">2. <strong>Empirical Risk Minimization</strong></h2><p>An obvious solution for ill-formed problem is to replace ture P(X,Y) with the observed <span class="math inline">\(\hat{P(X,Y)}\)</span> on training dataset.</p><p>Suppose the weight of all sample are equivalent, we define <span class="math display">\[R_{emp}(\theta) = \frac{1}{N}\sum_{n=1}^N L(y, f(x,\theta)\]</span> Where <span class="math inline">\(R_{emp}\)</span> is called empirical risk.</p><p>When we use empirical risk as our objective function, we call the learning principle of the machine learning "ERM"</p><p>For a probability model, under some condition, we can consider ERM as equivalent to a <strong>Maximum Likehood Estimation</strong>(MLE). Refer to another article about parameter estimation</p><h2 id="structural-risk-minimization">3. Structural Risk Minimization</h2><p>When sample size is big enough, empirical risk would be close enough to the real expected risk. However, in real problem we will not have infinite samples. We would probably obtain a subset of the sample with unmeasured varaible and noise. Such situation would often lead to overfitting. In such case, we need introduce regularization: <span class="math display">\[R_{srm}(\theta) = R_{emp} + \lambda J(\theta)\]</span> where <span class="math inline">\(J(\theta)\)</span> is a function represent the complexity of the model, and <span class="math inline">\(\lambda\)</span> is a penalized parameter used the control the degree of regularization</p><p>When we use structural risk as our objective function, we call the learning principle of the machine learning "SRM"</p><p>For a probability model, under some condition, we can consider SRM as equivalent to a <strong>Maximum-A-Posterior</strong>(MAP). Refer to another article about parameter estimation</p><h2 id="objective-function-for-unsupervised-learning">4. Objective function for Unsupervised Learning</h2><p>[ongoing]</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Learning Principle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic knowledge about Machine Learning(Supervised Learning)</title>
    <link href="/2022/09/11/basic_knowledgefor_ML/"/>
    <url>/2022/09/11/basic_knowledgefor_ML/</url>
    
    <content type="html"><![CDATA[<h1 id="machine-learning">Machine Learning</h1><h2 id="about-machine-learning">1.About Machine Learning</h2><h3 id="what-is-machine-learning">1.1 What is Machine Learning</h3><p>The definition about machine learning can be expressed as:</p><p>Suppose we want a model to perform a task T, and we have a approach E to evaluate the goodness the model complete this task. If we improve E through updating an optimization algorithm O using an observational dataset D, we can call such process a machine learning process</p><h3 id="three-components-of-machine-learning">1.2 Three components of Machine Learning</h3><p><strong>Model</strong></p><p>A model is a learnner trying to solve the task we want to perform. In most cases, that task would fitting a real mapping between variables. Thus, a model can usually be noted as <span class="math inline">\(f(x,\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the parameter of the model, and the fitting task is to find the best <span class="math inline">\(\theta\)</span> according to the training dataset.</p><p>As me may not know the sepcific form of the real mapping, we need to give the algorithm a scope for leaning, we call such scope the Hypothesis Space, noted as F. For all potential fitted mapping f , <span class="math inline">\(f\in F\)</span></p><p><strong>Learning Principle</strong></p><p>To find the best <span class="math inline">\(\theta\)</span>, we need a principle to judge how good a <span class="math inline">\(\theta\)</span> is. This principle usually involve the definition and calculation of a Loss function</p><p><strong>Optimization Algorithm</strong></p><p>After confirming the training dataset, hypothesis space and learning principle, the task of finding the best parameters become a optimization problem. An optimization algorithm is a solver to solve such problem</p><h3 id="proprocess-of-the-data">1.3 Proprocess of the data</h3><h3 id="evaluation-of-the-model">1.4 Evaluation of the model</h3><h2 id="supervised-machine-learning">2. Supervised Machine Learning</h2><p>Supervised learning is a kind of machine learning where the output variable of the model is clearly labeled in the dataset. That is to say, we would have a real outcome <span class="math inline">\(Y_{true}\)</span> and a predicted outcome <span class="math inline">\(Y_{pred}\)</span></p><h3 id="joint-probability-distribution"><strong>2.1 Joint Probability Distribution</strong></h3><p>Joint probability is the probability that multiple conditions are satisfied same time, noted as <span class="math inline">\(P(X,Y)\)</span></p><p>the relationship among joint probability, conditional probability and edge probability are: <span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\\P(Y) = \sum_{i=1}^N P(X,Y)P(X)\]</span> For two independent variable: <span class="math display">\[P(X, Y) = P(X)P(Y)\\P(Y|X) = P(Y)\]</span></p><p>The fundamental hypothesis of supervised machine learning is the existence of the Joint Probability Distribution of input variable X and output variable Y</p><p>The task we want a supervised machine learning algorithm to perform is to imitate the real JPD <span class="math inline">\(P(X,Y)\)</span>, so that we can calculate <span class="math inline">\(P(Y| X)\)</span> when input X is given</p><h3 id="category-of-supervised-machine-learning">2.2 Category of Supervised Machine Learning</h3><p><strong>Probability model v.s. Non-probability model</strong></p><p>The Non-Probability model try to learn the mapping relationship f directly. Usually it confirm a hypothesis space according to prior knowledge(e.g a linear space ). For example, KNN, SVM, NN are all non-probability model</p><p>The probability model try to directly learn one or more of <span class="math inline">\(P(X,Y)\)</span> , <span class="math inline">\(P( X|Y )\)</span> and <span class="math inline">\(P(Y)\)</span></p><p>Usually, it would pre-decide the distribution form of P(Y|X) or P(Y), for example, the logistic regression suppose P(Y| X) follow a Bernoulli Distribution. Logistic Regression, Naive Bayes are probability model</p><p><strong>Parameter model v.s. Non-parameter model</strong></p><p>In statistics, a parameter estimation means an estimation with given hypothesis on the distribution of the whole population, while a non-parameter estimation does not have such hypothesis</p><p>For machine learning:</p><p>A parameter model means you have an explicit hypothesis on the mapping or the probability distribution, like liner regression, logistic regression, naive bayes(limited dimention of <span class="math inline">\(\theta\)</span>) or MLP. The advantage of such kind of models is, if the hypothesis is correct, the model can be fit with very small dataset. However, if the hypothesis is incorrect, no matter how large the dataset is, there would still be inevitable bias</p><p>A non-parameter model means you put no or limited hypothesis on the mapping or distribution, like KNN, tree-based model, SVM(non-linear). The cost of storage and calculation of non-parameter model would be bigger, but theoretically, a non-parameter can fit any complicated mapping as long as we have enough data. Usually a non-parameter model has a few hyperparameters and infinite parameters</p><p><strong>Discriminative model v.s. Generative model </strong></p><p>A discriminative model directly model on Y:</p><ul><li>All non-probability models are discriminative models</li><li>If a probability models try to directly learn <span class="math inline">\(P(Y|X)\)</span>, it is a discriminative model</li></ul><p>including most machine learning model like MLP, logistic regression, decision tree, KNN and SVM</p><p>A Generative model try to induce <span class="math inline">\(P( X| Y)\)</span> through learning P(X, Y) and P(Y), with <span class="math display">\[P(X,Y) = P(X| Y )P(Y)\]</span></p><p><span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\]</span> including naive bayes，GMM</p><h2 id="unsupervised-machine-learning">3. Unsupervised Machine Learning</h2><p>Unsupervised learning is a kind of machine learning where the output variable of the model is not labeled in the dataset. Typically we can separate unsupervised machine learning algorithm into serval types according to the task we want it to perform</p><h3 id="feature-learning">3.1 Feature learning</h3><p>Mining useful expression or combination of features in unlabeled dataset. Usually applied in dimensionality reduction or visualization, including:</p><ul><li>PCA , T- SNE, SVD</li><li>Sparse Encoding, Auto-encoder ,Denoising Autoencoder</li></ul><h3 id="probabilistic-density-estimation">3.2 Probabilistic Density Estimation</h3><p>Induce the probability density function of a variable through observational data, can be classified as:</p><ul><li>Parametric Density Estimation: have prior hypothesis on the distribution form of the variable,including MLE, MAP etc.</li><li>Non-parametric Density Estimation: do not have a prior hypothesi, including histogram, Kernel Density Estimation(KDE) etc.</li></ul><h3 id="clustering">3.3 Clustering</h3><p>Segment unlabeled dataset into different groups. Including K-Means, DBscan, hierarchical clustering etc.</p><h3 id="other-unsupervised-learning">3.4 Other Unsupervised Learning</h3><p>There are lots of emerging unsupervised learning algorithm like pred-Net, GAN etc. These algorithm would be elaborated in single section in other articles</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hypothesis Testing for AB Testing</title>
    <link href="/2022/03/25/Hypothesis-Testing/"/>
    <url>/2022/03/25/Hypothesis-Testing/</url>
    
    <content type="html"><![CDATA[<h1 id="hypothesis-testing">Hypothesis Testing</h1><h2 id="terms-and-definition">1. Terms and Definition</h2><h3 id="terms-in-hypothesis-testing">1.1 Terms in Hypothesis Testing</h3><table><thead><tr class="header"><th style="text-align: center;">Term</th><th style="text-align: center;">Meaning</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Hypothesis</td><td style="text-align: center;">A claim to test</td></tr><tr class="even"><td style="text-align: center;">Null Hypothesis (<span class="math inline">\(H_0\)</span>)</td><td style="text-align: center;">Currently accepted value for a parameter(e.g diff = 0)</td></tr><tr class="odd"><td style="text-align: center;">Alternative Hypothesis(<span class="math inline">\(H_a\)</span>)</td><td style="text-align: center;">The claims to be tested. <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span> are mathematically opposites</td></tr><tr class="even"><td style="text-align: center;">Test Outcomes</td><td style="text-align: center;">Reject <span class="math inline">\(H_0\)</span> or fail to reject <span class="math inline">\(H_0\)</span></td></tr><tr class="odd"><td style="text-align: center;">Test Statistics</td><td style="text-align: center;">Statistics calculated from the data samples used to decide whether to reject <span class="math inline">\(H_0\)</span></td></tr><tr class="even"><td style="text-align: center;">Big/Small Sample</td><td style="text-align: center;">sample size -&gt; inf / sample size is fixed, usually set threshold to 30</td></tr><tr class="odd"><td style="text-align: center;">Central Limit Theorem</td><td style="text-align: center;">A theorem that indicates no matter what distribution the total population is, when sample size n is big enough, the average of a statistics of a sample <span class="math inline">\(\bar{X}\)</span> follows a normal distribution N(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\frac{\sigma^2}{n}\)</span>). This theorem allows implementation of T- test on average of continuous test statistics, like average salary of a department</td></tr><tr class="even"><td style="text-align: center;">Degree of Freedom</td><td style="text-align: center;">The number of samples(n) -1</td></tr><tr class="odd"><td style="text-align: center;">Effect Size</td><td style="text-align: center;">The degree the test statistics differ from the accepted value</td></tr><tr class="even"><td style="text-align: center;">Significance Level(<span class="math inline">\(\alpha\)</span>)</td><td style="text-align: center;">A decided threshold of <span class="math inline">\(\alpha\)</span>. <span class="math inline">\(\alpha\)</span> is the probability of rejecting <span class="math inline">\(H_ 0\)</span> when <span class="math inline">\(H_ 0\)</span> is ture(Type I Error)</td></tr><tr class="odd"><td style="text-align: center;">Confidence Level</td><td style="text-align: center;">How confident we are to reject <span class="math inline">\(H_0\)</span> (1-<span class="math inline">\(\alpha\)</span>)</td></tr><tr class="even"><td style="text-align: center;">p value</td><td style="text-align: center;">The probability that the observed statistics significance are caused by random factor</td></tr><tr class="odd"><td style="text-align: center;">statistical power(<span class="math inline">\(1-\beta\)</span>)</td><td style="text-align: center;"><span class="math inline">\(\beta\)</span> is the the probability of accepting <span class="math inline">\(H_ 0\)</span> when <span class="math inline">\(H_ 0\)</span> is False(Type II Error). Typically we need the power of a test to be greater than 80%</td></tr><tr class="even"><td style="text-align: center;">Confidence Interval</td><td style="text-align: center;">With <span class="math inline">\(\alpha\)</span> as significance level, <span class="math inline">\(\theta\)</span> as test statistics, if <span class="math inline">\(P\{ \theta_n &lt; \theta &lt; \theta_m \} \ge 1-\alpha\)</span>, then we call <span class="math inline">\((\theta_n , \theta_m)\)</span> the confidence interval of <span class="math inline">\(\theta\)</span> uhder the significance level <span class="math inline">\(1-\alpha\)</span></td></tr><tr class="odd"><td style="text-align: center;">One-tail Test/Two-tail Test</td><td style="text-align: center;">In one-tail test, the <span class="math inline">\(H_a\)</span> has direction, so <span class="math inline">\(H_a\)</span> would be like <span class="math inline">\(\mu &gt; x\)</span>, and we only focus on one side of the rejection area in this case. While in two tail test, <span class="math inline">\(H_a\)</span> would be like <span class="math inline">\(\mu \ne x\)</span></td></tr><tr class="even"><td style="text-align: center;">Rejection Area</td><td style="text-align: center;">Let the the area of PDF on <span class="math inline">\((-inf,z_1],[z_2,inf)] = \alpha\)</span>, then this area is called rejection area, <span class="math inline">\(b_0,b_1\)</span> are called rejection boundaries, which are the z-score when the area one the left/right side is <span class="math inline">\(\frac{\alpha}{2}\)</span>. When the test statistics fall outside the rejection boundaries, we can reject <span class="math inline">\(H_0\)</span> under the Level of Significance</td></tr><tr class="odd"><td style="text-align: center;">MDE</td><td style="text-align: center;">The minimum detectable effect size of a experiment when <span class="math inline">\(1-\alpha\)</span> and <span class="math inline">\(\beta\)</span> is given. If detected effect size <span class="math inline">\(d &lt; mde\)</span>, it might be caused by random factor, and we cannot reject <span class="math inline">\(H_ 0\)</span></td></tr></tbody></table><p><img src="/2022/03/25/Hypothesis-Testing/img1.png"></p><p><strong>Power and Error</strong></p><p><img src="/2022/03/25/Hypothesis-Testing/img3.PNG"></p><h3 id="formula">1.2 Formula</h3><p>Calculation of effect size:</p><ol type="1"><li><p><strong>Cohen's d:</strong> describe the difference of average variables of two group <span class="math display">\[d = \frac{\mu_A - \mu_B }{\sigma_{pooled}}\\\sigma_{pooled} = \sqrt{\frac{(n_1-1)s_ 1^2+(n_2-1)s_ 2^2}{(n_ 1-1)+(n_ 2-1)}}\]</span></p></li><li><p><strong>Cramer's V</strong>: describe the correction of categorical variable <span class="math display">\[V = \sqrt{\frac{\chi^2/n}{min(c- 1,r-1)}}\]</span></p></li><li><p>Cohen's h: decribe the differnce of ratio variable of two groups <span class="math display">\[h = 2(arcsin \sqrt{p1} - arcsin\sqrt{p2})\]</span></p></li></ol><p>Caluclation of MDE <span class="math display">\[MDE = (t_{\frac{1-\alpha}{2}} + t_ {1-\beta})\sqrt{\frac{\sigma_1^2}{n_ 1} + \frac{\sigma_2^2}{n_ 2}}\]</span></p><h2 id="type-of-hypothesis-testing">2. Type of Hypothesis Testing</h2><p>Ther are various type of hypothesis testing fitting different situations. Fot this article, we mainly discuss about four type of tests frequently used in A/B testing</p><h3 id="four-common-types-of-experiment">2.1 Four Common Types of Experiment</h3><p><strong>Z-test</strong>:</p><ol type="1"><li>Test statistics should be average value of a group or ratio</li><li>The variance of the population should be given</li><li>If the population does not follow Normal distribution, the sample size should be big</li></ol><p>In most application, the variance of the population is unknown. In addition,we usually cannot estimate the distribution of the total population, which requires the sample size of a Z-test to be big. Thus, z-test is not frequently used comparing to t-test</p><p><strong>T- test</strong></p><p>T- test is used to examine</p><ol type="1"><li>whether the accepted average or ratio of a population is correct(One sample)</li><li>Whether there's a difference on a statistics between two group(Two sample)</li></ol><p>The following condidtions should be guaranteed:</p><ol type="1"><li>Test statistics sould be average value or ratio</li><li>The variance of the population is not needed</li><li>If the population does not follow Normal distribution, the sample size should be big</li><li>The variance of the populations should be same</li></ol><p><strong>F-test</strong>:</p><p>F-test is used to examine</p><ol type="1"><li>whether the standard deviation of two or more population are same</li><li>whether a categorical variable has an effect on the average of a population</li></ol><p>The following condidtions should be guaranteed:</p><ol type="1"><li>The two population should both follows normal distribution</li><li>The variance of the populations should be same</li></ol><p><strong>chi-test</strong></p><p>chi-squared test is used to examine:</p><ol type="1"><li>Whether the observed frequency of a variable in the sample is aligned with the expected frequency</li><li>Whether two categorical variables are correlated</li></ol><p>The following condidtions should be guaranteed:</p><ol type="1"><li>Test statistics should be frequency or categorical variable</li><li>Non-parameter testing. The population do not need to follow normal distribution</li></ol><h3 id="how-to-choose-experiment-type">2.2 How to choose Experiment Type</h3><p><img src="/2022/03/25/Hypothesis-Testing/img5.png" style="zoom:150%;"></p><p>where:</p><ul><li><span class="math inline">\(\bar{x}\)</span> is the mean of the sample</li><li><span class="math inline">\(\mu_0\)</span> is the mean of population in <span class="math inline">\(H_0\)</span></li><li>s is the standard deviation of sample</li><li><span class="math inline">\(\sigma\)</span> is the standard deviation of population</li><li>n is the number of samples</li></ul><h2 id="zt-test">3. Z/T-test</h2><h3 id="one-sample-test">3.1 One Sample Test</h3><p>Objective: to test whether the accepted mean or ratio of a population is correct through a sample. In this case, we can assume <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\pi_0\)</span> is given</p><h4 id="sigma-givent-or-z">3.1.1 <span class="math inline">\(\sigma\)</span> Given(T or Z)</h4><p>In most application, the standard deviation of the whole population is unknown. But suppose we have the <span class="math inline">\(\sigma\)</span></p><p><strong>Big Sample</strong></p><p>For big sample, we conduct z-test and give hypothesis as:</p><p><span class="math inline">\(H_0: \mu = \mu_ 0\)</span> or <span class="math inline">\(\pi = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu \ne \mu_ 0\)</span> or <span class="math inline">\(\pi \ne \pi_0\)</span> for ratio test</p><p>where <span class="math inline">\(\mu_0\)</span> is the accepted value of the parameter and construct z-score as: <span class="math display">\[z = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}\]</span> for ratio test, construct z as: <span class="math display">\[z = \frac{p-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\]</span> reject the null hypothesis when:</p><ul><li><span class="math inline">\(|z| &gt; z_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|z| &gt; mde\)</span></li></ul><p><strong>Small Sample</strong></p><p>When the sample size is small, we can assume the sample ~ t distribution. The construction of the test statistics and reject condition are the same, the only difference is that the test statistics is now following t distribution</p><h4 id="sigma-unknownt">3.1.2 <span class="math inline">\(\sigma\)</span> Unknown(T)</h4><p>When <span class="math inline">\(\sigma\)</span> is unknown, the basic strategy is to replace population std with sample std, and apply t-test</p><p>We can conduct t-test and give hypothesis as:</p><p><span class="math inline">\(H_0: \mu = \mu_ 0\)</span> or <span class="math inline">\(\pi = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu \ne \mu_ 0\)</span> or <span class="math inline">\(\pi \ne \pi_0\)</span> for ratio test <span class="math display">\[t = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}\]</span> for ratio test, construct t as: <span class="math display">\[t = \frac{p-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\]</span></p><p>reject the null hypothesis when:</p><ul><li><span class="math inline">\(|t| &gt; t_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|t| &gt; mde\)</span></li></ul><h3 id="two-sample-test">3.2 Two Sample Test</h3><p>Objective: to test whether a condition would effect a metric through following one group before and after experiment or comparing two groups</p><h4 id="match-test">3.2.1 Match Test</h4><p>Usually applied when testing treatment on a same group of people in different time(e. g medical treatment). In this context, we can assum the sample size and standard deviation remain as same: <span class="math inline">\(n_ 1 = n_ 2, \sigma_1 = \sigma_2\)</span></p><h5 id="sigma-givenz">3.2.1.1 <span class="math inline">\(\sigma\)</span> Given(Z)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test <span class="math display">\[z = \frac{(\bar{x_1}-\bar{x_0}) - (\mu_1-\mu_0)}{\sigma\sqrt{\frac{2}{n}}} = \frac{\bar{d}}{\sigma\sqrt{\frac{2}{n}}}\]</span> for ratio test, construct z as: <span class="math display">\[z = \frac{(p_1-p_0) - (\pi_1 - \pi_0)}{\sqrt{p(1-p)(\frac{2}{n})}}\]</span></p><p><span class="math display">\[p = \frac{p_0+p_1}{2}\]</span></p><p>reject the null hypothesis when:</p><ul><li><span class="math inline">\(|z| &gt; z_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|z| &gt; mde\)</span></li></ul><h5 id="sigma-unkownt">3.2.1.2 <span class="math inline">\(\sigma\)</span> Unkown(T)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test <span class="math display">\[t = \frac{\sqrt{n}((\bar{x_1}-\bar{x_0}) - (\mu_1-\mu_0))}{s_d} = \frac{\sqrt{n}\bar{d}}{s_d}\]</span> where <span class="math inline">\(s_d\)</span> is the standard deviation of d</p><p>for ratio test, construct z as: <span class="math display">\[t = \frac{(p_1-p_0) - (\pi_1 - \pi_0)}{\sqrt{p(1-p)(\frac{2}{n})}}\]</span></p><p><span class="math display">\[p = \frac{p_0+p_1}{2}\]</span></p><p>Reject when:</p><ul><li><span class="math inline">\(|t| &gt; t_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|t| &gt; mde\)</span></li></ul><h4 id="independent-test">3.2.2 Independent Test</h4><p>Usually applied when evaluation the effect of a treatment by comparing a experiment group and control group, which is A/B Testing</p><h5 id="sigma-givenz-1">3.2.2.1 <span class="math inline">\(\sigma\)</span> Given(Z)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test</p><p>if <span class="math inline">\(\sigma_1 = \sigma_2\)</span> <span class="math display">\[z = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{\sigma \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\]</span></p><p>if <span class="math inline">\(\sigma_1 \ne \sigma_2\)</span> <span class="math display">\[z  = \frac{(\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}\]</span> for ratio test: <span class="math display">\[z = \frac{(p_1-p_2) - (\pi_1 - \pi_2)}{\sqrt{p(1-p)(\frac{1}{n_1}+\frac{1}{n_2})}}\]</span></p><p><span class="math display">\[p = \frac{p1*n1+p_2*n_2}{n_1+n_2}\]</span></p><p>reject the null hypothesis when:</p><ul><li><span class="math inline">\(|z| &gt; z_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|z| &gt; mde\)</span></li></ul><h5 id="sigma-uknownt">3.2.2.1 <span class="math inline">\(\sigma\)</span> Uknown(T)</h5><p><span class="math inline">\(H_0: \mu_1 = \mu_ 0\)</span> or <span class="math inline">\(\pi_1 = \pi_0\)</span> for ratio test</p><p><span class="math inline">\(H_1: \mu_1 \ne \mu_ 0\)</span> or <span class="math inline">\(\pi_1 \ne \pi_0\)</span> for ratio test</p><p>if <span class="math inline">\(\sigma_1 = \sigma_2\)</span> <span class="math display">\[t = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\]</span></p><p><span class="math display">\[s_p = \sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}}\]</span></p><p>if <span class="math inline">\(\sigma_1 \ne \sigma_2\)</span> <span class="math display">\[t = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}\]</span> for ratio test: <span class="math display">\[t = \frac{(p_1-p_2) - (\pi_1 - \pi_2)}{\sqrt{p(1-p)(\frac{1}{n_1}+\frac{1}{n_2})}}\]</span></p><p><span class="math display">\[p = \frac{p1*n1+p_2*n_2}{n_1+n_2}\]</span></p><p>reject the null hypothesis when:</p><ul><li><span class="math inline">\(|t| &gt; t_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li>|t| &gt; mde</li></ul><h3 id="sample-size-and-duration">3.3 Sample size and Duration</h3><h4 id="sample-size">3.3.1 Sample Size</h4><p>In A/B testing, most of time, you can calculate the sample size N using the following equation: <span class="math display">\[N = \frac{(t_{(1-\alpha)/2} + t_{1-\beta})^2\sigma^2}{mde^2}\]</span> Where:</p><ul><li><p><span class="math inline">\(\sigma\)</span> is the standard error of the two sample</p></li><li><p>N is the sample size of <strong>one sample</strong>, the total sample size is 2N</p></li><li><p>The greater the mde is(which means the sensitivity of a experiment is lower), the smaller sample size is need</p></li><li><p>Note: the mde in the formular is the expected mde to calculated the minimum sample size. The actual mde of a experiment should be calculated after the experiment is launched by the following formula: <span class="math display">\[MDE = (t_{(1-\alpha)/2} + t_{1-\beta})\sqrt{\frac{\sigma_ 1}{n_ 1}+\frac{\sigma_ 2}{n_ 2}}\]</span></p></li></ul><p><a href="https://www.evanmiller.org/ab-testing/">here</a> are some useful tools for calculate the sample size A/B</p><h4 id="duration">3.3.2 Duration</h4><p>You can calculate the duration by calculating sample size/daily processed samples. For example, If the sample size of a web A/B test is 1000, and the daily view of the webpage is 500, than you can run it foe two days. However, in real world problems, this calculation is not reliable, as there might be other factors like holiday or events that can affect the experiment. Thus, the specific process to decide the duration of a test remain to be discussed by experts.</p><h2 id="f-test">4. F-test</h2><h3 id="equality-of-variances">4.1 Equality of Variances</h3><p>Through F-test, we can examine the equality of variance</p><p>Let <span class="math inline">\(X_1,X- 2\)</span> be two independent variables, where: <span class="math display">\[X_1~N(\mu_1,\sigma_1^2),  X_2~N(\mu_2,\sigma_2^2)\]</span> Get a sample from each variable: <span class="math inline">\(x_1, x_2\)</span>, the sample sizes are <span class="math inline">\(n_1,n_2\)</span></p><p>Let <span class="math inline">\(\bar{x}_1,\bar{x}_2\)</span> be the mean of the samples, <span class="math inline">\(s_1,s_2\)</span> be the standard error of the samples</p><p>Set the null hypothesis and alternative hypothesis of Experiment: <span class="math display">\[H_ 0: \sigma_1^2 = \sigma_2^2\\H_ 1: \sigma_1^2 \neq \sigma_2^2\]</span> If it is a one-tail experiment: <span class="math display">\[H_ 0: \sigma_1^2 &lt; \sigma_2^2\\H_ 1: \sigma_1^2 \ge \sigma_2^2\]</span></p><p>Construct F statistics: <span class="math display">\[F(n_1-1,n_2-1) = \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}=\frac{s_ 1^2}{s_2^2}\]</span> Note: As a convention, we normally select the greater s as <span class="math inline">\(s_ 2\)</span> to let f score &lt;1</p><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span></p><p>If it is a one-tail experiment, then reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(F &lt; F_{1-\alpha}\)</span></p><h3 id="single-factor-anova">4.2 Single Factor ANOVA</h3><p>We can use f- test to examine the impact of a factor to an indicator by judging if the indicator is same when the factor is set to different value</p><p>Let the factor be Y, indicator be x</p><p>Suppose we got the following observations:</p><table><thead><tr class="header"><th>Y = Y1</th><th>Y=y2</th><th>y=y3</th><th>y=y4</th></tr></thead><tbody><tr class="odd"><td>x=1</td><td>x=2</td><td>...</td><td>...</td></tr><tr class="even"><td>x=3</td><td>x=3</td><td>...</td><td>...</td></tr><tr class="odd"><td>x=2</td><td>x=4</td><td>...</td><td>...</td></tr><tr class="even"><td>x=6</td><td>x=6</td><td>...</td><td>...</td></tr></tbody></table><p>Let k = number of groups(number of different values of Y)</p><p>Let <span class="math inline">\(n_i\)</span> = the number of samples in <span class="math inline">\(i_{th}\)</span> group</p><p>Let n = <span class="math inline">\(max(n_ k)\)</span></p><p>Construct <span class="math display">\[F = \frac{SSA/df1}{SSE/df2}\]</span> where:</p><ul><li><p>df1 = k - 1</p></li><li><p>df2 = n- k</p></li><li><p>SSA is Sum of Square Between Groups:</p><p><span class="math display">\[SSA = \sum_{i=1}^kn_i(\bar{x_i}-\bar{x})\]</span> Where:</p><ul><li><span class="math inline">\(\bar{x_i}\)</span> is the average of the <span class="math inline">\(i_{th}\)</span> group</li><li><span class="math inline">\(\bar{x}\)</span> is the average of all <span class="math inline">\(\bar{x_i}\)</span></li></ul></li><li><p>SSE is the Sum of square error <span class="math display">\[SSE = \sum_{i=1}^k(n_i-1)s_i^2\]</span> Where:</p><ul><li><span class="math inline">\(s_1^2\)</span> is the variance(square of standard error) of the <span class="math inline">\(i_{th}\)</span> group</li></ul></li></ul><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span>(the factor do has an impact)</p><h3 id="exam-on-the-significance-of-the-linear-regression">4.3 Exam on the significance of the Linear Regression</h3><p>We can use f-test to examine on whether a liner model(linear hypothesis) fit a problem well</p><p>Suppose we got the following linear hypothesis function: <span class="math display">\[y = \beta_1x_1 + \beta_2x_ 2 + \beta_ 0\]</span> Define SSR as: <span class="math display">\[SSR = ||\hat{y}-\bar{y}1_n||^2\]</span> Where:</p><ul><li><span class="math inline">\(\hat{y}\)</span> is the prediction of y</li><li>{ y } is the average of true y</li><li>1n is a vector with all one</li><li></li></ul><p>Define SSE as: <span class="math display">\[SSR = ||y- \hat{y}||^2\]</span> Construct F statistics as: <span class="math display">\[F = \frac{SSR/p}{SSE/(n-p-1)}\]</span> where:</p><ul><li><p>n is number of samples</p></li><li><p>p is number of variables(x)</p></li></ul><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span>. In such case, the linear model is significant to the change of the x and y, thus the linear hypothesis is acceptable. Otherwise, consider a non-linear model</p><h2 id="chi-squared-test">5. Chi-squared Test</h2><h4 id="chi-squared-test-for-independence">5.1 Chi-squared Test for Independence</h4><p>chi-squared test is a <strong>supervised</strong> hypothesis testing method to calculate the probability that 2 categorical variables are correlated</p><p>Suppose there are categorical variables X and Y, X has r possible values, with probability <span class="math inline">\((p_{x,1},...p_{x,r})\)</span> and Y has c possible values, with probability <span class="math inline">\((p_{y,1},...p_{y,c})\)</span>.</p><p>Under the null hypothesis, since X and Y are independent, the probability of observing <span class="math inline">\(X=c_{x,i}, Y = c_{y,j}\)</span> would be <span class="math inline">\(p_{i,j} = p_{x,i}p_{y,j}\)</span></p><p>In such case, let <span class="math inline">\(t_{i,j}\)</span> be the observed times that <span class="math inline">\(X=c_{x,i}, Y = c_{y,j}\)</span>, t would follow a binomial distribution <span class="math inline">\(Bin(t;n,p_{ij})\)</span></p><p>According to properties of binomial distribution, when n is big enough, <span class="math inline">\(Bin(t;n,p_{i,j})\)</span> is approximately <span class="math inline">\(N(np_{i,j},np_{i,j}(1-p_{i,j}))\)</span></p><p>According to the definition of <span class="math inline">\(\chi^2\)</span> distribution, let <span class="math display">\[\chi^2=\sum_i^r\sum_j^c \frac{(t_{i,j}-E[t_{i,j})^2]}{E[t_{i,j}]}\]</span> <span class="math inline">\(\chi^2\)</span> would follow a <span class="math inline">\(\chi^2\)</span> distribution, with the degree of freedom being <span class="math inline">\((r-1)(c-1)\)</span></p><p>We can calculate the <span class="math inline">\(\chi^2\)</span> and obtain its according p value through p value table for <span class="math inline">\(\chi^2\)</span> distribution. If p &lt; 0.05, we can reject the null hypothesis that the two variables is independent</p><p>Note that in a <span class="math inline">\(\chi^2\)</span> testm the probability <span class="math inline">\((p_{x,1},...p_{x,r})\)</span> and <span class="math inline">\((p_{y,1},...p_{y,c})\)</span> is not a hypothesis to test, it is a believed fact. In real application, it needs to be estimated from observations. When number of observations is big enough, the error of estimation can be ignored.</p><p>A example is given below:</p><ol type="1"><li><p>According to the observations, make the frequency table</p></li><li><p>calculate the row total and the column total</p></li><li><p>calculate the expectations for each cell by <span class="math inline">\(\frac{R_i*C_j}{total}\)</span> (<span class="math inline">\(\frac{125*310}{600} = 65\)</span>,etc.)</p></li><li><p>the <span class="math inline">\(\chi^2\)</span> would be <span class="math display">\[\chi_{i,j}^2 = \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\\chi^2 = \sum_{i,j}^{r,c}\chi_{i,j}^2\]</span></p><table><thead><tr class="header"><th>movie type</th><th>High POPULARITY</th><th>low POPULARITY</th><th>Row total</th></tr></thead><tbody><tr class="odd"><td>type1</td><td>50(65)</td><td>75(60)</td><td>125</td></tr><tr class="even"><td>type2</td><td>125(155)</td><td>175(145)</td><td>300</td></tr><tr class="odd"><td>type3</td><td>...</td><td>...</td><td>...</td></tr><tr class="even"><td>type4</td><td>...</td><td>...</td><td>...</td></tr><tr class="odd"><td>column total</td><td>310</td><td>290</td><td>600</td></tr></tbody></table></li><li><p>calculate the degree of freedom, <span class="math inline">\(k=(r-1)(c-1)\)</span></p></li><li><p>check the significance table of chi distribution to find the threshold of <span class="math inline">\(\chi^2\)</span> given k and <span class="math inline">\(\alpha\)</span></p></li><li><p>compare the <span class="math inline">\(\chi^2\)</span> with the rejection area, if <span class="math inline">\(\chi^2\)</span> is greater than the threshold(which means p value is smaller than <span class="math inline">\(\alpha\)</span>), then reject the null hypothesis</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest, chi2<br>X, y = data_0,target_0<br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><h3 id="chi-squared-test-for-goodness-of-fit">5.2 Chi-squared Test for Goodness of fit</h3><p>The process is basically the same.</p><table><thead><tr class="header"><th>Category</th><th>Predicted times</th><th>Observed time</th></tr></thead><tbody><tr class="odd"><td>C1</td><td>...</td><td></td></tr><tr class="even"><td>C2</td><td></td><td></td></tr><tr class="odd"><td>C3</td><td></td><td></td></tr></tbody></table><p>Construct the <span class="math inline">\(\chi^2\)</span> as: <span class="math display">\[\chi^2 = \sum_i^k\frac{O_i-P_i}{P_i}\]</span> the <span class="math inline">\(\chi^2\)</span> follows a <span class="math inline">\(\chi^2(k-1)\)</span> when number of observations is big enough</p>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hypothesis Testing</tag>
      
      <tag>A/B Test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Feature Selection: Statistical Method</title>
    <link href="/2022/02/09/feature-selection-stats-method/"/>
    <url>/2022/02/09/feature-selection-stats-method/</url>
    
    <content type="html"><![CDATA[<h1 id="feature-selection">Feature Selection</h1><h2 id="filter">1. Filter</h2><p>Evaluate features on divergence and correlation. set one or more thresholds and select the feature.</p><p><strong>Pro</strong>: fast, scalable, independent of the model</p><p><strong>Con</strong>: ignore the dependence of features</p><h3 id="varianceskewness">1.1 Variance/Skewness</h3><p>For most models, variables with high variance or skewness would be weighted more and deemed as more important. Thus, we can</p><ol type="1"><li>calculate the variance(not STD) of each feature</li><li>set a threshold, select all features whose variance bigger than the threshold</li></ol><p>Calculation of Skewness: <span class="math display">\[SK = \frac{n\sum(x_i-\bar{x})^3}{(n-1)(n-2)\sigma^3}\]</span> An implementation of feature selection based on variance in Python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold<br><span class="hljs-comment">#threshold 是方差的阈值</span><br><span class="hljs-comment">#返回选择后的特征</span><br>data_1 = VarianceThreshold(threshold=<span class="hljs-number">0.25</span>).fit_transform(data_0)<br></code></pre></td></tr></table></figure><h3 id="correlation">1.2 Correlation</h3><ol type="1"><li>calculate the correlation value between each feature and the target</li><li>select the features with top K biggest correlation value</li></ol><h4 id="pearson-r">1.2.1 Pearson R</h4><p>Pearson R calculates correlation based on covariance. <span class="math display">\[p_{X,Y} = \frac{cov(X,Y)}{\sigma_X \sigma_Y} = \frac{E[(X - \mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y} = \frac{E[XY]-E[X]E[Y]}{\sqrt{(E[X^2]-E[X])^2} \sqrt{(E[Y^2]-E[Y])^2}}\]</span> For a sample with n samples: <span class="math display">\[r = \frac{\sum_i^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sqrt{\sum_i^n(X_i-\bar{X})^2} \sqrt{\sum_i^n(Y_i-\bar{Y})^2}} = \frac{\sum XY - \frac{\sum X \sum Y}{n}}{\sqrt{\sum X^2 - \frac{(\sum X)^2 }{n}} \sqrt{\sum Y^2 - \frac{(\sum Y)^2 }{n}}}\]</span> The Pearson R has following properties:</p><ul><li>The range of Pearson R is [-1,1], positive numbers indicate positive correlation</li><li>The Pearson R represents the linear correlation between two variables, linear transformation of X or Y does not change Pearson R</li><li>X and Y must be numerical variables and follow normal distribution</li><li>The observations of X and Y are in pairs</li></ul><h4 id="spearman">1.2.2 Spearman</h4><p>Suppose we have samples of X and Y with n observations. Sort the observations X and Y to obtain two new set <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, where <span class="math inline">\(a_i,b_i\)</span> is the rank of <span class="math inline">\(X_i,Y_i\)</span> in X,Y</p><p>define Spearman correlation as: <span class="math display">\[\rho = \frac{6\sum_i^n(a_i-b_i)^2}{n(n^2-1)}\]</span> The Spearman correlation has following properties:</p><ul><li>The range of Spearman correlations is [-1,1], positive numbers indicate positive correlation</li><li>The Spearman correlation represents rank correlation (simply based on size relationship)</li><li>X and Y does not need to follow certain distribution. Aithough the sample must be the same, they do not need to be in pairs<br></li><li>The statistical power of Spearman is relatively lower</li></ul><p><img src="/2022/02/09/feature-selection-stats-method/1.png"></p><h4 id="kendall">1.2.3 Kendall</h4><p>Suppose we combine variables X and Y into a new element <span class="math inline">\((X,Y)\)</span>. If two elements <span class="math inline">\((X_i, Y_i)\)</span> and <span class="math inline">\(X_j,Y_j\)</span> satisfy either of these two case:</p><ul><li><span class="math inline">\(X_i &gt; X_j\)</span> and <span class="math inline">\(Y_i &gt;Y_j\)</span></li><li><span class="math inline">\(X_i &lt; X_j\)</span> and <span class="math inline">\(Y_i &lt;Y_j\)</span></li></ul><p>Then we call these two elements have consistency</p><p>if <span class="math inline">\(X_i = X_j\)</span> and <span class="math inline">\(Y_i = Y_j\)</span>, we regard these two elements as having neither consistency nor inconsistency. Otherwise, we call these two elements have inconsistency</p><p>define Kendall correlation as: <span class="math display">\[\tau = \frac{C-D}{\sqrt{N3-N1}\sqrt{N_3-N_2}{}}\]</span> where:</p><ul><li><p>C is number of paris of elements(Two element <span class="math inline">\((X _1,Y_1),(X_2,Y_ 2)\)</span> is one pair) that have consistency</p></li><li><p>D is number of paris of elements that have inconsistency</p></li><li><p><span class="math inline">\(N1 = \sum_i^s \frac{ 1}{2}U_i(U_i-1)\)</span>, where:</p><ul><li>s is the number of values in X that appears more than once</li><li><span class="math inline">\(U_i\)</span> is the number those values appears(For <span class="math inline">\(i^{th}\)</span> Values in s)</li><li>For example, for a X={1,2,2,3,3,3,4}, s=2, <span class="math inline">\(U_1 = 2, U_2 = 3\)</span></li></ul></li><li><p>N2 is calculated same way as N1 on Y</p></li><li><p><span class="math inline">\(N3 = \frac{1}{2}N(N- 1)\)</span>, where is the number of samples</p></li></ul><p>The Kendall correlations have similar conditions as Pearson R. The only difference is it represents rank correlation instead of linear correlation</p><ul><li>The range of Pearson R is [-1,1], positive numbers indicate positive correlation</li><li>X and Y must be numerical variables and follow normal distribution</li><li>The observations of X and Y are in pairs</li></ul><p><strong>An implementation of feature selection based on Pearson R in Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> pearsonr<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_pscore</span>(<span class="hljs-params">x,y</span>):</span><br>    k = np.zeros([<span class="hljs-number">4</span>,<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,x.shape[<span class="hljs-number">1</span>]):<br>        temp = pearsonr(x[:,i],y[:,<span class="hljs-number">0</span>])<br>        k[i,:] = np.array(temp)<br>    <span class="hljs-keyword">return</span> [k.T[<span class="hljs-number">0</span>],k.T[<span class="hljs-number">1</span>]]<br><span class="hljs-comment"># 下面的写法有问题理论上应该与上面的函数等价，实际上在调用np.array时，会把最内层的pearsonr系数值变成一个数组对象，导致SelectKBest无法</span><br><span class="hljs-comment"># 遍历，需要用astype把第一列转化为float，但这样很难封装进lambda函数里，不如不用，写这个的人铁nt，害我研究一个多小时</span><br><span class="hljs-comment"># get_all_pscore =  lambda X,Y:np.array(list(map(lambda x:pearsonr(x,Y),X.T))).T</span><br><br><span class="hljs-comment">#第一个参数为一个callable()，该函数以feature，target为输入，输出可以是两种：</span><br><span class="hljs-comment"># 1. 一对数组pearsonr系数-p值（可以是tuple中两个数组，也可以是ndarray，总之第一个维度须为2</span><br><span class="hljs-comment"># 2. 一个单一的数组（只有相关系数）</span><br><span class="hljs-comment"># 第二个参数k，代表选择前k个</span><br>data_1 = SelectKBest(get_all_pscore(),k=<span class="hljs-number">2</span>).fit_transform(data_0,target_0)<br></code></pre></td></tr></table></figure><h3 id="hypothesis-testing">1.3 Hypothesis Testing</h3><h4 id="chi-squared-test">1.3.1 Chi-Squared Test</h4><p>chi-squared test is a <strong>supervised</strong> hypothesis testing method to calculate the probability that 2 variables are correlated</p><ol type="1"><li><p>According to the observations, make the frequency table</p></li><li><p>calculate the row total and the column total</p></li><li><p>calculate the expectations for each cell by <span class="math inline">\(\frac{R_i*C_j}{total}\)</span> (<span class="math inline">\(\frac{125*310}{600} = 65\)</span>,etc.)</p></li><li><p>the chi2 of each cell and the total chi2 are given by <span class="math display">\[\chi_{i,j}^2 = \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\\chi^2 = \sum_{i,j}^{r,c}\chi_{i,j}^2\]</span></p></li><li><p>calculate the degree of freedom, <span class="math inline">\(k=(r-1)(c-1)\)</span></p><table><thead><tr class="header"><th>movie type</th><th>High POPULARITY</th><th>low POPULARITY</th><th>Row total</th></tr></thead><tbody><tr class="odd"><td>type1</td><td>50(65)</td><td>75(60)</td><td>125</td></tr><tr class="even"><td>type2</td><td>125(155)</td><td>175(145)</td><td>300</td></tr><tr class="odd"><td>type3</td><td>...</td><td>...</td><td>...</td></tr><tr class="even"><td>type4</td><td>...</td><td>...</td><td>...</td></tr><tr class="odd"><td>column total</td><td>310</td><td>290</td><td>600</td></tr></tbody></table></li><li><p>check the significance table of chi distribution to find the value of the random variable under given k and <span class="math inline">\(\alpha\)</span></p></li><li><p>compare the <span class="math inline">\(\chi^2\)</span>z with the rejection area, if <span class="math inline">\(\chi^2\)</span> &gt; the found value(which means p value is smaller than <span class="math inline">\(\alpha\)</span>), then reject the null hypothesis</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest, chi2<br>X, y = data_0,target_0<br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><h3 id="mutual-information">1.4 Mutual Information</h3><p>For the theoretical part about Entrophy and Mutual Information, refer to <a href="http://zhengyuanyang.com/2022/09/22/information-theory-in-ML/">this article</a> <span class="math display">\[MI = H(x,y) - H(x|y) - H(y|x) = \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)}\\\]</span> Suppose we have an input variable X with n samples and m unique values {<span class="math inline">\(x_1 = c_ 1,x_2 =c_ 1,...x_n = c_m\)</span>} and an output variable Y with n samples and k unique values {<span class="math inline">\(y_1 = c_ 1,y_2 =c_ 1,...y_n = c_k\)</span>}</p><p>It's easy to calculate <span class="math inline">\(P(X=c_i),P(Y=c_j),P(X=c_i,Y=c_j)\)</span></p><p>Thus the MI can be calculated. The greater MI X and Y share, the greater dependency there exists, and X is thus a more important feature.</p><p>The MI mtheod have the following properties:</p><ul><li>MI method is sometime impractical with two continuous numerical variables, since there are too many unique values</li><li>MI needs some certain metrics to map the original values in to a range(usually [0,1]), so that MI score of different variables can be compared</li></ul><p>An implementation of feature selection based on Mutual Information in Python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mutual_info_score<br>MI_matrix = np.zeros([data_and_target.shape[<span class="hljs-number">1</span>],data_and_target.shape[<span class="hljs-number">1</span>]])<br><span class="hljs-comment"># calculate the MI matrix between each two feature</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,data_and_target.shape[<span class="hljs-number">1</span>]):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,data_and_target.shape[<span class="hljs-number">1</span>]):<br>        MI_matrix[i,j] = mutual_info_score(data_and_target[i+<span class="hljs-number">1</span>],data_and_target[j+<span class="hljs-number">1</span>])<br>df = pd.DataFrame(MI_matrix,columns=data_and_target.columns,index=data_and_target.columns)<br><span class="hljs-built_in">print</span>(df)<br></code></pre></td></tr></table></figure><h2 id="wrapper">2.Wrapper</h2><p>Through the evaluation of the model, add or drop some feature each trail to obtain a subset of all features, which is the selected feature.</p><p><strong>Pro</strong>: accurate, model-relevant</p><p><strong>Con</strong>: time-consuming</p><h3 id="recursive-feature-elimination">2.1 Recursive Feature Elimination</h3><ol type="1"><li>Train model with all m features</li><li>Select k best features and to take them out(or drop k worst feature from all feature)</li><li>Train the model with the rest feature and repeat step2, until we reach the max/min number of feature we want</li><li>The taken-out/ left-in feature is the final features space we want to preserve</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFECV<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-comment"># REF works for all models that have a weight on features</span><br><span class="hljs-comment"># step: the number(or precentage) of feature to eliminate in each iteration</span><br>estimator = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>)<br>selector = RFE(estimator= estimator, n_features_to_select= <span class="hljs-number">3</span>,step=<span class="hljs-number">1</span>)<br>data_transformed = selector.fit_transform(data_0,target_0)<br><br><span class="hljs-comment"># REFCV</span><br><span class="hljs-comment"># REF with cross validation</span><br>cv = StratifiedKFold(n_splits=<span class="hljs-number">5</span>)<br>selector = RFECV(estimator= estimator, min_features_to_select= <span class="hljs-number">3</span>, cv=cv, step=<span class="hljs-number">1</span>)<br>data_transformed = selector.fit_transform(data_0,target_0)<br></code></pre></td></tr></table></figure><h3 id="step-wise-regression">2.2 Step-wise Regression</h3><h2 id="embedded">3. Embedded</h2><p>Some models, like Lasso, Ridge, and Random Forest has method embedded in the model to evaluate features. Train these model first and than make selection of feature.</p><p><strong>Pro</strong>: fast, easy to apply</p><p><strong>Con</strong>: ignore the dependence of features</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br><br>estimator = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>)<br><span class="hljs-comment"># threshold:</span><br><span class="hljs-comment"># threshold of the feature importance to drop</span><br><span class="hljs-comment"># if you apply l1 in your model(or use lasso), the threshold is 1e-5 by default</span><br><span class="hljs-comment"># otherwise, the threshold is mean by default, which means features with a importance less than mean importance will</span><br><span class="hljs-comment"># all by dropped</span><br>selecor = SelectFromModel(estimator,threshold=<span class="hljs-number">0.03</span>)<br>data_transformed = selecor.fit_transform(data_0,target_0)<br><br><br>estimator = LinearSVC(C=<span class="hljs-number">0.01</span>,penalty=<span class="hljs-string">&#x27;l1&#x27;</span>,dual=<span class="hljs-literal">False</span>).fit(data_0,target_0)<br><span class="hljs-comment"># prefit: whether the model given to the selector is already fit</span><br>selecor = SelectFromModel(estimator,threshold=<span class="hljs-number">0.03</span>,prefit=<span class="hljs-literal">True</span>)<br>data_transformed = selecor.transform(data_0)<br><span class="hljs-built_in">print</span>(data_transformed)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Feature Selection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL 基础知识</title>
    <link href="/2022/02/09/MySQL-Basic/"/>
    <url>/2022/02/09/MySQL-Basic/</url>
    
    <content type="html"><![CDATA[<h1 id="mysql基础知识">MySQL基础知识</h1><h2 id="数据库三大范式">1. 数据库三大范式</h2><ol type="1"><li><p>数据库表中，所有字段都是不可分解的原子值，互相不依赖</p></li><li><p>所有非主键字段都依赖于主键</p><p>（学号，姓名，专业名称）其中专业名称不依赖于主键，所以不满足第二范式</p></li><li><p>所有非主键属性都直接依赖于主键</p><p>（学号，姓名，专业id，专业名称）其中包含间接依赖关系 学校-&gt;专业id-&gt;专业名称，不满足第三范式</p></li></ol><h2 id="mysql的架构">2. MySQL的架构</h2><p>MySQL可以分为应用层,逻辑层,数据库引擎层,物理层。</p><p>应用层：负责和客户端，响应客户端请求，建立连接，返回数据</p><p>逻辑层：包括SQK接口，解析器，优化器，Cache与buffer</p><p>数据库引擎层：有常见的MyISAM,InnoDB等等</p><p>物理层：负责文件存储，日志等等</p><h3 id="一条sql语句执行的过程">2.1 一条SQL语句执行的过程</h3><ol type="1"><li>客户端首先通过连接器进行身份认证和权限相关</li><li>如果是执行查询语句的时候，会先查询缓存，但MySQL 8.0 版本后该步骤移除。</li><li>没有命中缓存的话，SQL 语句就会经过解析器，分析语句，包括语法检查等等</li><li>通过优化器，将用户的SQL语句按照 MySQL 认为最优的方案去执行。</li><li>执行语句，并从存储引擎返回数据</li></ol><h3 id="buffer-与-cache-的对比">2.2 buffer 与 cache 的对比</h3><ol type="1"><li>Cache(如Qcache)一般用来把固定语句对应的结果集放在内存，目的是为了提高读取速度</li><li>buffer（如buffer pool）一般用来把页面加载到内存，每次写入时，先更新buffer pool中的日志（redo log），把要修改的页记为脏页。后台进程每隔一段时间将buffer pool中的日志进行刷盘，从而提高了磁盘IO效率，降低了大量写入带来的冲击</li></ol><h2 id="mysql的引擎">3. MySQL的引擎</h2><h3 id="mysql支持的引擎">3.1 MySQL支持的引擎</h3><table><thead><tr class="header"><th>InnoDB</th><th>MyISAM</th></tr></thead><tbody><tr class="odd"><td>支持事务</td><td>不支持</td></tr><tr class="even"><td>支持外键</td><td>不支持</td></tr><tr class="odd"><td>聚集索引</td><td>非聚集索引</td></tr><tr class="even"><td>行级锁和表级锁</td><td>表级锁</td></tr></tbody></table><p>Memory 存储引擎：</p><p>Memory存储引擎将所有数据都保存在内存，不需要磁盘 IO。支持哈希索引，因此查找速度极快。Memory 表使用表级锁，因此并发写入的性能较低。</p><h3 id="innodb的索引">3.2 InnoDB的索引</h3><p>索引是一种数据结构，用于帮助存储引擎快速的找到数据，索引存储在内存上</p><h4 id="聚集索引和非聚集索引">3.2.1 聚集索引和非聚集索引</h4><p>聚集索引中，索引和数据绑定在一起，通过查找索引可以直接返回数据。主键都是聚集索引。非聚集索引中，数据和索引分离，通过一个或多个非聚集索引的查询，查到对应某一值的主键值/地址，然后再使用主键值找到数据</p><p>有时非聚集索引无法直接找到主键值，只能查到满足非聚集索引某一组值的锁欧聚集索引，此时会回到聚集索引中根据主键值继续查找数据，这一过程称为<strong>回表查询</strong></p><h4 id="联合索引和最左匹配原则"><strong>3.2.2 联合索引和最左匹配原则：</strong></h4><p>联合索引是指对表上的多个列的关键词进行索引。</p><p>对于联合索引的查询，如果精确匹配联合索引的左边连续一列或者多列，则mysql会一直向右匹配直到遇到范围查询（&gt;,&lt;,between,like）就停止匹配。where语句中，所有命中的索引被称为匹配列，没有命中但出现在索引中的成为过滤列，所有不在索引中的成为非匹配列</p><p>例如联合索引为（user, age, id), 而范围查询中的谓语为“where id = 1 and age &gt; 10 and user = wang and sex = 'M' ”,此时：</p><ol type="1"><li>在where语句语句中查找谓语user，命中，user为匹配列</li><li>在where语句语句中查找谓语age，命中，但age为范围查询，因此age后的列无法再命中</li><li>id未命中，但出现在索引中，为过滤列</li><li>sex不在索引中，为非匹配列</li></ol><p>对于user和sex，SQL将在内存中的索引（b+树）进行查询（通过explain可查看查询方式为REF）</p><p>对于id，SQL将在符合user，age条件的子树上遍历，这样的复杂度并不降低，但由于是在内存上执行，速度仍然比回表后快（通过explain可查看查询方式为INDEX）</p><p>对于sex，SQL将先回表，返回所有满足user，age，id条件的数据结果，然后再遍历获得满足sex条件的数据结果（通过explain可查看查询方式为ALL）</p><p>Mysql会对第一个索引字段数据进行排序，在第一个字段基础上，再对第二个字段排序。</p><h3 id="索引的数据结构">3.3 索引的数据结构</h3><p>InnoDB采用的是B+Tree索引</p><p>B-Tree：一种自平衡多插树。每个节点都存储key和value。因为每个节点都有数据，所以查询效率较高</p><p>B+Tree：也是自平衡多叉树，但中间节点不存放数据只存放key。只在叶节点存放数据，结构矮胖，出度更大，可以在相同的磁盘空间下容纳更多数据。且在叶节点之间链指针，所以进行范围查询时只需要遍历叶节点即可。</p><p>Hash索引：哈希索引对于每一行数据计算一个哈希码，并将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。只有 Memory 引擎显式支持哈希索引。</p><p><strong>为何不使用红黑树：</strong></p><p>红黑树是二叉查找树，出度为2，因此红黑树存储一张表，高度会比B Tree大很多，IO次数多，检索时间长</p><h2 id="mysql的事务">4. MySQL的事务</h2><h3 id="acid">4.1 ACID</h3><p>事务满足如下几个特性：</p><ul><li><p>原子性（Atomicity）: 一个事务中的所有操作要么全部完成，要么全部不完成。</p></li><li><p>一致性（Consistency）: 事务执行前后数据库的状态保存一致。</p></li><li><p>隔离性（Isolation） 多个并发事务对数据库进行操作，事务间互不干扰。</p></li><li><p>持久性（Durability） 事务执行完毕，对数据的修改是永久的，即使系统故障也不会丢失</p></li></ul><h3 id="事务的并发问题和隔离等级">4.2 事务的并发问题和隔离等级</h3><p>并发问题：</p><ul><li>丢失修改</li><li>脏读：当前事务可以查看到别的事务未提交的数据。</li><li>不可重读：在同一事务中，使用相同的查询语句，同一数据资源莫名改变了。</li><li>幻读：在同一事务中，使用相同的查询语句，莫名多出了一些之前不存在的数据，或莫名少了一些原先存在的数据，例如第二次读取前，表格被插入了新行。</li></ul><p>隔离等级</p><ul><li><p>读未提交： 一个事务还没提交，它做的变更就能被别的事务看到。</p></li><li><p>读已提交： 一个事务提交后，它做的变更才能被别的事务看到。</p></li><li><p>可重复读： 一个事务执行过程中看到的数据总是和事务启动时看到的数据是一致的。在这个级别下事务未提交，做出的变更其它事务也看不到。</p></li><li><p>串行化： 对于同一行记录进行读写会分别加读写锁，当发生读写锁冲突，后面执行的事务需等前面执行的事务完成才能继续执行。</p></li></ul><p>并发问题的解决：</p><p><img src="/2022/02/09/MySQL-Basic/img1.PNG"></p><h2 id="mysql的锁innodb">5. MySQL的锁（InnoDB）</h2><h3 id="锁的性质">5.1 锁的性质</h3><h4 id="共享性">5.1.1 共享性</h4><p>共享锁：其他事务可以读但不能写，又称为读锁</p><p>排他锁：其他事务不能读写，又称为写锁</p><h4 id="粒度">5.1.2 粒度</h4><p><strong>表级锁:</strong> 对当前操作的整张表加锁,实现简单，加锁快，但并发能力低。</p><p><strong>行锁:</strong> 锁住某一行，如果表存在索引，那么记录锁是锁在索引上的，如果表没有索引，那么 InnoDB 会创建一个隐藏的聚簇索引加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁慢，会出现死锁。</p><p><strong>Gap 锁</strong>：锁住一个间隙以防止插入，但不包括间隙的两端。假设索引列有2, 4, 8 三个值，如果对 4 加行锁，那么也会同时对(2,4)和(4,8)这两个间隙加锁。其他事务无法插入值所对应的索引值在这两个间隙之间的记录</p><p><strong>next-key-lock：</strong>next-key lock 实际上就是 行锁+这条记录前面的 gap lock 的组合。假设有索引值10,11,13和 20,那么可能的 next-key lock 包括:</p><p>(负无穷,10],(10,11],(11,13],(13,20],(20,正无穷)</p><p>在 RR 隔离级别下，InnoDB 使用 next-key lock 主要是防止幻读问题产生。</p><h4 id="意向锁">5.1.3 意向锁</h4><p>意向锁必须是表级锁，可以有共享性和排他性。意向锁的作用在于保护不同级别的锁共享性不冲突，或者说提高检测这一冲突的效率。</p><p>假设事务A正在update表t的某一行r，并给这一行添加了排他锁，同时，事务B想要给表t上一个表级共享锁，为了防止共享性冲突，事务B必须检查表t的每一行，检查是否有A上的锁这样的排他锁，然后才能上锁</p><p>因此，A给表t中的行r上排他锁时，同时还会给表t上一个表级的排他意向锁，此时B所上的表级共享锁将会被阻塞</p><p>注意，意向锁只会阻塞表级锁，上例中，如果有另一事务C试图给另一行r2加共享锁，则A所加的排他意向锁不会对其进行阻塞</p><h4 id="死锁">5.1.4 死锁</h4><p>两个事物同时上锁进入互相等待的死循环。</p><p>如何解决死锁：</p><ol type="1"><li>合理设计索引，尽可能通过索引定位行，防止过多的表和行被锁住，减少竞争</li><li>调整SQL语句顺序，update，delete等长时间持有锁的语句放在后面</li><li>把大事物分解成小事务执行</li><li>必要时，可以使用<code>KILL</code>杀死一些进程，释放其持有的锁</li></ol><p>详细分析方法可参考<a href="https://z.itpub.net/article/detail/7B944ED17C0084CF672A47D6E938B750">这篇文章</a></p><h4 id="乐观性">5.1.5 乐观性</h4><p>乐观锁：对于数据冲突保持一种乐观态度，操作数据时不会对操作的数据进行加锁，只有到数据提交的时候才通过一种机制来验证数据是否存在冲突。</p><p>悲观锁：对于数据冲突保持一种悲观态度，在修改数据之前把数据锁住，然后再对数据进行读写，在它释放锁之前任何人都不能对其数据进行操作，直到前面一个人把锁释放后下一个人数据加锁才可对数据进行加锁，然后才可以对数据进行操作，一般数据库的锁都是悲观锁</p><h3 id="上锁方式">5.2 上锁方式</h3><h4 id="表锁">5.2.1 表锁</h4><p>隐式上锁：使用一些关键词时，自动添加自动释放的上锁。如select语句会给表上共享锁，insert，update，delete会给表上排他锁</p><p>显示上锁：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs SQL">LOCK <span class="hljs-keyword">TABLE</span> t1 READ  <span class="hljs-comment">-- 给t1表上共享锁</span><br>LOCK <span class="hljs-keyword">TABLE</span> t1 WRITE  <span class="hljs-comment">-- 给t1表上排他锁</span><br>UNLOCK <span class="hljs-keyword">TABLE</span> t1 <span class="hljs-comment">-- 解锁t1</span><br>UNLOCK TABLES <span class="hljs-comment">-- 解锁所有表</span><br></code></pre></td></tr></table></figure><h4 id="行锁">5.2.2 行锁</h4><p>隐式上锁：使用一些关键词时，自动添加自动释放的上锁。insert，update，delete会给选中的行上排他锁。注意select不会给行加锁</p><p>显示上锁：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> t1 <span class="hljs-keyword">IN</span> SHARE MODE <span class="hljs-comment">-- 给选中的行加共享锁</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> t1 <span class="hljs-keyword">FOR</span> UPDATE <span class="hljs-comment">-- 给选中的行加排他锁</span><br><span class="hljs-keyword">COMMIT</span><br><span class="hljs-keyword">ROLLBACK</span> <span class="hljs-comment">-- 事务完成或回滚时，会解除事物中所加的行锁</span><br>KILL <span class="hljs-comment">-- 杀死某一进程，会解除这一进程所加的行锁</span><br></code></pre></td></tr></table></figure><h2 id="mysql的日志">6. MySQL的日志</h2><h3 id="log种类">6.1 log种类</h3><p>redo log: 存储引擎级别的log（InnoDB有，MyISAM没有），该log关注于事务的恢复。在重启mysql服务的时候，根据redo log进行重做，从而使事务有持久性。</p><p>undo log：是存储引擎级别的log（InnoDB有，MyISAM没有）保证数据的原子性，该log保存了事务发生之前的数据的一个版本，可以用于回滚，是MVCC的重要实现方法之一。</p><p>如果需要执行事务，使用redo log，执行失败则使用undo log，这样的组合保证了事务的一致性</p><p>bin log：数据库级别的log，关注恢复数据库的数据。有关bin log和redo log的区别</p><ol type="1"><li>redo log是InnoDB引擎特有的，只记录该引擎中表的修改记录。binlog是MySQL的Server层实现的，会记录所有引擎对数据库的修改，具有崩溃修复能力。InnoDB通过redo log保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe</li><li>redo log是物理日志，记录的是在具体某个数据页上做了什么修改；binlog是逻辑日志，记录的是这个语句的原始逻辑，例如语句的增删改，并不记录数据页具体发生了什么改变，因此单独的bin log不具备崩溃修复能力。</li><li>redo log是循环写的，空间固定会用完；binlog是可以追加写入的，binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</li></ol><h3 id="wal技术">6.2 WAL技术</h3><p>WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。事务在提交写入磁盘前，会先写到redo log里面去。如果直接写入磁盘涉及磁盘的随机I/O访问，涉及磁盘随机I/O访问是非常消耗时间的一个过程，相比之下先写入redo log，后面再找合适的时机批量刷盘能提升性能。</p><h3 id="两阶段提交">6.3 两阶段提交</h3><p>为了保证binlog和redo log两份日志的逻辑一致，最终保证恢复到主备数据库的数据是一致的，采用两阶段提交的机制。</p><ol type="1"><li>执行器调用存储引擎接口，存储引擎将修改更新到内存中后，将修改操作记录redo log中，此时redo log处于prepare状态。</li><li>存储引擎告知执行器执行完毕，执行器生成这个操作对应的binlog，并把binlog写入磁盘。</li><li>执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交commit状态，更新完成</li></ol><h3 id="mysql-主从机制">6.4 MySQL 主从机制</h3><h4 id="主从配置">6.4.1主从配置</h4><p>一个服务器充当主服务器（master），其余的服务器充当从服务器（slave）。从服务器主要用来读，主服务器主要用写，从服务器定期与主服务器进行同步，复制其数据。因为复制是异步进行的，所以从服务器不需要一直连接着主服务器</p><p>主从机制的优点：</p><ol type="1"><li>数据存在多个镜像，可以防止单一主机崩溃和数据丢失，如果主机宕机可以切换到从服务器上（由于异步同步，可能数据一致性存在问题）</li><li>从服务器可以分担主服务器的读的压力</li></ol><p>注意，部分数据实时性强，经常会被更新，这类数据不适合放在从服务器上，容易引发错误</p><h4 id="主从复制">6.4.2 主从复制</h4><p>MySQL主从复制流程：</p><ol type="1"><li>在事务完成之前，主库在binlog上记录这些改变，完成binlog写入过程后，主库通知存储引擎提交事物</li><li>从库将主库的binlog复制到对应的中继日志，即开辟一个I/O工作线程，I/O线程在主库上打开一个普通的连接，然后开始binlog dump process，将这些事件写入中继日志。从主库的binlog中读取事件，如果已经读到最新了，线程进入睡眠并等待ma主库产生新的事件。</li></ol><h2 id="多版本并发控制mvcc">7. 多版本并发控制(MVCC)</h2><p><strong>多版本并发控制（MVCC）</strong> 是通过保存数据在某个时间点的快照来实现并发控制的。也就是说，不管事务执行多长时间，事务内部看到的数据是不受其它事务影响的，根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。</p><p>可以认为MVCC 是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此可以实现读写并发，并降低死锁概率，IO开销更低。</p><p>MVCC只在 可重复读（REPEATABLE READ） 和读已提交（READ COMMITTED） 两个隔离级别下工作。其他两个隔离级别都和 MVCC 不兼容</p><p>通过MVCC，不显式加锁的一般select语句使用的都是快照读，即读取数据在事务语句执行前或执行后的历史版本，而非当前数据，也即是说其他并发的事物无法影响到该快照。而显式加锁的select语句执行的是当前读，获得目标位置最新的数据</p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
      <tag>Interview Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Statistics for Data Science</title>
    <link href="/2022/02/09/statistics-for-ds/"/>
    <url>/2022/02/09/statistics-for-ds/</url>
    
    <content type="html"><![CDATA[<h1 id="statistics-for-data-science">Statistics for Data Science</h1><h2 id="basic-concepts-for-statistics">1. Basic Concepts for Statistics</h2><p><strong>Statistics</strong>: Statistics gather, describe and analyze sample data in a numerical way to understand the whole population</p><p><strong>Target Population</strong>: A particular group of interest, the distribution of the target population is called</p><p><strong>Sample Population</strong>: A group which the sample is taken from. In most cases it equals to the target population</p><p><strong>Sample</strong>: A subset of the sample population from which data are collected, Doing statistics is a process trying to learn information of the target population through samples. Ideally, sample should be representative to sample population, the sample population should be the same or representative to target population.</p><p><img src="/2022/02/09/statistics-for-ds/1.PNG" alt style="zoom:80%;"></p><p><strong>Variable</strong>: A dimension of a sample representing a specific measure. A sample can contain multiple variables</p><p><strong>Data </strong>: the actual counts, measurements or observation about the variables that markdown with samples</p><p><strong>Data Point and Sample Size</strong>: Data point is a single record of data in the sample. Sample size is the number of data point in the sample</p><p><strong>Parameter</strong>: A numerical description of a population characteristic. Note that a parameter of target population and sample is not the same. We cannot construct statistics with unknown parameters of the whole population</p><p><strong>Sample Statistics</strong>: A function constructed from the sample. A statistics should containing no <strong>unknown</strong> parameters. Common sample statistics includes:</p><table><thead><tr class="header"><th>Sample Statistics</th><th>Format</th></tr></thead><tbody><tr class="odd"><td>mean</td><td><span class="math inline">\(\bar{X} = \frac{1}{n}\sum_i^nX_i\)</span></td></tr><tr class="even"><td>biased variance</td><td><span class="math inline">\(S_0^2 = \frac{1}{n}\sum_i^n(X_i - \bar{X})^2\)</span></td></tr><tr class="odd"><td>unbiased variance</td><td><span class="math inline">\(S^2 = \frac{1}{n-1}\sum_i^n(X_i - \bar{X})\)</span></td></tr><tr class="even"><td>standard deviation</td><td><span class="math inline">\(S = \sqrt{S^ 2}\)</span></td></tr><tr class="odd"><td>moment</td><td><span class="math inline">\(A_k = \frac{1}{n}\sum_i^nX_i^k\)</span></td></tr><tr class="even"><td>central moment</td><td><span class="math inline">\(B_ k = \frac{1}{n}\sum_i^n(X_i - \bar{X})^k\)</span></td></tr></tbody></table><h2 id="data-classification">2. Data Classification</h2><h3 id="categorical-numeric">2.1 Categorical &amp; Numeric</h3><p><strong>Categorical</strong>: consist of labels or description of traits. It's meaning less to apply quantitative calculation on it</p><p><strong>Numeric</strong>: consist of counts and measurement, it have meanings when you apply quantitative calculations</p><h3 id="discrete-continuous">2.2 Discrete &amp; Continuous</h3><p><strong>Discrete</strong>: numeric data that can take only particular values(counts, rate stars)</p><p><strong>Continuous</strong>: numeric data that can take any values in an interval</p><p><img src="/2022/02/09/statistics-for-ds/3.PNG"></p><h3 id="noir">2.3 NOIR</h3><p><strong>Nominal level data</strong>: description of categorical data that order do not matters</p><p><strong>Ordinal level data</strong>: categorical data that have a meaningful order(still not meaningful to add or divide)</p><p><strong>Interval level data</strong>: numeric data that can be arranged in a meaningful order, and the different between data entries are meaningful(timestamp, shoe size, temperature degree)</p><p><strong>Ratio level data</strong>: Interval data where zero indicates absence of something(like body height, 0 inch body high do not have a actual meaning, it means the data are not collected, where 0 degree do have an actual meaning, so it is not a ratio data.) As ratio data cannot participate calculation if data is not included, it must be non-zero in a calculation, thus it can divide other numeric data, that is why it is called ratio data</p><p><img src="/2022/02/09/statistics-for-ds/2.PNG"></p><h2 id="two-important-theorem">3. Two Important Theorem</h2><p>There are two important probability theorem for statistics:</p><h3 id="law-of-large-number">3.1 Law of Large Number</h3><p><strong>Chebyshev's inequality</strong></p><p>For a random variable X, if E[X] and V[X] both exist: <span class="math display">\[P(|X-X[x ]| \ge \epsilon ) \le \frac{D[X]}{\epsilon^2}\]</span> This inequality implies taht the probability of an observation fall far from the expectation is small. The greater <span class="math inline">\(\epsilon\)</span> is, the smaller this probability is.</p><p><strong>Chebyshev's Law of Large Number</strong></p><p>Let a sequence <span class="math inline">\(X_n \to a\)</span>: <span class="math display">\[\lim_{n\to \infty} P(|X_n - a| &lt; \epsilon) = 1 \qquad \forall \epsilon\]</span> This law implies that a statistics on a sample would approaches the same statistics on the population as the sample size is big. In other word, the sample can represent the population when n is big.</p><h3 id="central-limit-theorem">3.2 Central Limit Theorem</h3><p>If a random phenomenon is caused by numerous factors that have same distribution but are independent to each other, then the limit of thu sum of these factors, which is the phenomenon, follows a normal distribution.</p><p>The CLT can be expressed in the following format:</p><p><strong>Lindeberg–Lévy CLT</strong></p><p>Let <span class="math inline">\(X_1, X_2,..X_n\)</span> be a series of independent random variables following same distribution, <span class="math inline">\(E[X_i] = \mu, V[X_ i] = \sigma^2\)</span>, then <span class="math display">\[\lim_{n\to \infty}P(\frac{ \sum_i^n X_ i-n\mu}{\sqrt{n}\sigma } \le X) = \Phi_0(X)\]</span> where <span class="math inline">\(\Phi_0\)</span> is a standard normal distribution</p><p>This theorem implies that when n is big enough, let <span class="math inline">\(Y = \sum_i^nX_i\)</span>, we can regarding Y as following a normal distribution <span class="math inline">\(N(n\mu, n\sigma)\)</span></p><p>Such a conclusion has very important meaning to hypothesis testing. It indicates that, if a statistics is constructed through through adding up sample point, lilke mean, then this statistics should follow a normal distribution no matter what distribution each sample follows. In hypothesis test, if we want to test on the mean of a measure of the population, we can regard that measure of each sample point as a random variable, these variables are independent and same-distributed, so no matter what distribution that measure follows, the mean of it on the total should follow a normal distribution. According to law of large number, as long as n is big enough, the mean on the sample should also follow normal distribution. Thus CLT make hypothesis testing on mean statistics possible.</p><p>Let <span class="math inline">\(\mu, \sigma^2\)</span> be the mean and variance of the population, <span class="math inline">\(\bar{X},S^2\)</span> be the mean and variance of the sample. According to Law of Large Number and CLT: <span class="math display">\[E[\bar{ X}] = \mu\]</span></p><p><span class="math display">\[V[\bar{X}] = \frac{1}{n}\sigma^2\]</span></p><p><span class="math display">\[E[S^2] = \sigma^2\]</span></p><h2 id="topic-in-applied-statistics">4. Topic in Applied Statistics</h2><p>Some topics in statistics are widely applied in domain like machine learning and A/B Test</p><h3 id="sampling">1.Sampling</h3><h3 id="probability-density-estimation">2. Probability Density Estimation</h3><h3 id="statistical-learning-and-machine-learning">3. Statistical Learning and Machine Learning</h3><h3 id="experiment-and-hypothesis-testing">4. Experiment and Hypothesis Testing</h3><h3 id="observational-study-and-causal-inference">5. Observational Study and Causal Inference</h3>]]></content>
    
    
    <categories>
      
      <category>Probability &amp; Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MongoDB Guildbook</title>
    <link href="/2022/02/06/MongoDB-Guildline/"/>
    <url>/2022/02/06/MongoDB-Guildline/</url>
    
    <content type="html"><![CDATA[<h1 id="mongodb">MongoDB</h1><h2 id="about-mongodb">About MongoDB</h2><p>MongoDB is a open- source NoSQL database stroing data in json like documents with schema.</p><p>mongoDB do not have concepts like join.</p><p>mongoDB provides APIs for most programing language</p><h2 id="three-ways-to-access-mongodb">Three ways to access MongoDB</h2><ol type="1"><li>Community server</li><li>VS extending</li><li>MongoDB Altas</li></ol><h2 id="concepts-in-mondodb">Concepts in MondoDB:</h2><p><strong>Document:</strong> a set of K-V pairs. Every document has a unique value via key "_id". Documents have dynamic schema, documents in same collection can have different schema. They. Can hold data of any types allowed by mongodb.</p><p><strong>Collection:</strong> a group of mongodb documents, similar to "tables in other database. Unlike tables, collections does nothave any schema definition, and it cannnot be join. Usually, documents with in a collection belonging to a particular subject.</p><p><strong>Database:</strong> A database is a container of collections of data</p><p><strong>Comparison between RDBMS and MongoDB</strong></p><table><thead><tr class="header"><th style="text-align: left;">RDBMS</th><th style="text-align: left;"></th><th></th><th></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Tables</td><td style="text-align: left;">stand for entity</td><td>Collections</td><td>A set of documents representing one object</td></tr><tr class="even"><td style="text-align: left;">rows</td><td style="text-align: left;">stand for an actual record</td><td>Documents</td><td>a json objects</td></tr><tr class="odd"><td style="text-align: left;">columns</td><td style="text-align: left;">stand for attributes</td><td>Fields</td><td>The first level of the schema</td></tr></tbody></table><h2 id="start-with-mongodb">Start with MongoDB</h2><ol type="1"><li><p>Build a sever(for Mac)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mongod --config /opt/homebrew/etc/mongod.conf<br></code></pre></td></tr></table></figure><p>This create a temp mongo server. If you close the session by ctrl+c, he connecttion would be terminated</p><p>Or you can use the command</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">brew services start mongodb/brew/mongodb-community<br></code></pre></td></tr></table></figure><p>This create a back-support mongoDB server, you can still access to it after you close the terminal, to stop the back-support:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">brew services stop mongodb/brew/mongodb-community<br></code></pre></td></tr></table></figure></li><li><p>Start a connection</p><ol type="1"><li>After start the server, you can type <code>mongo</code> in bash to start a connection. Use <code>exit</code> to stop the connection</li><li>you can use mongodb compass to start a connection, this is a mongondb management UI</li><li>use Datagrip/Dataspell to build a connection</li></ol><p>Notes:</p><ul><li>The port for local host is always 27017</li><li>There's no default username and password</li></ul></li></ol><h2 id="data-types-in-mongodb">Data types in MongoDB</h2><p>Allowed data dytes in mongoDB include:</p><ul><li><p>BSON</p><p>​ A mongoDB data type, it's binary encoded json that can processed faster, it support some types(data, timestamp,object id) that are not supported by json</p></li><li><p>JSON</p></li><li><p>Integer</p></li><li><p>Boolean</p></li><li><p>Double</p></li><li><p>Arrays</p></li><li><p>Objects</p><p>​ Used to store embedded documents. If a documents A contains a K-V pair {"file":B}, where B is another document, the type of B in A's schema is Object</p></li><li><p>Null</p></li><li><p>Date</p></li><li><p>Timestemp</p></li><li><p>Object ID</p><p>​ ObjectIds are small, likely unique, fast to generate, and ordered. It's a usually used as a PK of a document. ObjectId values are 12 bytes in length, consisting of a 4-bytes timestamp value representiong the ObjectId's creation, a 5-bytes random value, a 3-byte incrementing value</p></li><li><p>Code</p><p>​ Like javascript code</p></li></ul><h2 id="create-and-drop-database">Create and Drop Database</h2><p>you can create databse by:</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">use</span> &lt;database-<span class="hljs-keyword">name</span>&gt;<br></code></pre></td></tr></table></figure><p>It would select the DB, if not exists, it create the DB</p><p>Notes: The created databse will not be visible untill you insert any data into it</p><p>To drop the database:</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">db.dropDatabase<span class="hljs-comment">()</span><br></code></pre></td></tr></table></figure><p>where <code>db</code> refer to the currently used database.</p><p>Before you drop the DB, makesure you select the DB first.</p><p>Some other commands related to create and drop</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-literal">show</span> databases -- list <span class="hljs-literal">all</span> visible DB<br>db -- <span class="hljs-literal">show</span> the <span class="hljs-literal">current</span> DB<br></code></pre></td></tr></table></figure><h2 id="create-and-drop-colletions">Create and drop colletions</h2><p>to create a collection in Database:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.create<span class="hljs-constructor">Collection(<span class="hljs-string">&quot;name&quot;</span>,<span class="hljs-params">options</span>)</span><br></code></pre></td></tr></table></figure><p>to drop a collection:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">db</span>.collection_name.<span class="hljs-keyword">drop</span>()<br></code></pre></td></tr></table></figure><p>to show collections in current DB</p><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dart"><span class="hljs-keyword">show</span> collections<br></code></pre></td></tr></table></figure><h2 id="insert-documents">Insert documents</h2><p>to create one documents:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>insert<span class="hljs-constructor">One(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;)</span>;<br></code></pre></td></tr></table></figure><p>to create many documents:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>insert<span class="hljs-constructor">Many([&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;B&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;C&quot;</span>&#125;])</span><br></code></pre></td></tr></table></figure><p>Note: the input of insertMany() should be a list[]</p><h2 id="update-documents">Update documents</h2><p>To update a documents</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.updateOne</span>(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;B&quot;</span>&#125;,&#123;<br>        <span class="hljs-variable">$set</span>:&#123;<br>            <span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;123456&quot;</span><br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><p>Where:</p><ul><li>the first parameter eplicts the documents to update</li><li>the second parameter explicts the operation to conduct</li></ul><p>Notes:This command only applies to the first document that meet the search condition(the first pararmeter)</p><p>to update many documents:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.updateMany</span>(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;123456&quot;</span>&#125;,&#123;<br>        <span class="hljs-variable">$set</span>:&#123;<br>            <span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span><br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><h2 id="read-data">Read data</h2><p>To read(and modify) data:</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment">-- find all documents in the collection</span><br>db.C1.find()<br><br><span class="hljs-comment">--find particular documents </span><br>db.C1.find(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>,<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span>&#125;) <br><br><span class="hljs-comment">-- find the first documents that fits the condition</span><br>db.C1.findOne(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and delete the first documenttaht fits the condition</span><br>db.C1.findOneAndDelete(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;D&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and replace the first documenttaht fits the condition</span><br><span class="hljs-comment">-- the second parameter gives a whole documents(like the format in insert)</span><br>db.C1.findOneAndReplace(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>,<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;23456&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and update</span><br><span class="hljs-comment">-- the second parameter gives a the operation to apply on the  documents(like the format in update)</span><br>db.C1.findOneAndUpdate(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>&#125;,&#123;<br>$<span class="hljs-keyword">set</span>:&#123;<br><span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span><br>&#125;<br>&#125;<br>   )<br>   <br><span class="hljs-comment">-- find and Modify</span><br><span class="hljs-comment">-- pass a json that tell the command to conduct multiple operaions</span><br>db.C1.findAndModify(<br>    &#123;<br>    <span class="hljs-string">&quot;query&quot;</span>: &#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>&#125;,<br>    <span class="hljs-string">&quot;update&quot;</span>:&#123;$<span class="hljs-keyword">set</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;&#125;,<br>    &#125;<br>)<br><br></code></pre></td></tr></table></figure><h2 id="delete-documents">Delete documents</h2><p>to delete documents from a collection:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">-- delete the first documetn fits the condition<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>delete<span class="hljs-constructor">One(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;)</span><br>-- delete all documents fit the condition<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>delete<span class="hljs-constructor">Many(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;)</span><br></code></pre></td></tr></table></figure><h2 id="query">Query</h2><p>The first parameter is called query condition, by passing a json map, you tell the command constraints of fields.</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>find(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>,<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;T&quot;</span>&#125;)<br></code></pre></td></tr></table></figure><p>you can also use the $and operand:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">db.C1.<span class="hljs-builtin-name">find</span>(<br>&#123;<br><span class="hljs-variable">$and</span>:[<br>&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;T&quot;</span>&#125;<br>]<br>&#125;<br>)<br></code></pre></td></tr></table></figure><p>Other operand in this formats:</p><ul><li>or</li><li>nor</li></ul><p>to query documents with quantify condition:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">db.C1.<span class="hljs-builtin-name">find</span>(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:&#123;<span class="hljs-variable">$gte</span>:<span class="hljs-string">&quot;20000&quot;</span>&#125;&#125;)<br></code></pre></td></tr></table></figure><p>Other operand in this formats:</p><ul><li>lte: less than ot equal</li><li>gt: greater than</li><li>lt: less than</li><li>eq: equal</li><li>neq: not equal</li></ul><h2 id="select-specific-fields">Select specific fields</h2><p>to select specific fields:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">var</span> pipline = [<br>    &#123;<span class="hljs-variable">$sort</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;&#125;<br>]<br><span class="hljs-keyword">db</span>.C1.aggregate(pipline)<br></code></pre></td></tr></table></figure><p>Notes:</p><ul><li>the first parameter explicts the query condition</li><li>the second parameter explicts the fields you want or do not want(Projection)</li><li><strong>You can not mix inclusion and exclusion</strong> in the second parameter like {"name":0,"mobile":1}</li><li>**The only field that can be mixed is "_id"** of documents. {"name":1,"_id":0}</li></ul><h2 id="projection">Projection</h2><p>Projection is a mechanism allowing you to select specific fieds, like slice of an array.</p><h2 id="aggregation">Aggregation</h2><p>To perform aggregation on collections:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">var</span> pipline = [<br>    &#123;<span class="hljs-variable">$sort</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;&#125;,<br>    &#123;<span class="hljs-variable">$limit</span>:4&#125;<br>]<br><span class="hljs-keyword">db</span>.C1.aggregate(pipline)<br></code></pre></td></tr></table></figure><p>where pip line is list of operations,</p><ul><li>$count</li><li>$group</li><li>$limit</li><li>$lookup</li><li>$match</li><li>$merge</li><li>$sort</li><li>project</li><li>unwind</li><li>unset</li></ul><h2 id="limit-and-skip">Limit and skip</h2><p>Limit the results returned by:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.limit</span>(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>showing the first there results</p><p>Skip the first 2 results and show the rest:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.skip</span>(<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="sort">Sort</h2><p>to sort results:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.sort</span>(&#123;<span class="hljs-string">&quot;name&quot;</span>:-<span class="hljs-number">1</span>&#125;)<br></code></pre></td></tr></table></figure><p>where the json map in sort() specify the sorting depending on which fields. 1 for ascending and -1 for descending. If you pass multiple fields, it sort the next fields in the groups of previous fields.</p><h2 id="create-and-drop-index">Create and Drop index</h2><p>Create indexes:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>create<span class="hljs-constructor">Index(&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;)</span><br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>create<span class="hljs-constructor">Indexes([&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;,&#123;<span class="hljs-string">&quot;mobile&quot;</span>:1&#125;])</span><br></code></pre></td></tr></table></figure><p>Drop index:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>drop<span class="hljs-constructor">Index(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:1&#125;)</span><br><br>-- drop all indexes except _id<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>drop<span class="hljs-constructor">Indexes()</span><br></code></pre></td></tr></table></figure><p>Group by:</p><p>Group by operand:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.aggregate</span>(<br>    &#123;<br>        <span class="hljs-variable">$group</span>:&#123;<br>            _id:<span class="hljs-string">&quot;$name&quot;</span>,<br>            <span class="hljs-string">&quot;count&quot;</span>:&#123;<span class="hljs-variable">$sum</span>:<span class="hljs-number">1</span>&#125;<br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><p>Where:</p><ul><li>_id is an essential arguments, it explain group by which field</li><li>put a $ before the field to group by, it's necessary</li><li>if you want to group by multiple levels, use _id:["$name","$mobile"]</li><li>the "count" is an alias defined by user</li><li>{$sum:1} = count(*)</li><li>{$sum:"$field"} = sum(field)</li><li>other operans includes: $avg, $min, $max</li><li>$push: push all values of the given field in the group into one array</li><li>$addToSet: same as $push, but return a unique set</li><li>$first, $last: return the first/last value</li><li>if _id:null, then return all documents in one group</li></ul><h2 id="back-up-restore">Back up &amp; Restore</h2><p>to back up all databases</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">-- dump the current database<br>mongodump<br></code></pre></td></tr></table></figure><p>To restore all databases</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mongorestore<br></code></pre></td></tr></table></figure><h2 id="transaction">Transaction</h2><p>For transation realted content in MongoDB, please refer to <a href="https://zhuanlan.zhihu.com/p/71679945">this link</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NoSQL</tag>
      
      <tag>MongoDB</tag>
      
      <tag>Guidebook</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
