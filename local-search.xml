<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Information theory in ML</title>
    <link href="/2022/09/22/information-theory-in-ML/"/>
    <url>/2022/09/22/information-theory-in-ML/</url>
    
    <content type="html"><![CDATA[<h1 id="information-theory-in-machine-learning">Information theory in Machine Learning</h1><h2 id="information-and-entrophy">1. Information and Entrophy</h2><p><strong>self-info</strong></p><p>self-information is a measure related to the outcome of a probabilistic event. To quantify the information, we hope the measure would have following properties:</p><ul><li>A low-probability outcome contains more infomation than a high-probability event. For example, "I won't die" contains more information than "I will die". To some extent, we can regard self-info as the degree we would feel surprise about an certain outcome of an event</li><li>Information quantity must not be negative</li><li>The total imformation of several outcome should be the sum of ther individual information</li><li><span class="math inline">\(I(y=m_1,y=m_2) = I(y=m_1)+I(y=m_2)\)</span></li><li>I(y) must be continuous if P(y) is continuous</li></ul><p>It can be proved that the only expression that satisfies these properties is <span class="math inline">\(Klog(P(y))\)</span> where K is negative. Set K= 1, we define the self-information of an outcome of an event as: <span class="math display">\[I  = log(\frac{1}{P(y=c_m)}) = -log(P(y=c_m))\]</span></p><p><strong>Entrophy</strong></p><p>Entrophy is the expectation of self-info. It represent the mean information of an event related to an random variable y, the greater H is, the more uncertain event y is <span class="math display">\[H(Y) = E[I(y)] = -\sum_i^m p(y=c_m)log(p(y=c_m))\]</span></p><h2 id="entrophy-measure-for-two-variables">2. Entrophy measure for two variables</h2><h3 id="joint-entrophy">2.1 Joint Entrophy</h3><p>Joint entropy measure the uncertainty of a joint event (X, Y) <span class="math display">\[H(T,Y) =-\sum_t\sum_y p(t,y)log(p(t,y))\]</span></p><h3 id="conditional-entrophy">2.2 Conditional Entrophy</h3><p>Conditional Entrophy represent the Information recieved of one event when another event is for certain <span class="math display">\[H(Y|T) =-\sum_t\sum_y p(t,y)log(p(y|t)) = -\sum_t\sum_y p(y|t)p(t)log(p(y|t))\]</span> The higher H(Y|T) is, the less "extra information" about Y is needed when T is certain, which means the corralation between Y and T l is higher</p><p>The relationship between conditional entropy and joint entropy: <span class="math display">\[H(Y｜T) = H(Y, T) - H(T)\]</span></p><h3 id="inofrmation-gain">2.3 Inofrmation Gain</h3><p>Information gain represent the amount of uncertainty reduction of a variable when another variable is certain <span class="math display">\[IG(Y|T) = H(Y) - H(Y| T)\]</span> It is usually applied as an impurity metrics in splitting algorithm like Decision Tree</p><h3 id="mutual-information">2.4 Mutual Information</h3><p>The nature of Mutual Information and Information Gain is the same, mutual information is the shared entropy of two variable</p><p><img src="/2022/09/22/information-theory-in-ML/1.png"></p><p>First, when we consider two event together, we can rewrite H(Y) as: <span class="math display">\[&amp; H(Y) = -\sum_i^m p(y=c_i)log(p(y=c_i) \\&amp; = -\sum_j^n p(x = c_j)\sum_i^mp(y=c_i|x_=c_j)log(p(y=c_i)) \\&amp;= \sum_x\sum_yp(x,y)log(p(y))\]</span></p><p><span class="math display">\[MI = H(x,y) - H(x|y) - H(y|x) = \sum_x\sum_yp(x,y)log\frac{p(x,y)}{p(x)p(y)}\\IG = H(y)-H(y|x) = \sum_x\sum_yp(x,y)log(p(y)) - \sum_x\sum_yp(x,y)log(p(y|x)) = MI\]</span></p><p>MI is usually mentioned when calculating the correlation of two variable, IG is mentioned when calculating the impurity reduction of one variable when splitting another variable</p><h2 id="entrophy-measure-for-two-distribution">3. Entrophy measure for two distribution</h2><h3 id="kl-divergencerelative-entrophy">3.1 KL Divergence(Relative Entrophy)</h3><p>The KL divergence is the expectation(under true distribution) of the difference between information amount under predicted and true distribution <span class="math display">\[D_{KL}(p||q) = \sum_{i=1}^mp(y_i)\log(\frac{p(y_i)}{q(y_i)})\]</span> Where:</p><ul><li>q is the estimated distribution of y</li><li>p is the real distribution of y</li></ul><p>it can be used to evaluate an estimation of a distribution with the real distribution given</p><h3 id="cross-entrophy">3.2 Cross Entrophy</h3><p>The equation KL divergence can be written as: <span class="math display">\[D_{KL}(p||q) = \sum_{i=1}^mp(y_i)\log(\frac{p(y_i)}{q(y_i)}) = -\sum_i^mp(y_i)log(q(y_i)) - (-\sum_i^mp(y_i)log(p(y_i)))\]</span> Obviously, the second term of this equation is <span class="math inline">\(H_p(x)\)</span>, which is the entrophy of the real data. In a machine learning problem, since real entrophy is fixed for a dataset, we can only minimize the first term. we call this term Cross entropy: <span class="math display">\[CH(p,q) = D_ {KL}(p||q) + H_p(X) = -\sum_i^mp(y_i)log(q(y_i))\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Loss Function</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Common Loss Function in Machine Learning</title>
    <link href="/2022/09/22/loss-function/"/>
    <url>/2022/09/22/loss-function/</url>
    
    <content type="html"><![CDATA[<h1 id="common-loss-function-in-machine-learning">Common Loss Function in Machine Learning</h1><h2 id="numerical-output">Numerical Output</h2><h3 id="msel2-lossrmse">1. MSE(L2 Loss)/RMSE</h3><p>Mean Squared Error <span class="math display">\[MSE = \frac{1}{n}\sum_i^n(y_i-\hat{y})^2\]</span></p><ul><li>MSE is differentiable any where, which makes it suitable for gradient descent optimizer</li><li>With gradient decrease, the MSE will decrease, which means MSE is effective with fixed learning rate</li><li>The squared operator gives enlarge the loss, thus MSE is sensitive ti outlier, if outlier is meant to be detected, suing MSE is fine. If the outlier is treat as part of the training data, MSE is not an ideal loss function (e. g. prediction model for sales promotion day)</li></ul><p><img src="/2022/09/22/loss-function/1.png"></p><p>RMSE is the square root of MSE <span class="math display">\[RMSE = \sqrt{MSE}\]</span></p><ul><li>RMSE is on the same scale with MSE, but it make the loss more interpretable in some case(e.g prediction on product price)</li></ul><h3 id="mae-l1-loss">2. MAE (L1 Loss)</h3><p>Mean Absolute Error <span class="math display">\[MAE = \frac{1}{n}\sum_i^n|y_i-\hat{y}|\]</span></p><ul><li>MAE is not sensitive to outlier, it is less likely to cause gradient explosion</li><li>The gradient for any loss is the same, which means MAE does not converge well with fix learning rate</li><li>MAE is not differentiable at 0</li></ul><p><img src="/2022/09/22/loss-function/2.png"></p><h3 id="smooth-l1huber-loss">3. Smooth L1(Huber Loss)</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^nz_ i\\z_i = \left\{\begin{aligned}MSE \qquad&amp; |y -\hat{y}| &lt; \beta \\MAE \qquad &amp; elsewhere \\\end{aligned}\right.\]</span></p><ul><li><span class="math inline">\(\beta\)</span> is a defined threshold for error of outlier. If the error is within <span class="math inline">\(\beta\)</span>, apply MSE, otherwise, apply MAE. Huber Loss combines the advantages of L1 and L2 Loss</li><li>Huber loss is ideal for NN</li></ul><p><img src="/2022/09/22/loss-function/3.png"></p><h3 id="log-cosh-loss">4. Log-cosh Loss</h3><p><span class="math display">\[L = \sum_ i^ nlog(cosh(y_i-\hat{y_i}))\]</span></p><ul><li>Log-cosh has almost every virtue of Huber loss, the difference is, it is second-differentiable anywhere, such method is very useful when applying newton-method optimzer</li><li>However, when erro is very big, Log-cosh loss still would have gradient or hessian problem(gradient remains same when loss decrease). Just like MAE</li></ul><h2 id="categoricalprobability-output">Categorical/Probability Output</h2><h3 id="log-losscross-entropy-loss">1. Log Loss/Cross Entropy Loss</h3><p>Log Loss is basically the same concept as the cross entropy, the only difference is that it can be applied at a single sample by replaceing true probability p(x) with 1 if <span class="math inline">\(y_ {true} = m\)</span>. The average loss of all sample is the Log Loss or Cross Entropy Loss.</p><p>For Cross Entropy, refer to <a href>ongoing</a></p><p>When it come to a binary classification scenario, the Log Loss can be written as: <span class="math display">\[LogLoss = -\frac{1}{n}\sum_i^n [y_ilog(p(y_i))+(1-y_i)log(1-p(y_i))]\]</span></p><ul><li>The gradient would decrease as Log Loss decrease, thus Log Loss can work with fixed learning rate</li><li>Sentive to outlier</li><li>GDBT usually apply Log Loss as loss function(classification)</li></ul><p>The following picture is the log loss of the prediction on a positive sample:</p><p><img src="/2022/09/22/loss-function/4.png"></p><h3 id="exponential-loss">2. Exponential Loss</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^ne^{-yf(x)}\]</span></p><ul><li>Theoretically, the optimal of Exponential Loss and Log Loss is the same(<span class="math inline">\(\frac{1}{2}log\ odds\)</span>), the advantage of exponential loss is that is is easier to calculate, thus can make optimzer update the weight with less cost</li><li>Sentive to outlier</li><li>AdaBoosting usually apply Exponential Loss as loss function(classification)</li></ul><h3 id="hinge-loss">3. Hinge Loss</h3><p><span class="math display">\[L = \frac{1}{n}\sum_i^n max(0,1-yf(x))\]</span></p><ul><li>If the model label the sample correct, the loss is 0</li><li>Less sensitive to outlier</li><li>SVM usually adopt Hinge Loss as loss function</li></ul><p><img src="/2022/09/22/loss-function/5.png"></p><h3 id="focal-loss">4. Focal Loss</h3><p>Focal Loss is a improved version of Cross Entrophy Loss <span class="math display">\[L_ f = -\frac{1}{n}\sum_i^n\sum_ j^m\alpha_ j(1-p(y ))^\gamma y log(p(y))\]</span></p><ul><li>Focal Loss add a focus factor <span class="math inline">\((1-p(y))\)</span>, so that those sample with high predicted probability, which are the ""easy samples", donate less loss. Compare to CE Loss, Focal Loss focus on those "hard samples"</li><li>Focal Loss also add a balance factor(optional), which is the percentage of a certain category of y among all samples. This make focal loss can deal with imbalanced data</li><li><span class="math inline">\(\gamma\)</span> is a influence parameter. When <span class="math inline">\(\gamma = 0\)</span>, the Focal Loss become CE Loss. In preactice, we usually set <span class="math inline">\(\gamma = 2\)</span></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Loss Function</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Evaluation Method for Classification</title>
    <link href="/2022/09/21/Evaluation_Classification/"/>
    <url>/2022/09/21/Evaluation_Classification/</url>
    
    <content type="html"><![CDATA[<h1 id="evaluation-method-for-classification">Evaluation Method for Classification</h1><h2 id="confusion-matrix">1. Confusion Matrix</h2><p>The confusion matrix is a table that count different cases of the predicted outcome given by a classification model. For a binary classification model, it can be given as:</p><p><img src="/2022/09/21/Evaluation_Classification/confusion-matrix.png"></p><p>For a multiple classification case, the confusion matrix can be given as:</p><p><img src="/2022/09/21/Evaluation_Classification/2.png"></p><p>For such case, we can define TP, TN, FP, FN for each category by deeming all true “not-bird” samples predicted “not-bird” as TN, etc.</p><h2 id="accuracy-precision-recall-and-f1-score">2. Accuracy, Precision, Recall and F1 score</h2><p>With confusion matrix given, we can now define the following matrix:</p><p><strong>Accuracy</strong></p><p>Accuracy = samples predicted correctly / all predicted samples <span class="math display">\[accuracy = \frac{TP+TN}{TP+FP+FN+FP} \]</span> Accuracy is a very intuitive metrics. However, it sometimes cannot directly reflect the predicting performance of the model as it cannot deal with imbalanced data. For example, suppose we have a sample set of 100 sample with 99 positive and 1 negative, even if the model simply predicted all samples as positive without any training, it would still receive an accuracy of 99%.</p><p><strong>Recall and Precision</strong></p><p>Recall = correctly predicted positive samples / all actual positive samples <span class="math display">\[racall = \frac{TP}{TP + FN}\]</span> Recall represent the ability to find positive samples among all actual postive samples. It can deal with imbalanced data. It is sensitive to FN case, thus is suitable for the business case where FN would bring significant cause(e. g Explosion recognition, vehicle safety judgement). On the other side, recall does not consider FN, a model can simply improved recall by judging all samples as positive, which is not good in some cases.</p><p>Precision = correctly predicted positive samples / all predicted positive samples <span class="math display">\[precision = \frac{TP}{TP + FP}\]</span> Precision represent the probability that a model's judgment on positive case is correct. It can partly deal with imbalanced data. It is sensitive to FP case, thus is suitable for the business case where FP would bring significant cause(e. g Crime judgment, disease diagnosis).</p><p>Recall and Precision can both deal with imbalanced data. However, there's a trade-off between these two metrics. Thus, which metric to put emphasis on idepends on specific business application. Nevertheless, in most cases, since we can just flip P/N, or 1/0, precision is more like an accompanied constraint of recall to prevent model from foucsing too much on capture the minor category samples, as FN and FP are both bad in most business application.</p><p><strong>F1 Score</strong></p><p>The F-measure is a function that balance Precision and recall <span class="math display">\[F_\alpha = \frac{(1+\alpha^2)*P*R}{(\alpha^2*P)+R}\]</span> when <span class="math inline">\(\alpha\)</span> = 1, we call this metric F1 score: <span class="math display">\[F1 = \frac{2PR}{P+R}\]</span> F1 socre combine Recall and Precision to find a balance. It is suitable for many business case where we cannot decide a clear preference.</p><h2 id="p--r-curve-roc-curve-and-auc">3. P- R Curve, ROC Curve and AUC</h2><p><strong>P- R Curve</strong></p><p>P-R Curve is a curve to depict the relationship between Precision and Recall. It's application is similar to F1 score, but lessly used as F1 score is more concise to read( The higher the better)</p><p><img src="/2022/09/21/Evaluation_Classification/3.PNG"></p><p>Usually, we can regard model B as a better model if it can completely wrap the curve of A. If that; s not the case, we can mark the point on the curve where precision equals recall(Break Event Point, or BEP). The curve with a BEP closer towards up- right direction is better.</p><p><strong>ROC Curve</strong></p><p>We define:</p><ul><li>True positive rate (sensitivity): the ratio that actual positive samples predicted correctly</li><li>True negative rate (specificity): the ratio that actual negative samples predicted correctly</li><li>False positive rate (1-specificity): the ratio that actual negative samples predicted wrongly</li></ul><p><span class="math display">\[TPR = \frac{TP}{TP+FN} = Recall\\TNR = \frac{TN}{FP+TN} = 1-TNP\\FPR = \frac{FP}{FP+TN}\]</span></p><p>From a probabilistic aspect:</p><table><tbody><tr class="odd"><td>Precision</td><td style="text-align: center;"><span class="math inline">\(P(Y=1|\hat{Y}=1)\)</span></td></tr><tr class="even"><td>Recall (sensitivity)</td><td style="text-align: center;"><span class="math inline">\(P(\hat{Y}=1|Y=1)\)</span></td></tr><tr class="odd"><td>Specificity</td><td style="text-align: center;"><span class="math inline">\(P(\hat{Y}=0|Y=0)\)</span></td></tr></tbody></table><p>From this interpreation, we found that sensitivity and specificity are condition on Y, which means The influence of P(Y) are blocked whe calculating these two metrics. Therefore, these two metrics are not influenced by the imbalance of data.</p><p>The Reciever Operating Characteristics cureve(ROC curve) take both metrics into consideration by depict the relationship between sensitivity and 1-specificity:</p><p><img src="/2022/09/21/Evaluation_Classification/4.png"></p><p><strong>AUC</strong></p><p>Area Under Curve(AUC) is the area beneath the ROC curve.</p><p>Suppose our model completely randomly classifies the samples, the the probability it regard an actual postive sample or an actual negative sample as a positive sample is equal, in this case, AUC would be 0.5. If AUC &gt; 0.5, it means when the model predicts a sample, <span class="math inline">\(P(\hat{Y}=1|Y=1) &gt; P(\hat{Y}=1|Y=0)\)</span> , which means the prediction is effective. Thus, the higher the AUC is, the better the model performs.</p><p>Obviously, AUC is not influenced by imbalanced data, and it's delivery information concisely. Thus, it is one of the most frequently used metrics in classification.</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
      <tag>Classification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Causes and Solutions of Overfitting</title>
    <link href="/2022/09/19/overfitting/"/>
    <url>/2022/09/19/overfitting/</url>
    
    <content type="html"><![CDATA[<h1 id="overfitting-cause-and-solution">Overfitting: Cause and Solution</h1><h2 id="biasvariance-and-overfitting">1. Bias，Variance and Overfitting</h2><p>Theoretically, we consider the expected risk of the model on real data consists of two parts:</p><p><strong>Bias</strong></p><p>The model's failure to imitate the real mapping, probability distribution or other relationship of the data. For example, if we try to fit a non-linear relationship with a linear model, there would be inevitable bias. Another example is that we leave some important factors omitted</p><p><strong>Variance</strong></p><p>The model's sensitivity to changes in data. High variance occurs when the model try to seize any details. It put too many weights on unimportant feature or noise in order to reduce error in training process.</p><p>Neither high bias nor high variance is good. However, when we take measure to solve bias, like applying complex model or increase feature, the variance of the model increase accordingly. In other worlds, there is a trade-off between bias and variance. In training process, our aim would be finding a model with bias and variance acceptable.</p><p><strong>Underfitting and Overfitting</strong></p><p>When the bias is high, we cannot describe the pattern of the data correctly, we call such a scene Underfitting</p><p>When the bias is low, but the variance is high, the model is trapped in the details or noises of the data, ot fit well on the given data but it has poor generality so that it would fail on other data</p><p><img src="/2022/09/19/overfitting/1.PNG"></p><p>The performance on training and testing dataset under these scenes</p><table><thead><tr class="header"><th></th><th>training SET Erro</th><th>Testing Set ERROR</th></tr></thead><tbody><tr class="odd"><td>Underfit</td><td>high</td><td>high</td></tr><tr class="even"><td>Overfit</td><td>low</td><td>high</td></tr><tr class="odd"><td>Optimum</td><td>low</td><td>low</td></tr></tbody></table><h2 id="causes-of-overfitting">2. Causes of Overfitting</h2><p>As specified above, the core reason that cause of overfitting is the imbalanced trade-off(High variance, low bias). Specifically, we induct the reasons into:</p><ul><li>Complexity of the model: The model is too complex for the pattern we want to discover in the data</li><li>Defects of data: the samples contains so much noises that the model cannot ignore them</li><li>Overtraining: the model is trained with too much epochs that force the model to learn noises in order to converge</li><li>Improper sampling/splitting: the training set fails to represent the distribution of the real data. Or, in some other case, the real distribution itself decides that the model is hard to imitate it.</li></ul><h2 id="solutions-for-overfitting">3. Solutions for Overfitting</h2><p>According to four reasons, we can also induct the solutions into:</p><h3 id="control-complexity">3.1 <strong>Control complexity</strong></h3><p><strong>Regularization</strong></p><p>In machine learning, regularization refer to constraints on the number of feature dimensions. Usually, it is realized through adding an regular term to the loss function to penalize putting weight on too many features. This includes:</p><ul><li>Lasso regression and Ridge regression</li><li>Soft margin for SVM</li><li>Regular term in XGBoost</li><li>...</li></ul><p><strong>Feature Engineering</strong></p><p>Reduce the number of features. FIlter those redundant feature through feature engineering methods like correlation analysis and dimensionality reduction. For details, please refer to <a href>ongoing</a></p><p><strong>Simplify Structure</strong></p><p>An important rule for machine learning is to solve the task with possible simplest model. A model with simpler structure can usually solve overfitting. Specific action includes:</p><ul><li>Dropout in NN</li><li>pruning in tree-based model</li><li>Hyperparameter like hidden size, max-leaf-node</li></ul><h3 id="data-augmentation"><strong>3.2 Data augmentation </strong></h3><p>The best way to eliminate the variance caused by data defects is simply increasing more data to the training set. Since sufficient data are sometimes unavailable in real project. We can apply data augmentation to generate more training data. Data augmentation is more common in deep learning feild.</p><h3 id="early-stopping"><strong>3.3 Early stopping</strong></h3><p>Stop the optimizer earlier to prevent overtraining. For example:</p><ul><li>raise error threshold for an optimzer</li><li>set max depth for an decision tree</li><li>set max iterations for an neural network</li></ul><h3 id="sampling-and-spliting"><strong>3.4 Sampling and spliting</strong></h3><p><strong>Cross Validation</strong></p><p>Cross validation means spliting the dataset into subsets. Use some of them to estimate the distribution and other of them to evaluate the estimation. Such procedure can effectively control the varaince caused by sample selection bias. For the details of cross validation, refer to<a href>ongoing</a></p><p><strong>Sampling </strong></p><p>From an theoretical perspective, sampling itself is actually a kind of non-parameter ML model. When you do sampling, yur actual target is to imitate the distribution of the real population through getting a sample. Thus, the sample itself would have bias if it cannot represent the true distribution of the population. Training a mode using these samples would obviously cause variance.</p><p>Thus, a way might help solving overfitting is improving your sampling method. For details of sampling methods in ML, refer to <a href>ongoing</a></p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Evaluation</tag>
      
      <tag>Overfitting</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Principle</title>
    <link href="/2022/09/12/Principle/"/>
    <url>/2022/09/12/Principle/</url>
    
    <content type="html"><![CDATA[<h1 id="learning-principle">Learning Principle</h1><p>In machine learning, learning principle refer to the standard for judging whether a model is good or not</p><h2 id="loss-function-risk-function-and-objective-function">1. Loss function, Risk function and Objective function</h2><p>Loss function is a function used to evaluate the fitness of a model</p><p>For supervised learning, loss function evaluate the difference between the predicted output and true output, noted as <span class="math inline">\(L(Y,f(x, \theta))\)</span>. Ususally, loss function is applied to a single sample or a part of the samples, it cannot evaluate the overall performance of the model. To obtain that, we define: <span class="math display">\[R_{exp}(\theta) = E_P[L(Y,f(x, \theta))] = \int_{X*Y}L(Y,f(x, \theta)P(X,Y)dxdy\]</span> Where <span class="math inline">\(R_{exp}\)</span> is called <strong>risk function</strong> or **expected loss*</p><p>To convert a ML problem into an optimization problem, a ideal practice is to adopt risk function as the objective function for the optimization program. However, this require us to know the true JPD P(X, Y), which is usually unknown in real problem(we can only know the estimated JPD through observational dataset), this is called an ill-formed problem</p><p><em>Note: sometimes, instead of directly use loss function, we would use a function of the loss function to construct objective function, for example, the boosting algorithm</em></p><p>For unsupervised learning, the loss function is a totally different thing, we usually directly talked about the objective function since we do not have a <span class="math inline">\(Y_{true}\)</span> to compare with. The specific form of the objective function varies from specific type of unsupervised learning</p><h2 id="empirical-risk-minimization">2. <strong>Empirical Risk Minimization</strong></h2><p>An obvious solution for ill-formed problem is to replace ture P(X,Y) with the observed <span class="math inline">\(\hat{P(X,Y)}\)</span> on training dataset.</p><p>Suppose the weight of all sample are equivalent, we define <span class="math display">\[R_{emp}(\theta) = \frac{1}{N}\sum_{n=1}^N L(y, f(x,\theta)\]</span> Where <span class="math inline">\(R_{emp}\)</span> is called empirical risk.</p><p>When we use empirical risk as our objective function, we call the learning principle of the machine learning "ERM"</p><p>For a probability model, under some condition, we can consider ERM as equivalent to a <strong>Maximum Likehood Estimation</strong>(MLE). Refer to another article about parameter estimation</p><h2 id="structural-risk-minimization">3. Structural Risk Minimization</h2><p>When sample size is big enough, empirical risk would be close enough to the real expected risk. However, in real problem we will not have infinite samples. We would probably obtain a subset of the sample with unmeasured varaible and noise. Such situation would often lead to overfitting. In such case, we need introduce regularization: <span class="math display">\[R_{srm}(\theta) = R_{emp} + \lambda J(\theta)\]</span> where <span class="math inline">\(J(\theta)\)</span> is a function represent the complexity of the model, and <span class="math inline">\(\lambda\)</span> is a penalized parameter used the control the degree of regularization</p><p>When we use structural risk as our objective function, we call the learning principle of the machine learning "SRM"</p><p>For a probability model, under some condition, we can consider SRM as equivalent to a <strong>Maximum-A-Posterior</strong>(MAP). Refer to another article about parameter estimation</p><h2 id="objective-function-for-unsupervised-learning">4. Objective function for Unsupervised Learning</h2><p>[ongoing]</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Learning Principle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Basic knowledge about Machine Learning(Supervised Learning)</title>
    <link href="/2022/09/11/basic_knowledgefor_ML/"/>
    <url>/2022/09/11/basic_knowledgefor_ML/</url>
    
    <content type="html"><![CDATA[<h1 id="machine-learning">Machine Learning</h1><h2 id="about-machine-learning">1.About Machine Learning</h2><h3 id="what-is-machine-learning">1.1 What is Machine Learning</h3><p>The definition about machine learning can be expressed as:</p><p>Suppose we want a model to perform a task T, and we have a approach E to evaluate the goodness the model complete this task. If we improve E through updating an optimization algorithm O using an observational dataset D, we can call such process a machine learning process</p><h3 id="three-components-of-machine-learning">1.2 Three components of Machine Learning</h3><p><strong>Model</strong></p><p>A model is a learnner trying to solve the task we want to perform. In most cases, that task would fitting a real mapping between variables. Thus, a model can usually be noted as <span class="math inline">\(f(x,\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the parameter of the model, and the fitting task is to find the best <span class="math inline">\(\theta\)</span> according to the training dataset.</p><p>As me may not know the sepcific form of the real mapping, we need to give the algorithm a scope for leaning, we call such scope the Hypothesis Space, noted as F. For all potential fitted mapping f , <span class="math inline">\(f\in F\)</span></p><p><strong>Learning Principle</strong></p><p>To find the best <span class="math inline">\(\theta\)</span>, we need a principle to judge how good a <span class="math inline">\(\theta\)</span> is. This principle usually involve the definition and calculation of a Loss function</p><p><strong>Optimization Algorithm</strong></p><p>After confirming the training dataset, hypothesis space and learning principle, the task of finding the best parameters become a optimization problem. An optimization algorithm is a solver to solve such problem</p><h3 id="proprocess-of-the-data">1.3 Proprocess of the data</h3><h3 id="evaluation-of-the-model">1.4 Evaluation of the model</h3><h2 id="supervised-machine-learning">2. Supervised Machine Learning</h2><p>Supervised learning is a kind of machine learning where the output variable of the model is clearly labeled in the dataset. That is to say, we would have a real outcome <span class="math inline">\(Y_{true}\)</span> and a predicted outcome <span class="math inline">\(Y_{pred}\)</span></p><h3 id="joint-probability-distribution"><strong>2.1 Joint Probability Distribution</strong></h3><p>Joint probability is the probability that multiple conditions are satisfied same time, noted as <span class="math inline">\(P(X,Y)\)</span></p><p>the relationship among joint probability, conditional probability and edge probability are: <span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\\P(Y) = \sum_{i=1}^N P(X,Y)P(X)\]</span> For two independent variable: <span class="math display">\[P(X, Y) = P(X)P(Y)\\P(Y|X) = P(Y)\]</span></p><p>The fundamental hypothesis of supervised machine learning is the existence of the Joint Probability Distribution of input variable X and output variable Y</p><p>The task we want a supervised machine learning algorithm to perform is to imitate the real JPD <span class="math inline">\(P(X,Y)\)</span>, so that we can calculate <span class="math inline">\(P(Y| X)\)</span> when input X is given</p><h3 id="category-of-supervised-machine-learning">2.2 Category of Supervised Machine Learning</h3><p><strong>Probability model v.s. Non-probability model</strong></p><p>The Non-Probability model try to learn the mapping relationship f directly. Usually it confirm a hypothesis space according to prior knowledge(e.g a linear space ). For example, KNN, SVM, NN are all non-probability model</p><p>The probability model try to directly learn one or more of <span class="math inline">\(P(X,Y)\)</span> , <span class="math inline">\(P( X|Y )\)</span> and <span class="math inline">\(P(Y)\)</span></p><p>Usually, it would pre-decide the distribution form of P(Y|X) or P(Y), for example, the logistic regression suppose P(Y| X) follow a Bernoulli Distribution. Logistic Regression, Naive Bayes are probability model</p><p><strong>Parameter model v.s. Non-parameter model</strong></p><p>In statistics, a parameter estimation means an estimation with given hypothesis on the distribution of the whole population, while a non-parameter estimation does not have such hypothesis</p><p>For machine learning:</p><p>A parameter model means you have an explicit hypothesis on the mapping or the probability distribution, like liner regression, logistic regression, naive bayes(limited dimention of <span class="math inline">\(\theta\)</span>) or MLP. The advantage of such kind of models is, if the hypothesis is correct, the model can be fit with very small dataset. However, if the hypothesis is incorrect, no matter how large the dataset is, there would still be inevitable bias</p><p>A non-parameter model means you put no or limited hypothesis on the mapping or distribution, like KNN, tree-based model, SVM(non-linear). The cost of storage and calculation of non-parameter model would be bigger, but theoretically, a non-parameter can fit any complicated mapping as long as we have enough data. Usually a non-parameter model has a few hyperparameters and infinite parameters</p><p><strong>Discriminative model v.s. Generative model </strong></p><p>A discriminative model directly model on Y:</p><ul><li>All non-probability models are discriminative models</li><li>If a probability models try to directly learn <span class="math inline">\(P(Y|X)\)</span>, it is a discriminative model</li></ul><p>including most machine learning model like MLP, logistic regression, decision tree, KNN and SVM</p><p>A Generative model try to induce <span class="math inline">\(P( X| Y)\)</span> through learning P(X, Y) and P(Y), with <span class="math display">\[P(X,Y) = P(X| Y )P(Y)\]</span></p><p><span class="math display">\[P(Y|X) = \frac{P(X,Y)}{P(X)}\]</span> including naive bayes，GMM</p><h2 id="unsupervised-machine-learning">3. Unsupervised Machine Learning</h2><p>Unsupervised learning is a kind of machine learning where the output variable of the model is not labeled in the dataset. Typically we can separate unsupervised machine learning algorithm into serval types according to the task we want it to perform</p><h3 id="feature-learning">3.1 Feature learning</h3><p>Mining useful expression or combination of features in unlabeled dataset. Usually applied in dimensionality reduction or visualization, including:</p><ul><li>PCA , T- SNE, SVD</li><li>Sparse Encoding, Auto-encoder ,Denoising Autoencoder</li></ul><h3 id="probabilistic-density-estimation">3.2 Probabilistic Density Estimation</h3><p>Induce the probability density function of a variable through observational data, can be classified as:</p><ul><li>Parametric Density Estimation: have prior hypothesis on the distribution form of the variable,including MLE, MAP etc.</li><li>Non-parametric Density Estimation: do not have a prior hypothesi, including histogram, Kernel Density Estimation(KDE) etc.</li></ul><h3 id="clustering">3.3 Clustering</h3><p>Segment unlabeled dataset into different groups. Including K-Means, DBscan, hierarchical clustering etc.</p><h3 id="other-unsupervised-learning">3.4 Other Unsupervised Learning</h3><p>There are lots of emerging unsupervised learning algorithm like pred-Net, GAN etc. These algorithm would be elaborated in single section in other articles</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hypothesis Testing for Data Science</title>
    <link href="/2022/03/25/Hypothesis-Testing/"/>
    <url>/2022/03/25/Hypothesis-Testing/</url>
    
    <content type="html"><![CDATA[<h1 id="hypothesis-testing">Hypothesis Testing</h1><h2 id="terms-and-definition">1. Terms and Definition</h2><h3 id="terms-in-hypothesis-testing">1.1 Terms in Hypothesis Testing</h3><table><colgroup><col style="width: 32%"><col style="width: 67%"></colgroup><thead><tr class="header"><th style="text-align: center;">Term</th><th style="text-align: center;">Meaning</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Hypothesis</td><td style="text-align: center;">A claim to test</td></tr><tr class="even"><td style="text-align: center;">Null Hypothesis (<span class="math inline">\(H_0\)</span>)</td><td style="text-align: center;">Currently accepted value for a parameter(e.g diff = 0)</td></tr><tr class="odd"><td style="text-align: center;">Alternative Hypothesis(<span class="math inline">\(H_a\)</span>)</td><td style="text-align: center;">The claims to be tested. <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span> are mathematically opposites</td></tr><tr class="even"><td style="text-align: center;">Test Outcomes</td><td style="text-align: center;">Reject <span class="math inline">\(H_0\)</span> or fail to reject <span class="math inline">\(H_0\)</span></td></tr><tr class="odd"><td style="text-align: center;">Test Statistics</td><td style="text-align: center;">Statistics calculated from the data samples used to decide whether to reject <span class="math inline">\(H_0\)</span></td></tr><tr class="even"><td style="text-align: center;">Big/Small Sample</td><td style="text-align: center;">sample size -&gt; inf / sample size is fixed, usually set threshold to 30</td></tr><tr class="odd"><td style="text-align: center;">Central Limit Theorem</td><td style="text-align: center;">A theorem that indicates no matter what distribution the total population is, when sample size n is big enough, the average of a statistics of a sample <span class="math inline">\(\bar{X}\)</span> follows a normal distribution N(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\frac{\sigma^2}{n}\)</span>). This theorem allows implementation of T- test on average of continuous test statistics, like average salary of a department</td></tr><tr class="even"><td style="text-align: center;">Degree of Freedom</td><td style="text-align: center;">The number of samples(n) -1</td></tr><tr class="odd"><td style="text-align: center;">Effect Size</td><td style="text-align: center;">The degree the test statistics differ from the accepted value</td></tr><tr class="even"><td style="text-align: center;">Significance Level(<span class="math inline">\(\alpha\)</span>)</td><td style="text-align: center;">A decided threshold of <span class="math inline">\(\alpha\)</span>. <span class="math inline">\(\alpha\)</span> is the probability of rejecting <span class="math inline">\(H_ 0\)</span> when <span class="math inline">\(H_ 0\)</span> is ture(Type I Error)</td></tr><tr class="odd"><td style="text-align: center;">Confidence Level</td><td style="text-align: center;">How confident we are to reject <span class="math inline">\(H_0\)</span> (1-<span class="math inline">\(\alpha\)</span>)</td></tr><tr class="even"><td style="text-align: center;">p value</td><td style="text-align: center;">The probability that the observed statistics significance are caused by random factor</td></tr><tr class="odd"><td style="text-align: center;">statistical power(<span class="math inline">\(1-\beta\)</span>)</td><td style="text-align: center;"><span class="math inline">\(\beta\)</span> is the the probability of accepting <span class="math inline">\(H_ 0\)</span> when <span class="math inline">\(H_ 0\)</span> is False(Type II Error). Typically we need the power of a test to be greater than 80%</td></tr><tr class="even"><td style="text-align: center;">Confidence Interval</td><td style="text-align: center;">With <span class="math inline">\(\alpha\)</span> as significance level, <span class="math inline">\(\theta\)</span> as test statistics, if <span class="math inline">\(P\{ \theta_n &lt; \theta &lt; \theta_m \} \ge 1-\alpha\)</span>, then we call <span class="math inline">\((\theta_n , \theta_m)\)</span> the confidence interval of <span class="math inline">\(\theta\)</span> uhder the significance level <span class="math inline">\(1-\alpha\)</span></td></tr><tr class="odd"><td style="text-align: center;">One-tail Test/Two-tail Test</td><td style="text-align: center;">In one-tail test, the <span class="math inline">\(H_a\)</span> has direction, so <span class="math inline">\(H_a\)</span> would be like <span class="math inline">\(\mu &gt; x\)</span>, and we only focus on one side of the rejection area in this case. While in two tail test, <span class="math inline">\(H_a\)</span> would be like <span class="math inline">\(\mu \ne x\)</span></td></tr><tr class="even"><td style="text-align: center;">Rejection Area</td><td style="text-align: center;">Let the the area of PDF on <span class="math inline">\((-inf,z_1],[z_2,inf)] = \alpha\)</span>, then this area is called rejection area, <span class="math inline">\(b_0,b_1\)</span> are called rejection boundaries, which are the z-score when the area one the left/right side is <span class="math inline">\(\frac{\alpha}{2}\)</span>. When the test statistics fall outside the rejection boundaries, we can reject <span class="math inline">\(H_0\)</span> under the Level of Significance</td></tr><tr class="odd"><td style="text-align: center;">MDE</td><td style="text-align: center;">The minimum detectable effect size of a experiment when <span class="math inline">\(1-\alpha\)</span> and <span class="math inline">\(\beta\)</span> is given. If detected effect size <span class="math inline">\(d &lt; mde\)</span>, it might be caused by random factor, and we cannot reject <span class="math inline">\(H_ 0\)</span></td></tr></tbody></table><p><img src="/2022/03/25/Hypothesis-Testing/img1.png"></p><p><strong>Power and Error</strong></p><p><img src="/2022/03/25/Hypothesis-Testing/img3.png"></p><h3 id="formula">1.2 Formula</h3><p>Calculation of effect size:</p><ol type="1"><li><p><strong>Cohen's d:</strong> describe the difference of average variables of two group <span class="math display">\[d = \frac{\mu_A - \mu_B }{\sigma_{pooled}}\\\sigma_{pooled} = \sqrt{\frac{(n_1-1)s_ 1^2+(n_2-1)s_ 2^2}{(n_ 1-1)+(n_ 2-1)}}\]</span></p></li><li><p><strong>Cramer's V</strong>: describe the correction of categorical variable <span class="math display">\[V = \sqrt{\frac{\chi^2/n}{min(c- 1,r-1)}}\]</span></p></li><li></li><li><p>Cohen's h: decribe the differnce of ratio variable of two groups <span class="math display">\[h = 2(arcsin \sqrt{p1} - arcsin\sqrt{p2})\]</span></p></li></ol><p>Caluclation of MDE <span class="math display">\[MDE = (t_{\frac{1-\alpha}{2}} + t_ {1-\beta})\sqrt{\frac{\sigma_1^2}{n_ 1} + \frac{\sigma_2^2}{n_ 2}}\]</span></p><h2 id="type-of-hypothesis-testing">2. Type of Hypothesis Testing</h2><h3 id="four-common-types-of-experiment">2.1 Four Common Types of Experiment</h3><p><strong>Z-test</strong>:</p><ol type="1"><li>Test statistics should be average value or ratio</li><li>The variance of the population should be given</li><li>If the population does not follow Normal distribution, the sample size should be big</li></ol><p>In most application, the variance of the population is unknown. In addition,we usually cannot estimate the distribution of the total population, which requires the sample size of a Z-test to be big. Thus, z-test is not frequently used comparing to t-test</p><p><strong>T- test</strong></p><p>T- test is used to examine</p><ol type="1"><li>whether the accepted average or ratio of a population is correct(One sample)</li><li>Whether there's a difference on a statistics between two group(Two sample)</li></ol><p>The following condidtions should be guaranteed:</p><ol type="1"><li>Test statistics sould be average value or ratio</li><li>The variance of the population is not needed</li><li>If the population does not follow Normal distribution, the sample size should be big</li><li>The variance of the populations should be same</li></ol><p><strong>F-test</strong>:</p><p>F-test is used to examine</p><ol type="1"><li>whether the standard deviation of two or more population are same</li><li>whether a categorical variable has an effect on the average of a population</li></ol><p>The following condidtions should be guaranteed:</p><ol type="1"><li>The two population should both follows normal distribution</li><li>The variance of the populations should be same</li></ol><p><strong>chi-test</strong></p><p>chi-squared test is used to examine:</p><ol type="1"><li>Whether the observed frequency of a variable in the sample is aligned with the expected frequency</li><li>Whether two categorical variables are correlated</li></ol><p>The following condidtions should be guaranteed:</p><ol type="1"><li>Test statistics should be frequency or categorical variable</li><li>Non-parameter testing. The population do not need to follow normal distribution</li></ol><h3 id="how-to-choose-experiment-type">2.2 How to choose Experiment Type</h3><p><img src="/2022/03/25/Hypothesis-Testing/img4.png" style="zoom:150%;"></p><p>where:</p><ul><li><span class="math inline">\(\bar{x}\)</span> is the mean of the sample</li><li><span class="math inline">\(\mu_0\)</span> is the mean of population in <span class="math inline">\(H_0\)</span></li><li>s is the standard deviation of sample</li><li><span class="math inline">\(\sigma\)</span> is the standard deviation of population</li><li>n is the number of samples</li></ul><h2 id="zt-test">3. Z/T-test</h2><h3 id="one-sample-test">3.1 One Sample Test</h3><p>Objective: to test whether the accepted mean or ratio of a population is correct through a sample</p><h4 id="parameter-test">3.1.1 Parameter Test</h4><p>To test whether the accepted mean is correct</p><h5 id="big-sample"><strong>3.1.1.1 Big sample</strong></h5><p>When the sample is big(<span class="math inline">\(n\ge30\)</span>), we can assume the sample ~ Norm(<span class="math inline">\(\mu,\sigma\)</span>), thus we can use z-test</p><p><strong>Make hypothesis:</strong></p><p>Two-tail: <span class="math inline">\(H_0: \mu = \mu_0\)</span>, <span class="math inline">\(H_1:\mu \ne \mu_0\)</span></p><p>One-tail(left side): <span class="math inline">\(H_0: \mu \ge \mu_0\)</span>, <span class="math inline">\(H_1:\mu \lt \mu_0\)</span></p><p><strong>Construct test statistics</strong></p><p><span class="math inline">\(\sigma\)</span> known: <span class="math display">\[z = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}\]</span></p><p><span class="math inline">\(\sigma\)</span> unknown: <span class="math display">\[z = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}\]</span> where:</p><ul><li><span class="math inline">\(\bar{x}\)</span> is the mean of the sample</li><li><span class="math inline">\(\mu_0\)</span> is the mean of population in <span class="math inline">\(H_0\)</span></li><li>s is the standard deviation of sample</li><li><span class="math inline">\(\sigma\)</span> is the standard deviation of population</li><li>n is the number of samples</li></ul><p><strong>Rejection condition</strong>:</p><p>Two-tail:</p><ol type="1"><li><span class="math inline">\(|z| &gt; z_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|z-z_0| &gt; mde\)</span></li></ol><p>One-tail(left side):</p><ol type="1"><li><span class="math inline">\(z &lt; -z_{\alpha}\)</span></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|z-z_0| &gt; mde\)</span></li></ol><h5 id="small-sample"><strong>3.1.1.2 Small sample</strong></h5><p>When the sample is small(<span class="math inline">\(n\lt30\)</span>), we can assume the sample ~ t distribution</p><p><strong>Make hypothesis:</strong></p><p>Two-tail: <span class="math inline">\(H_0: \mu = \mu_0\)</span>, <span class="math inline">\(H_1:\mu \ne \mu_0\)</span></p><p>One-tail(left side): <span class="math inline">\(H_0: \mu \ge \mu_0\)</span>, <span class="math inline">\(H_1:\mu \lt \mu_0\)</span></p><p><strong>Construct test statistics</strong></p><p><span class="math inline">\(\sigma\)</span> known: <span class="math display">\[t = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}\]</span></p><p><span class="math inline">\(\sigma\)</span> unknown: <span class="math display">\[t = \frac{\bar{x}-\mu_0}{s/\sqrt{n}}\]</span> where:</p><ul><li><span class="math inline">\(\bar{x}\)</span> is the mean of the sample</li><li><span class="math inline">\(\mu_0\)</span> is the mean of population in <span class="math inline">\(H_0\)</span></li><li>s is the standard deviation of sample</li><li><span class="math inline">\(\sigma\)</span> is the standard deviation of population</li><li>n is the number of samples</li></ul><p><strong>Rejection condition</strong>:</p><p>Two-tail:</p><ol type="1"><li><span class="math inline">\(|t| &gt; t_{\frac{\alpha}{2}}\)</span><br></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|t-t_0| &gt; mde\)</span></li></ol><p>One-tail(left side):</p><ol type="1"><li><span class="math inline">\(t &lt; -t_{\alpha}\)</span></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li><span class="math inline">\(|t-t_0| &gt; mde\)</span></li></ol><h4 id="ratio-test">3.1.2 Ratio Test</h4><p>To test whether the accepted ratio is correct. The following step is for big sample(n$$5), for small sample, replace z-score with t-score.</p><p><strong>Make hypothesis:</strong></p><p>Two-tail: <span class="math inline">\(H_0: \pi = \pi_0\)</span>, <span class="math inline">\(H_1:\pi \ne \pi_0\)</span></p><p>One-tail(left side): <span class="math inline">\(H_0: \pi \ge \pi_0\)</span>, <span class="math inline">\(H_1:\pi \lt \pi_0\)</span></p><p><strong>Construct test statistics</strong> <span class="math display">\[z = \frac{p-\pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}}\]</span> where</p><ul><li><p>p is the ratio of a kind of observations in the sample and the whole sample</p></li><li><p><span class="math inline">\(\pi\)</span> is the ratio of a kind of case in the population and the whole population</p></li><li><p>n is the size of sample</p></li></ul><p><strong>Rejection condition</strong>:</p><p>Two-tail: <span class="math inline">\(|z| &gt; z_{\frac{\alpha}{2}}\)</span> or <span class="math inline">\(p &lt; \alpha\)</span></p><p>One-tail(left side): <span class="math inline">\(z &lt; -z_{\alpha}\)</span> or <span class="math inline">\(p &lt; \alpha\)</span></p><h3 id="two-sample-test">3.2 Two Sample Test</h3><p>Objective: to test whether a condition would effect a metric through following one group before and after experiment or comparing two groups</p><h4 id="parameter-testmatch">3.2.1 Parameter Test(Match)</h4><p>Testing whether a condition would effect a metric through following one group before and after experiment. In this case, we assume <span class="math inline">\(n_1=n_2, \sigma_1 = \sigma_2\)</span></p><p><strong>Make hypothesis:</strong></p><p>Two-tail: <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span>, <span class="math inline">\(H_1:\mu_1 \ne \mu_2\)</span></p><p>One-tail(left side): <span class="math inline">\(H_0: \mu_1 \ge \mu_2\)</span>, <span class="math inline">\(H_1:\mu_1 \lt \mu_2\)</span></p><p><strong>Construct test statistics</strong></p><p><span class="math inline">\(\sigma\)</span> known: <span class="math display">\[z = \frac{(\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2)}{\sigma\sqrt{\frac{2}{n}}} = \frac{\bar{d}}{\sigma\sqrt{\frac{2}{n}}}\]</span></p><p><span class="math inline">\(\sigma\)</span> unknown: <span class="math display">\[t = \frac{\sqrt{n}((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{s_d} = \frac{\sqrt{n}\bar{d}}{s_d}\]</span> where:</p><ul><li><span class="math inline">\(\bar{x_1},\bar{x_2}\)</span> is the mean of the sample before and after experiment</li><li><span class="math inline">\(\mu_1 - \mu_2\)</span> is 0 under <span class="math inline">\(H_0\)</span></li><li><span class="math inline">\(\bar{d} = \bar{x_1}-\bar{x_2}\)</span></li><li><span class="math inline">\(\sigma\)</span> is the standard deviation of the population</li><li><span class="math inline">\(s_d\)</span> is the standard deviation of <span class="math inline">\(\bar{d}\)</span></li><li>n is the size of the sample</li></ul><p><strong>Rejection condition</strong>:</p><p><span class="math inline">\(\sigma\)</span> known(z-test):</p><p>One-tail(left side):</p><ol type="1"><li><span class="math inline">\(t &lt; -t_{\alpha}\)</span></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li>effect size &gt; mde</li></ol><p><span class="math inline">\(\sigma\)</span> unknown(t-test):</p><p>replace z-score with t score</p><h4 id="parameter-testindependent">3.2.2 Parameter Test(Independent)</h4><p>Testing whether a condition would effect a metric through comparing two group.</p><p><strong>Make hypothesis:</strong></p><p>Two-tail: <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span>, <span class="math inline">\(H_1:\mu_1 \ne \mu_2\)</span></p><p>One-tail(left side): <span class="math inline">\(H_0: \mu_1 \ge \mu_2\)</span>, <span class="math inline">\(H_1:\mu_1 \lt \mu_2\)</span></p><p><strong>Construct test statistics</strong></p><p><span class="math inline">\(\sigma\)</span> known,and <span class="math inline">\(\sigma_1 = \sigma_2\)</span>:</p><p>z-score is same as Format(3), but replace <span class="math inline">\(s_p\)</span> with <span class="math inline">\(\sigma\)</span></p><p><span class="math inline">\(\sigma\)</span> known,and <span class="math inline">\(\sigma_1 = \sigma_2\)</span>:</p><p>z-score is same as Format(4), but replace <span class="math inline">\(s_1,s_2\)</span> with <span class="math inline">\(\sigma_1,\sigma_2\)</span></p><p><span class="math inline">\(\sigma\)</span> unknown,and <span class="math inline">\(\sigma_1 = \sigma_2\)</span>: <span class="math display">\[t = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\\s_p = \sqrt{\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}}\]</span> <span class="math inline">\(\sigma\)</span> unknown,and <span class="math inline">\(\sigma_1 \ne \sigma_2\)</span>: <span class="math display">\[t = \frac{((\bar{x_1}-\bar{x_2}) - (\mu_1-\mu_2))}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}\]</span> where</p><ul><li><span class="math inline">\(\bar{x_1},\bar{x_2}\)</span> is the mean of the sample before and after experiment</li><li><span class="math inline">\(\mu_1 - \mu_2\)</span> is 0 under <span class="math inline">\(H_0\)</span></li><li><span class="math inline">\(\sigma_1,\sigma_2\)</span> is the standard deviation of the population 1 and population 2</li><li><span class="math inline">\(s_1,s_2\)</span> is the standard deviation of the sample 1 and sample 2</li><li>n is the size of the sample</li></ul><p><strong>Rejection condition</strong>:</p><p><span class="math inline">\(\sigma\)</span> known(z-test):</p><p>One-tail(left side):</p><ol type="1"><li><span class="math inline">\(t &lt; -t_{\alpha}\)</span></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li>effect size &gt; mde</li></ol><p><span class="math inline">\(\sigma\)</span> unknown(t-test):</p><p>replace z-score with t score</p><h4 id="ratio-test-1">3.2.3 Ratio Test</h4><p>To test whether the change of a condition has an effect on ratio. The following step is for big sample(n$$5), for small sample, replace z-score with t-score.</p><p><strong>Make hypothesis:</strong></p><p>Two-tail: <span class="math inline">\(H_0: \pi_1 = \pi_2\)</span>, <span class="math inline">\(H_1:\pi_1 \ne \pi_2\)</span></p><p>One-tail(left side): <span class="math inline">\(H_0: \pi_1 \ge \pi_2\)</span>, <span class="math inline">\(H_1:\pi_1 \lt \pi_2\)</span></p><p><strong>Construct test statistics</strong> <span class="math display">\[z = \frac{(p_1-p_2) - (\pi_1 - \pi_2)}{\sqrt{p(1-p)(\frac{1}{n_1}+\frac{1}{n_2})}}\\p = \frac{p1*n1+p_2*n_2}{n_1+n_2}\]</span> where</p><ul><li><span class="math inline">\(p_1,p_2\)</span> is the ratio of sample 1 and sample 2</li><li><span class="math inline">\(\pi_1,\pi_2\)</span> is the ratio of population 1 and population 2</li><li><span class="math inline">\(\pi_1 - \pi_2\)</span> equals 0 under Null Hypothesis</li></ul><p><strong>Rejection condition</strong>:</p><ol type="1"><li><span class="math inline">\(t &lt; -t_{\alpha}\)</span></li><li><span class="math inline">\(p &lt; \alpha\)</span></li><li>effect size &gt; mde</li></ol><h3 id="sample-size-and-duration">3.3 Sample size and Duration</h3><h4 id="sample-size">3.3.1 Sample Size</h4><p>In A/B testing, most of time, you can calculate the sample size N using the following equation: <span class="math display">\[N = \frac{(t_{(1-\alpha)/2} + t_{1-\beta})^2\sigma^2}{mde^2}\]</span> Where:</p><ul><li><p><span class="math inline">\(\sigma\)</span> is the standard error of the two sample</p></li><li><p>N is the sample size of <strong>one sample</strong>, the total sample size is 2N</p></li><li><p>The greater the mde is(which means the sensitivity of a experiment is lower), the smaller sample size is need</p></li><li><p>Note: the mde in the formular is the expected mde to calculated the minimum sample size. The actual mde of a experiment should be calculated after the experiment is launched by the following formula: <span class="math display">\[MDE = (t_{(1-\alpha)/2} + t_{1-\beta})\sqrt{\frac{\sigma_ 1}{n_ 1}+\frac{\sigma_ 2}{n_ 2}}\]</span></p></li></ul><p><a href="https://www.evanmiller.org/ab-testing/">here</a> are some useful tools for calculate the sample size A/B</p><h4 id="duration">3.3.2 Duration</h4><p>You can calculate the duration by calculating sample size/daily processed samples. For example, If the sample size of a web A/B test is 1000, and the daily view of the webpage is 500, than you can run it foe two days. However, in real world problems, this calculation is not reliable, as there might be other factors like holiday or events that can affect the experiment. Thus, the specific process to decide the duration of a test remain to be discussed by experts.</p><h2 id="f-test">4. F-test</h2><h3 id="equality-of-variances">4.1 Equality of Variances</h3><p>Through F-test, we can examine the equality of variance</p><p>Let <span class="math inline">\(X_1,X- 2\)</span> be two independent variables, where: <span class="math display">\[X_1~N(\mu_1,\sigma_1^2),  X_2~N(\mu_2,\sigma_2^2)\]</span> Get a sample from each variable: <span class="math inline">\(x_1, x_2\)</span>, the sample sizes are <span class="math inline">\(n_1,n_2\)</span></p><p>Let <span class="math inline">\(\bar{x}_1,\bar{x}_2\)</span> be the mean of the samples, <span class="math inline">\(s_1,s_2\)</span> be the standard error of the samples</p><p>Set the null hypothesis and alternative hypothesis of Experiment: <span class="math display">\[H_ 0: \sigma_1^2 = \sigma_2^2\\H_ 1: \sigma_1^2 \neq \sigma_2^2\]</span> If it is a one-tail experiment: <span class="math display">\[H_ 0: \sigma_1^2 &lt; \sigma_2^2\\H_ 1: \sigma_1^2 \ge \sigma_2^2\]</span></p><p>Construct F statistics: <span class="math display">\[F(n_1-1,n_2-1) = \frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}=\frac{s_ 1^2}{s_2^2}\]</span> Note: As a convention, we normally select the greater s as <span class="math inline">\(s_ 2\)</span> to let f score &lt;1</p><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span></p><p>If it is a one-tail experiment, then reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(F &lt; F_{1-\alpha}\)</span></p><h3 id="single-factor-anova">4.2 Single Factor ANOVA</h3><p>We can use f- test to examine the impact of a factor to an indicator by judging if the indicator is same when the factor is set to different value</p><p>Let the factor be Y, indicator be x</p><p>Suppose we got the following observations:</p><table><thead><tr class="header"><th>Y = Y1</th><th>Y=y2</th><th>y=y3</th><th>y=y4</th></tr></thead><tbody><tr class="odd"><td>x=1</td><td>x=2</td><td>...</td><td>...</td></tr><tr class="even"><td>x=3</td><td>x=3</td><td>...</td><td>...</td></tr><tr class="odd"><td>x=2</td><td>x=4</td><td>...</td><td>...</td></tr><tr class="even"><td>x=6</td><td>x=6</td><td>...</td><td>...</td></tr></tbody></table><p>Let k = number of groups(number of different values of Y)</p><p>Let <span class="math inline">\(n_i\)</span> = the number of samples in <span class="math inline">\(i_{th}\)</span> group</p><p>Let n = <span class="math inline">\(max(n_ k)\)</span></p><p>Construct <span class="math display">\[F = \frac{SSA/df1}{SSE/df2}\]</span> where:</p><ul><li><p>df1 = k - 1</p></li><li><p>df2 = n- k</p></li><li><p>SSA is Sum of Square Between Groups:</p><p><span class="math display">\[SSA = \sum_{i=1}^kn_i(\bar{x_i}-\bar{x})\]</span> Where:</p><ul><li><span class="math inline">\(\bar{x_i}\)</span> is the average of the <span class="math inline">\(i_{th}\)</span> group</li><li><span class="math inline">\(\bar{x}\)</span> is the average of all <span class="math inline">\(\bar{x_i}\)</span></li></ul></li><li><p>SSE is the Sum of square error <span class="math display">\[SSE = \sum_{i=1}^k(n_i-1)s_i^2\]</span> Where:</p><ul><li><span class="math inline">\(s_1^2\)</span> is the variance(square of standard error) of the <span class="math inline">\(i_{th}\)</span> group</li></ul></li></ul><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span>(the factor do has an impact)</p><h3 id="exam-on-the-significance-of-the-linear-regression">4.3 Exam on the significance of the Linear Regression</h3><p>We can use f-test to examine on whether a liner model(linear hypothesis) fit a problem well</p><p>Suppose we got the following linear hypothesis function: <span class="math display">\[y = \beta_1x_1 + \beta_2x_ 2 + \beta_ 0\]</span> Define SSR as: <span class="math display">\[SSR = ||\hat{y}-\bar{y}1_n||^2\]</span> Where:</p><ul><li><span class="math inline">\(\hat{y}\)</span> is the prediction of y</li><li>{ y } is the average of true y</li><li>1n is a vector with all one</li><li></li></ul><p>Define SSE as: <span class="math display">\[SSR = ||y- \hat{y}||^2\]</span> Construct F statistics as: <span class="math display">\[F = \frac{SSR/p}{SSE/(n-p-1)}\]</span> where:</p><ul><li><p>n is number of samples</p></li><li><p>p is number of variables(x)</p></li></ul><p>Look up the F score table, if <span class="math inline">\(F &gt; F_{\alpha}\)</span>, reject <span class="math inline">\(H_0\)</span>. In such case, the linear model is significant to the change of the x and y, thus the linear hypothesis is acceptable. Otherwise, consider a non-linear model</p><h2 id="chi-squared-test">5. Chi-squared Test</h2><h4 id="chi-squared-test-for-independence">5.1 Chi-squared Test for Independence</h4><p>chi-squared test is a <strong>supervised</strong> hypothesis testing method to calculate the probability that 2 categorical variables are correlated</p><ol type="1"><li><p>According to the observations, make the frequency table</p></li><li><p>calculate the row total and the column total</p></li><li><p>calculate the expectations for each cell by <span class="math inline">\(\frac{R_i*C_j}{total}\)</span> (<span class="math inline">\(\frac{125*310}{600} = 65\)</span>,etc.)</p></li><li><p>the chi2 of each cell and the total chi2 are given by <span class="math display">\[\chi_{i,j}^2 = \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\\chi^2 = \sum_{i,j}^{r,c}\chi_{i,j}^2\]</span></p></li><li><p>calculate the degree of freedom, <span class="math inline">\(k=(r-1)(c-1)\)</span></p><table><thead><tr class="header"><th>movie type</th><th>High POPULARITY</th><th>low POPULARITY</th><th>Row total</th></tr></thead><tbody><tr class="odd"><td>type1</td><td>50(65)</td><td>75(60)</td><td>125</td></tr><tr class="even"><td>type2</td><td>125(155)</td><td>175(145)</td><td>300</td></tr><tr class="odd"><td>type3</td><td>...</td><td>...</td><td>...</td></tr><tr class="even"><td>type4</td><td>...</td><td>...</td><td>...</td></tr><tr class="odd"><td>column total</td><td>310</td><td>290</td><td>600</td></tr></tbody></table></li><li><p>check the significance table of chi distribution to find the value of the random variable under given k and <span class="math inline">\(\alpha\)</span></p></li><li><p>compare the <span class="math inline">\(\chi^2\)</span> with the rejection area, if <span class="math inline">\(\chi^2\)</span> &gt; the found value(which means p value is smaller than <span class="math inline">\(\alpha\)</span>), then reject the null hypothesis</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest, chi2<br>X, y = data_0,target_0<br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><h3 id="chi-squared-test-for-goodness-of-fit">5.2 Chi-squared Test for Goodness of fit</h3><p>The process is basically the same. The rows represent the possible categories of a categorical variable. The columns are observed frequency and predicted frequency of each category.</p>]]></content>
    
    
    <categories>
      
      <category>Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hypothesis Testing</tag>
      
      <tag>A/B Test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Feature Selection: Statistical Method</title>
    <link href="/2022/02/09/feature-selection-stats-method/"/>
    <url>/2022/02/09/feature-selection-stats-method/</url>
    
    <content type="html"><![CDATA[<h1 id="feature-selection">Feature Selection</h1><h2 id="filter">1. Filter</h2><p>Evaluate features on divergence and correlation. set one or more thresholds and select the feature.</p><p><strong>Pro</strong>: fast, scalable, independent of the model</p><p><strong>Con</strong>: ignore the dependence of features</p><h3 id="variance">1.1 Variance</h3><ol type="1"><li>calculate the variance(not STD) of each feature</li><li>set a threshold, select all features whose variance bigger than the threshold</li></ol><figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> VarianceThreshold<br><span class="hljs-comment">#threshold 是方差的阈值</span><br><span class="hljs-comment">#返回选择后的特征</span><br>data_1 = VarianceThreshold(threshold=<span class="hljs-number">0.25</span>).fit_transform(data_0)<br></code></pre></td></tr></table></figure><h3 id="correlation">1.2 Correlation</h3><h4 id="pearson-r">1.2.1 Pearson R</h4><ol type="1"><li>calculate the correlation value between each feature and the target</li><li>select the features with top K biggest correlation value</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> pearsonr<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_pscore</span>(<span class="hljs-params">x,y</span>):</span><br>    k = np.zeros([<span class="hljs-number">4</span>,<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,x.shape[<span class="hljs-number">1</span>]):<br>        temp = pearsonr(x[:,i],y[:,<span class="hljs-number">0</span>])<br>        k[i,:] = np.array(temp)<br>    <span class="hljs-keyword">return</span> [k.T[<span class="hljs-number">0</span>],k.T[<span class="hljs-number">1</span>]]<br><span class="hljs-comment"># 下面的写法有问题理论上应该与上面的函数等价，实际上在调用np.array时，会把最内层的pearsonr系数值变成一个数组对象，导致SelectKBest无法</span><br><span class="hljs-comment"># 遍历，需要用astype把第一列转化为float，但这样很难封装进lambda函数里，不如不用，写这个的人铁nt，害我研究一个多小时</span><br><span class="hljs-comment"># get_all_pscore =  lambda X,Y:np.array(list(map(lambda x:pearsonr(x,Y),X.T))).T</span><br><br><span class="hljs-comment">#第一个参数为一个callable()，该函数以feature，target为输入，输出可以是两种：</span><br><span class="hljs-comment"># 1. 一对数组pearsonr系数-p值（可以是tuple中两个数组，也可以是ndarray，总之第一个维度须为2</span><br><span class="hljs-comment"># 2. 一个单一的数组（只有相关系数）</span><br><span class="hljs-comment"># 第二个参数k，代表选择前k个</span><br>data_1 = SelectKBest(get_all_pscore(),k=<span class="hljs-number">2</span>).fit_transform(data_0,target_0)<br></code></pre></td></tr></table></figure><p>* <em>question: what if I have multiple output feature? how to decide which one to drop?</em></p><p>* <em>Pearson R is a coefficient between [-1,1], and negative value means negative correlation, so I believe I should use abs when applying Pearson R</em></p><p>* <em>Pearson R is a method to find out the linear correlation between two variables, if two variables are non-linearly correlated, then even if the Pearson R is 0, we cannot say they are not correlated. In such case, we should use other correlation method, like distance correlation to make evaluation</em></p><h3 id="hypothesis-testing">1.3 Hypothesis Testing</h3><h4 id="chi-squared-test">1.3.1 Chi-Squared Test</h4><p>chi-squared test is a <strong>supervised</strong> hypothesis testing method to calculate the probability that 2 variables are correlated</p><ol type="1"><li><p>According to the observations, make the frequency table</p></li><li><p>calculate the row total and the column total</p></li><li><p>calculate the expectations for each cell by <span class="math inline">\(\frac{R_i*C_j}{total}\)</span> (<span class="math inline">\(\frac{125*310}{600} = 65\)</span>,etc.)</p></li><li><p>the chi2 of each cell and the total chi2 are given by <span class="math display">\[\chi_{i,j}^2 = \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\\chi^2 = \sum_{i,j}^{r,c}\chi_{i,j}^2\]</span></p></li><li><p>calculate the degree of freedom, <span class="math inline">\(k=(r-1)(c-1)\)</span></p><table><thead><tr class="header"><th>movie type</th><th>High POPULARITY</th><th>low POPULARITY</th><th>Row total</th></tr></thead><tbody><tr class="odd"><td>type1</td><td>50(65)</td><td>75(60)</td><td>125</td></tr><tr class="even"><td>type2</td><td>125(155)</td><td>175(145)</td><td>300</td></tr><tr class="odd"><td>type3</td><td>...</td><td>...</td><td>...</td></tr><tr class="even"><td>type4</td><td>...</td><td>...</td><td>...</td></tr><tr class="odd"><td>column total</td><td>310</td><td>290</td><td>600</td></tr></tbody></table></li><li><p>check the significance table of chi distribution to find the value of the random variable under given k and <span class="math inline">\(\alpha\)</span></p></li><li><p>compare the <span class="math inline">\(\chi^2\)</span>z with the rejection area, if <span class="math inline">\(\chi^2\)</span> &gt; the found value(which means p value is smaller than <span class="math inline">\(\alpha\)</span>), then reject the null hypothesis</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest, chi2<br>X, y = data_0,target_0<br>X_new = SelectKBest(chi2, k=<span class="hljs-number">2</span>).fit_transform(X, y)<br></code></pre></td></tr></table></figure><h3 id="mutual-information">1.4 Mutual Information</h3><h4 id="entropy-and-kl-divergence">1.41 Entropy and KL Divergence</h4><p>Suppose we have a variable X, the information amount of x is give by <span class="math display">\[I(X) =-\log_2p(X) =\int-\log_2p(x_i)\]</span> We want to know the average information amount X gives, so we define <strong>Entropy </strong> as: <span class="math display">\[H(X) = -\int p(X)\log_2p(X)dX\]</span> if X is continuous, or <span class="math display">\[H(X) = -\sum_i^N p(x_i)\log_2p(x_i)\]</span> if X is discrete.</p><p>Suppose we have an approximated distribution of X, p and q, and we want to know how this two distributions are similar , we can calculate the relative Entropy, or <strong>KL Divergence</strong> between two random <span class="math display">\[D_{KL}(p||q) = \sum_{i=1}^Np(xi)\log(\frac{p(x_i)}{q(x_i)})\]</span></p><h4 id="mutual-information-1">1.4.2 Mutual Information</h4><p>The mutual information is a measurement for the dependency between two variables, it is given by the KL divergence between the joint distribution of X and Y , which is p(X,Y), and the product of p(X) and p(Y). <span class="math display">\[MI(X,Y) =D_{KL}(p(x,y)||p(x)p(y)) =  \sum_y \sum_x p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\]</span> If MI is big, the two variable are strongly related, if MI = 0, which means p(x,y) = p(x)p(y), than these two variables are independent</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mutual_info_score<br>MI_matrix = np.zeros([data_and_target.shape[<span class="hljs-number">1</span>],data_and_target.shape[<span class="hljs-number">1</span>]])<br><span class="hljs-comment"># calculate the MI matrix between each two feature</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,data_and_target.shape[<span class="hljs-number">1</span>]):<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,data_and_target.shape[<span class="hljs-number">1</span>]):<br>        MI_matrix[i,j] = mutual_info_score(data_and_target[i+<span class="hljs-number">1</span>],data_and_target[j+<span class="hljs-number">1</span>])<br>df = pd.DataFrame(MI_matrix,columns=data_and_target.columns,index=data_and_target.columns)<br><span class="hljs-built_in">print</span>(df)<br></code></pre></td></tr></table></figure><h2 id="wrapper">2.Wrapper</h2><p>Through the Evaluation(like the MSE, AUC) of the model, add or drop some feature each trail.</p><p><strong>Pro</strong>: accurate</p><p><strong>Con</strong>: time-consuming</p><h4 id="recursive-feature-elimination">2.1 Recursive Feature Elimination</h4><p>Construct models, select the best features(or drop the worst feature) by the performance of the model, and than repeat this process on the rest of the features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE<br><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFECV<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> StratifiedKFold<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-comment"># REF works for all models that have a weight on features</span><br><span class="hljs-comment"># step: the number(or precentage) of feature to eliminate in each iteration</span><br>estimator = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>)<br>selector = RFE(estimator= estimator, n_features_to_select= <span class="hljs-number">3</span>,step=<span class="hljs-number">1</span>)<br>data_transformed = selector.fit_transform(data_0,target_0)<br><br><span class="hljs-comment"># REFCV</span><br><span class="hljs-comment"># REF with cross validation</span><br>cv = StratifiedKFold(n_splits=<span class="hljs-number">5</span>)<br>selector = RFECV(estimator= estimator, min_features_to_select= <span class="hljs-number">3</span>, cv=cv, step=<span class="hljs-number">1</span>)<br>data_transformed = selector.fit_transform(data_0,target_0)<br></code></pre></td></tr></table></figure><h2 id="embedded">3. Embedded</h2><p>Some models, like Lasso, Ridge, and Random Forest has method embedded in the model to evaluate features. Train these model first and than make selection of feature.</p><p><strong>Pro</strong>: fast, easy to apply</p><p><strong>Con</strong>: ignore the dependence of features</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br><br>estimator = RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>)<br><span class="hljs-comment"># threshold:</span><br><span class="hljs-comment"># threshold of the feature importance to drop</span><br><span class="hljs-comment"># if you apply l1 in your model(or use lasso), the threshold is 1e-5 by default</span><br><span class="hljs-comment"># otherwise, the threshold is mean by default, which means features with a importance less than mean importance will</span><br><span class="hljs-comment"># all by dropped</span><br>selecor = SelectFromModel(estimator,threshold=<span class="hljs-number">0.03</span>)<br>data_transformed = selecor.fit_transform(data_0,target_0)<br><br><br>estimator = LinearSVC(C=<span class="hljs-number">0.01</span>,penalty=<span class="hljs-string">&#x27;l1&#x27;</span>,dual=<span class="hljs-literal">False</span>).fit(data_0,target_0)<br><span class="hljs-comment"># prefit: whether the model given to the selector is already fit</span><br>selecor = SelectFromModel(estimator,threshold=<span class="hljs-number">0.03</span>,prefit=<span class="hljs-literal">True</span>)<br>data_transformed = selecor.transform(data_0)<br><span class="hljs-built_in">print</span>(data_transformed)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Feature Engineering</tag>
      
      <tag>Feature Selection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL 基础知识</title>
    <link href="/2022/02/09/MySQL-Basic/"/>
    <url>/2022/02/09/MySQL-Basic/</url>
    
    <content type="html"><![CDATA[<h1 id="mysql基础知识">MySQL基础知识</h1><h2 id="数据库三大范式">1. 数据库三大范式</h2><ol type="1"><li><p>数据库表中，所有字段都是不可分解的原子值，互相不依赖</p></li><li><p>所有非主键字段都依赖于主键</p><p>（学号，姓名，专业名称）其中专业名称不依赖于主键，所以不满足第二范式</p></li><li><p>所有非主键属性都直接依赖于主键</p><p>（学号，姓名，专业id，专业名称）其中包含间接依赖关系 学校-&gt;专业id-&gt;专业名称，不满足第三范式</p></li></ol><h2 id="mysql的架构">2. MySQL的架构</h2><p>MySQL可以分为应用层,逻辑层,数据库引擎层,物理层。</p><p>应用层：负责和客户端，响应客户端请求，建立连接，返回数据</p><p>逻辑层：包括SQK接口，解析器，优化器，Cache与buffer</p><p>数据库引擎层：有常见的MyISAM,InnoDB等等</p><p>物理层：负责文件存储，日志等等</p><h3 id="一条sql语句执行的过程">2.1 一条SQL语句执行的过程</h3><ol type="1"><li>客户端首先通过连接器进行身份认证和权限相关</li><li>如果是执行查询语句的时候，会先查询缓存，但MySQL 8.0 版本后该步骤移除。</li><li>没有命中缓存的话，SQL 语句就会经过解析器，分析语句，包括语法检查等等</li><li>通过优化器，将用户的SQL语句按照 MySQL 认为最优的方案去执行。</li><li>执行语句，并从存储引擎返回数据</li></ol><h3 id="buffer-与-cache-的对比">2.2 buffer 与 cache 的对比</h3><ol type="1"><li>Cache(如Qcache)一般用来把固定语句对应的结果集放在内存，目的是为了提高读取速度</li><li>buffer（如buffer pool）一般用来把页面加载到内存，每次写入时，先更新buffer pool中的日志（redo log），把要修改的页记为脏页。后台进程每隔一段时间将buffer pool中的日志进行刷盘，从而提高了磁盘IO效率，降低了大量写入带来的冲击</li></ol><h2 id="mysql的引擎">3. MySQL的引擎</h2><h3 id="mysql支持的引擎">3.1 MySQL支持的引擎</h3><table><thead><tr class="header"><th>InnoDB</th><th>MyISAM</th></tr></thead><tbody><tr class="odd"><td>支持事务</td><td>不支持</td></tr><tr class="even"><td>支持外键</td><td>不支持</td></tr><tr class="odd"><td>聚集索引</td><td>非聚集索引</td></tr><tr class="even"><td>行级锁和表级锁</td><td>表级锁</td></tr></tbody></table><p>Memory 存储引擎：</p><p>Memory存储引擎将所有数据都保存在内存，不需要磁盘 IO。支持哈希索引，因此查找速度极快。Memory 表使用表级锁，因此并发写入的性能较低。</p><h3 id="innodb的索引">3.2 InnoDB的索引</h3><p>索引是一种数据结构，用于帮助存储引擎快速的找到数据，索引存储在内存上</p><h4 id="聚集索引和非聚集索引">3.2.1 聚集索引和非聚集索引</h4><p>聚集索引中，索引和数据绑定在一起，通过查找索引可以直接返回数据。主键都是聚集索引。非聚集索引中，数据和索引分离，通过一个或多个非聚集索引的查询，查到对应某一值的主键值/地址，然后再使用主键值找到数据</p><p>有时非聚集索引无法直接找到主键值，只能查到满足非聚集索引某一组值的锁欧聚集索引，此时会回到聚集索引中根据主键值继续查找数据，这一过程称为<strong>回表查询</strong></p><h4 id="联合索引和最左匹配原则"><strong>3.2.2 联合索引和最左匹配原则：</strong></h4><p>联合索引是指对表上的多个列的关键词进行索引。</p><p>对于联合索引的查询，如果精确匹配联合索引的左边连续一列或者多列，则mysql会一直向右匹配直到遇到范围查询（&gt;,&lt;,between,like）就停止匹配。where语句中，所有命中的索引被称为匹配列，没有命中但出现在索引中的成为过滤列，所有不在索引中的成为非匹配列</p><p>例如联合索引为（user, age, id), 而范围查询中的谓语为“where id = 1 and age &gt; 10 and user = wang and sex = 'M' ”,此时：</p><ol type="1"><li>在where语句语句中查找谓语user，命中，user为匹配列</li><li>在where语句语句中查找谓语age，命中，但age为范围查询，因此age后的列无法再命中</li><li>id未命中，但出现在索引中，为过滤列</li><li>sex不在索引中，为非匹配列</li></ol><p>对于user和sex，SQL将在内存中的索引（b+树）进行查询（通过explain可查看查询方式为REF）</p><p>对于id，SQL将在符合user，age条件的子树上遍历，这样的复杂度并不降低，但由于是在内存上执行，速度仍然比回表后快（通过explain可查看查询方式为INDEX）</p><p>对于sex，SQL将先回表，返回所有满足user，age，id条件的数据结果，然后再遍历获得满足sex条件的数据结果（通过explain可查看查询方式为ALL）</p><p>Mysql会对第一个索引字段数据进行排序，在第一个字段基础上，再对第二个字段排序。</p><h3 id="索引的数据结构">3.3 索引的数据结构</h3><p>InnoDB采用的是B+Tree索引</p><p>B-Tree：一种自平衡多插树。每个节点都存储key和value。因为每个节点都有数据，所以查询效率较高</p><p>B+Tree：也是自平衡多叉树，但中间节点不存放数据只存放key。只在叶节点存放数据，结构矮胖，出度更大，可以在相同的磁盘空间下容纳更多数据。且在叶节点之间链指针，所以进行范围查询时只需要遍历叶节点即可。</p><p>Hash索引：哈希索引对于每一行数据计算一个哈希码，并将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。只有 Memory 引擎显式支持哈希索引。</p><p><strong>为何不使用红黑树：</strong></p><p>红黑树是二叉查找树，出度为2，因此红黑树存储一张表，高度会比B Tree大很多，IO次数多，检索时间长</p><h2 id="mysql的事务">4. MySQL的事务</h2><h3 id="acid">4.1 ACID</h3><p>事务满足如下几个特性：</p><ul><li><p>原子性（Atomicity）: 一个事务中的所有操作要么全部完成，要么全部不完成。</p></li><li><p>一致性（Consistency）: 事务执行前后数据库的状态保存一致。</p></li><li><p>隔离性（Isolation） 多个并发事务对数据库进行操作，事务间互不干扰。</p></li><li><p>持久性（Durability） 事务执行完毕，对数据的修改是永久的，即使系统故障也不会丢失</p></li></ul><h3 id="事务的并发问题和隔离等级">4.2 事务的并发问题和隔离等级</h3><p>并发问题：</p><ul><li>丢失修改</li><li>脏读：当前事务可以查看到别的事务未提交的数据。</li><li>不可重读：在同一事务中，使用相同的查询语句，同一数据资源莫名改变了。</li><li>幻读：在同一事务中，使用相同的查询语句，莫名多出了一些之前不存在的数据，或莫名少了一些原先存在的数据，例如第二次读取前，表格被插入了新行。</li></ul><p>隔离等级</p><ul><li><p>读未提交： 一个事务还没提交，它做的变更就能被别的事务看到。</p></li><li><p>读已提交： 一个事务提交后，它做的变更才能被别的事务看到。</p></li><li><p>可重复读： 一个事务执行过程中看到的数据总是和事务启动时看到的数据是一致的。在这个级别下事务未提交，做出的变更其它事务也看不到。</p></li><li><p>串行化： 对于同一行记录进行读写会分别加读写锁，当发生读写锁冲突，后面执行的事务需等前面执行的事务完成才能继续执行。</p></li></ul><p>并发问题的解决：</p><p><img src="/2022/02/09/MySQL-Basic/img1.PNG"></p><h2 id="mysql的锁innodb">5. MySQL的锁（InnoDB）</h2><h3 id="锁的性质">5.1 锁的性质</h3><h4 id="共享性">5.1.1 共享性</h4><p>共享锁：其他事务可以读但不能写，又称为读锁</p><p>排他锁：其他事务不能读写，又称为写锁</p><h4 id="粒度">5.1.2 粒度</h4><p><strong>表级锁:</strong> 对当前操作的整张表加锁,实现简单，加锁快，但并发能力低。</p><p><strong>行锁:</strong> 锁住某一行，如果表存在索引，那么记录锁是锁在索引上的，如果表没有索引，那么 InnoDB 会创建一个隐藏的聚簇索引加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁慢，会出现死锁。</p><p><strong>Gap 锁</strong>：锁住一个间隙以防止插入，但不包括间隙的两端。假设索引列有2, 4, 8 三个值，如果对 4 加行锁，那么也会同时对(2,4)和(4,8)这两个间隙加锁。其他事务无法插入值所对应的索引值在这两个间隙之间的记录</p><p><strong>next-key-lock：</strong>next-key lock 实际上就是 行锁+这条记录前面的 gap lock 的组合。假设有索引值10,11,13和 20,那么可能的 next-key lock 包括:</p><p>(负无穷,10],(10,11],(11,13],(13,20],(20,正无穷)</p><p>在 RR 隔离级别下，InnoDB 使用 next-key lock 主要是防止幻读问题产生。</p><h4 id="意向锁">5.1.3 意向锁</h4><p>意向锁必须是表级锁，可以有共享性和排他性。意向锁的作用在于保护不同级别的锁共享性不冲突，或者说提高检测这一冲突的效率。</p><p>假设事务A正在update表t的某一行r，并给这一行添加了排他锁，同时，事务B想要给表t上一个表级共享锁，为了防止共享性冲突，事务B必须检查表t的每一行，检查是否有A上的锁这样的排他锁，然后才能上锁</p><p>因此，A给表t中的行r上排他锁时，同时还会给表t上一个表级的排他意向锁，此时B所上的表级共享锁将会被阻塞</p><p>注意，意向锁只会阻塞表级锁，上例中，如果有另一事务C试图给另一行r2加共享锁，则A所加的排他意向锁不会对其进行阻塞</p><h4 id="死锁">5.1.4 死锁</h4><p>两个事物同时上锁进入互相等待的死循环。</p><p>如何解决死锁：</p><ol type="1"><li>合理设计索引，尽可能通过索引定位行，防止过多的表和行被锁住，减少竞争</li><li>调整SQL语句顺序，update，delete等长时间持有锁的语句放在后面</li><li>把大事物分解成小事务执行</li><li>必要时，可以使用<code>KILL</code>杀死一些进程，释放其持有的锁</li></ol><p>详细分析方法可参考<a href="https://z.itpub.net/article/detail/7B944ED17C0084CF672A47D6E938B750">这篇文章</a></p><h4 id="乐观性">5.1.5 乐观性</h4><p>乐观锁：对于数据冲突保持一种乐观态度，操作数据时不会对操作的数据进行加锁，只有到数据提交的时候才通过一种机制来验证数据是否存在冲突。</p><p>悲观锁：对于数据冲突保持一种悲观态度，在修改数据之前把数据锁住，然后再对数据进行读写，在它释放锁之前任何人都不能对其数据进行操作，直到前面一个人把锁释放后下一个人数据加锁才可对数据进行加锁，然后才可以对数据进行操作，一般数据库的锁都是悲观锁</p><h3 id="上锁方式">5.2 上锁方式</h3><h4 id="表锁">5.2.1 表锁</h4><p>隐式上锁：使用一些关键词时，自动添加自动释放的上锁。如select语句会给表上共享锁，insert，update，delete会给表上排他锁</p><p>显示上锁：</p><figure class="highlight sql"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs SQL">LOCK <span class="hljs-keyword">TABLE</span> t1 READ  <span class="hljs-comment">-- 给t1表上共享锁</span><br>LOCK <span class="hljs-keyword">TABLE</span> t1 WRITE  <span class="hljs-comment">-- 给t1表上排他锁</span><br>UNLOCK <span class="hljs-keyword">TABLE</span> t1 <span class="hljs-comment">-- 解锁t1</span><br>UNLOCK TABLES <span class="hljs-comment">-- 解锁所有表</span><br></code></pre></td></tr></table></figure><h4 id="行锁">5.2.2 行锁</h4><p>隐式上锁：使用一些关键词时，自动添加自动释放的上锁。insert，update，delete会给选中的行上排他锁。注意select不会给行加锁</p><p>显示上锁：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> t1 <span class="hljs-keyword">IN</span> SHARE MODE <span class="hljs-comment">-- 给选中的行加共享锁</span><br><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> t1 <span class="hljs-keyword">FOR</span> UPDATE <span class="hljs-comment">-- 给选中的行加排他锁</span><br><span class="hljs-keyword">COMMIT</span><br><span class="hljs-keyword">ROLLBACK</span> <span class="hljs-comment">-- 事务完成或回滚时，会解除事物中所加的行锁</span><br>KILL <span class="hljs-comment">-- 杀死某一进程，会解除这一进程所加的行锁</span><br></code></pre></td></tr></table></figure><h2 id="mysql的日志">6. MySQL的日志</h2><h3 id="log种类">6.1 log种类</h3><p>redo log: 存储引擎级别的log（InnoDB有，MyISAM没有），该log关注于事务的恢复。在重启mysql服务的时候，根据redo log进行重做，从而使事务有持久性。</p><p>undo log：是存储引擎级别的log（InnoDB有，MyISAM没有）保证数据的原子性，该log保存了事务发生之前的数据的一个版本，可以用于回滚，是MVCC的重要实现方法之一。</p><p>如果需要执行事务，使用redo log，执行失败则使用undo log，这样的组合保证了事务的一致性</p><p>bin log：数据库级别的log，关注恢复数据库的数据。有关bin log和redo log的区别</p><ol type="1"><li>redo log是InnoDB引擎特有的，只记录该引擎中表的修改记录。binlog是MySQL的Server层实现的，会记录所有引擎对数据库的修改，具有崩溃修复能力。InnoDB通过redo log保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe</li><li>redo log是物理日志，记录的是在具体某个数据页上做了什么修改；binlog是逻辑日志，记录的是这个语句的原始逻辑，例如语句的增删改，并不记录数据页具体发生了什么改变，因此单独的bin log不具备崩溃修复能力。</li><li>redo log是循环写的，空间固定会用完；binlog是可以追加写入的，binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</li></ol><h3 id="wal技术">6.2 WAL技术</h3><p>WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。事务在提交写入磁盘前，会先写到redo log里面去。如果直接写入磁盘涉及磁盘的随机I/O访问，涉及磁盘随机I/O访问是非常消耗时间的一个过程，相比之下先写入redo log，后面再找合适的时机批量刷盘能提升性能。</p><h3 id="两阶段提交">6.3 两阶段提交</h3><p>为了保证binlog和redo log两份日志的逻辑一致，最终保证恢复到主备数据库的数据是一致的，采用两阶段提交的机制。</p><ol type="1"><li>执行器调用存储引擎接口，存储引擎将修改更新到内存中后，将修改操作记录redo log中，此时redo log处于prepare状态。</li><li>存储引擎告知执行器执行完毕，执行器生成这个操作对应的binlog，并把binlog写入磁盘。</li><li>执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交commit状态，更新完成</li></ol><h3 id="mysql-主从机制">6.4 MySQL 主从机制</h3><h4 id="主从配置">6.4.1主从配置</h4><p>一个服务器充当主服务器（master），其余的服务器充当从服务器（slave）。从服务器主要用来读，主服务器主要用写，从服务器定期与主服务器进行同步，复制其数据。因为复制是异步进行的，所以从服务器不需要一直连接着主服务器</p><p>主从机制的优点：</p><ol type="1"><li>数据存在多个镜像，可以防止单一主机崩溃和数据丢失，如果主机宕机可以切换到从服务器上（由于异步同步，可能数据一致性存在问题）</li><li>从服务器可以分担主服务器的读的压力</li></ol><p>注意，部分数据实时性强，经常会被更新，这类数据不适合放在从服务器上，容易引发错误</p><h4 id="主从复制">6.4.2 主从复制</h4><p>MySQL主从复制流程：</p><ol type="1"><li>在事务完成之前，主库在binlog上记录这些改变，完成binlog写入过程后，主库通知存储引擎提交事物</li><li>从库将主库的binlog复制到对应的中继日志，即开辟一个I/O工作线程，I/O线程在主库上打开一个普通的连接，然后开始binlog dump process，将这些事件写入中继日志。从主库的binlog中读取事件，如果已经读到最新了，线程进入睡眠并等待ma主库产生新的事件。</li></ol><h2 id="多版本并发控制mvcc">7. 多版本并发控制(MVCC)</h2><p><strong>多版本并发控制（MVCC）</strong> 是通过保存数据在某个时间点的快照来实现并发控制的。也就是说，不管事务执行多长时间，事务内部看到的数据是不受其它事务影响的，根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。</p><p>可以认为MVCC 是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此可以实现读写并发，并降低死锁概率，IO开销更低。</p><p>MVCC只在 可重复读（REPEATABLE READ） 和读已提交（READ COMMITTED） 两个隔离级别下工作。其他两个隔离级别都和 MVCC 不兼容</p><p>通过MVCC，不显式加锁的一般select语句使用的都是快照读，即读取数据在事务语句执行前或执行后的历史版本，而非当前数据，也即是说其他并发的事物无法影响到该快照。而显式加锁的select语句执行的是当前读，获得目标位置最新的数据</p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MySQL</tag>
      
      <tag>Interview Knowledge</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Statistics for Data Science</title>
    <link href="/2022/02/09/statistics-for-ds/"/>
    <url>/2022/02/09/statistics-for-ds/</url>
    
    <content type="html"><![CDATA[<h1 id="statistics-for-data-science">Statistics for Data Science</h1><h2 id="term-in-statistics">Term in statistics</h2><p><strong>statistics</strong></p><p>Statistics gather, describe and analyze sample data in a numerical way to understand the whole date</p><p><strong>target population</strong></p><p>A particular group of interest</p><p><strong>sample population</strong></p><p>A group which the sample is taken</p><p><strong>sample frame</strong></p><p>A physical list of all members of the sampled population</p><p><strong>sample</strong></p><p>A subset of the sample population from which data are collected</p><p>Ideally, sample should be representative to sample population, the sample population should be the same or representative to target population.</p><p><img src="/2022/02/09/statistics-for-ds/1.PNG" alt style="zoom:80%;"></p><p><strong>census</strong></p><p>A study on every member in the target population</p><p><strong>variable</strong></p><p>A value or characteristic that changes among members of the population</p><p><strong>data </strong></p><p>the actual counts, measurements or observation about the variables that you markdown with yout samples</p><p><strong>parameter</strong></p><p>A numerical description of a population characteristic (like mean of a variable's data)</p><p><strong>sample statistics</strong></p><p>A numerical description of a sample characteristic</p><h2 id="data-classification">Data Classification</h2><h3 id="categorical-numeric">Categorical &amp; Numeric</h3><p><strong>Categorical</strong>: consist of labels or description of traits. It's meaning less to apply quantitative calculation on it</p><p><strong>Numeric</strong>: consist of counts and measurement, it have meanings when you apply quantitative calculations</p><h3 id="discrete-continuous">Discrete &amp; Continuous</h3><p><strong>Discrete</strong>: numeric data that can take only particular values(counts, rate stars)</p><p><strong>Continuous</strong>: numeric data that can take any values in an interval</p><p><img src="/2022/02/09/statistics-for-ds/3.PNG"></p><h3 id="noir">NOIR</h3><p><strong>Nominal level data</strong>: description of categorical data that order do not matters</p><p><strong>Ordinal level data</strong>: categorical data that have a meaningful order(still not meaningful to add or divide)</p><p><strong>Interval level data</strong>: numeric data that can be arranged in a meaningful order, and the different between data entries are meaningful(timestamp, shoe size, temperature degree)</p><p><strong>Ratio level data</strong>: Interval data where zero indicates absence of something(like body height, 0 inch body high do not have a actual meaning, it means the data are not collected, where 0 degree do have an actual meaning, so it is not a ratio data.) As ratio data cannot participate calculation if data is not included, it must be non-zero in a calculation, thus it can divide other numeric data, that is why it is called ratio data</p><p><img src="/2022/02/09/statistics-for-ds/2.PNG"></p><h2 id="the-process-of-a-statistics-study">The Process of a Statistics Study</h2><h3 id="general-process-of-conducting-a-statistic-study">General process of conducting a statistic study</h3><ol type="1"><li>Determine the design of the study<ol type="1"><li>state the question to be studied</li><li>determine the population and variables</li><li>determine the sampling method</li></ol></li><li>Collect data<ol type="1"><li>An <a href="#observational-study">observational study</a> observe data that already exist</li><li>An experiment generates data to help find cause-and-effect relationship</li></ol></li><li>Organize the data</li><li>Analyze the data</li></ol><h3 id="methods-of-sampling">Methods of sampling:</h3><p><strong>simple random sample</strong></p><p>Every sample in the sample population has a equal chance to be selected</p><p><strong>stratified sample</strong></p><p>The population is divided into subgroups called strata. The grouping variable should be correlated with(or is) the variable we care about. For example, we want to find the relationship between tree's age and apples's color, but in our 100 sample, 80 observations have a red apple, 20 have a green apple. If we randomly 10 sample, we might select a sample with all red apple. In this case, we cannot analyze the relationship between age and color since the sample indicates trees of any ages grow apple of same color.</p><p>By applying stratified, we separate the observations into two groups according to apple color, and we select same percentage of observations from both group(8 from red group and 2 from blue group) and then we analyze cause-and-effect relationship</p><p><strong>Cluster Sample</strong></p><p>The population is divided into subgroups called clusters. The grouping variable should be correlated with the variable we care about.</p><p><strong>Systematic sample</strong></p><p>select every <span class="math inline">\(n^{th}\)</span> of the population</p><p><strong>Convenient sample</strong></p><p>sample on a personal bias, very unethical method to use</p><h3 id="observational-study">Observational Study</h3><p><strong>cross-sectional study</strong>: data are collected towards different members in a single point in time</p><p><strong>longitudinal study</strong>: data are gathered by following a particular group over a period of time</p><h3 id="experiment">Experiment</h3><p><strong>treatment</strong>: a condition applied to a group of subjects in an experiment</p><p><strong>subjects</strong>: people or thing to be studied in a group</p><p><strong>response variable</strong>: the variable in an experiment that responds to the treatment</p><p><strong>explanatory variable</strong>: the variable in an experiment that causes the change in the response variable</p><p><strong>control group</strong>: a group in the experiment that has no treatment applied</p><p><strong>treatment group</strong>: a group in the experiment with a treatment applied</p><p><strong>confounding variable</strong>: unmeasured factors other than the treatment that cause an effect on the subjects of an experiment</p><p><strong>single-blind experiment</strong>: subjects do not know which group they are in, but the people interacting(measurer) with the subjects know</p><p><strong>double-blind experiment</strong>: neither the subjects not the measurer know which group each subject belongs</p><p><strong>placebo</strong>: a substance or method to make subjects believe all subjects in both groups receive same treatment(in order to exclude personal factor)</p><p><strong>Institutional Review Board</strong>: a group of people who review the experiment design to make sure no harm will come to subjects involved</p><p><strong>Informed consent</strong>: completely disclosing to participant the goals and procedures of a study and obtaining their agreement to participate</p><p>Three principles of experiment design:</p><ol type="1"><li>randomize the control and treatment group</li><li>control for the outside effects on the response variable</li><li>replicate the experiment a significant number of times to see meaningful patterns</li></ol>]]></content>
    
    
    <categories>
      
      <category>Statistics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basic Statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MongoDB Guildbook</title>
    <link href="/2022/02/06/MongoDB-Guildline/"/>
    <url>/2022/02/06/MongoDB-Guildline/</url>
    
    <content type="html"><![CDATA[<h1 id="mongodb">MongoDB</h1><h2 id="about-mongodb">About MongoDB</h2><p>MongoDB is a open- source NoSQL database stroing data in json like documents with schema.</p><p>mongoDB do not have concepts like join.</p><p>mongoDB provides APIs for most programing language</p><h2 id="three-ways-to-access-mongodb">Three ways to access MongoDB</h2><ol type="1"><li>Community server</li><li>VS extending</li><li>MongoDB Altas</li></ol><h2 id="concepts-in-mondodb">Concepts in MondoDB:</h2><p><strong>Document:</strong> a set of K-V pairs. Every document has a unique value via key "_id". Documents have dynamic schema, documents in same collection can have different schema. They. Can hold data of any types allowed by mongodb.</p><p><strong>Collection:</strong> a group of mongodb documents, similar to "tables in other database. Unlike tables, collections does nothave any schema definition, and it cannnot be join. Usually, documents with in a collection belonging to a particular subject.</p><p><strong>Database:</strong> A database is a container of collections of data</p><p><strong>Comparison between RDBMS and MongoDB</strong></p><table><colgroup><col style="width: 8%"><col style="width: 30%"><col style="width: 12%"><col style="width: 48%"></colgroup><thead><tr class="header"><th style="text-align: left;">RDBMS</th><th style="text-align: left;"></th><th></th><th></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Tables</td><td style="text-align: left;">stand for entity</td><td>Collections</td><td>A set of documents representing one object</td></tr><tr class="even"><td style="text-align: left;">rows</td><td style="text-align: left;">stand for an actual record</td><td>Documents</td><td>a json objects</td></tr><tr class="odd"><td style="text-align: left;">columns</td><td style="text-align: left;">stand for attributes</td><td>Fields</td><td>The first level of the schema</td></tr></tbody></table><h2 id="start-with-mongodb">Start with MongoDB</h2><ol type="1"><li><p>Build a sever(for Mac)</p><figure class="highlight bash"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs bash">mongod --config /opt/homebrew/etc/mongod.conf<br></code></pre></td></tr></table></figure><p>This create a temp mongo server. If you close the session by ctrl+c, he connecttion would be terminated</p><p>Or you can use the command</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">brew services start mongodb/brew/mongodb-community<br></code></pre></td></tr></table></figure><p>This create a back-support mongoDB server, you can still access to it after you close the terminal, to stop the back-support:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">brew services stop mongodb/brew/mongodb-community<br></code></pre></td></tr></table></figure></li><li><p>Start a connection</p><ol type="1"><li>After start the server, you can type <code>mongo</code> in bash to start a connection. Use <code>exit</code> to stop the connection</li><li>you can use mongodb compass to start a connection, this is a mongondb management UI</li><li>use Datagrip/Dataspell to build a connection</li></ol><p>Notes:</p><ul><li>The port for local host is always 27017</li><li>There's no default username and password</li></ul></li></ol><h2 id="data-types-in-mongodb">Data types in MongoDB</h2><p>Allowed data dytes in mongoDB include:</p><ul><li><p>BSON</p><p>​ A mongoDB data type, it's binary encoded json that can processed faster, it support some types(data, timestamp,object id) that are not supported by json</p></li><li><p>JSON</p></li><li><p>Integer</p></li><li><p>Boolean</p></li><li><p>Double</p></li><li><p>Arrays</p></li><li><p>Objects</p><p>​ Used to store embedded documents. If a documents A contains a K-V pair {"file":B}, where B is another document, the type of B in A's schema is Object</p></li><li><p>Null</p></li><li><p>Date</p></li><li><p>Timestemp</p></li><li><p>Object ID</p><p>​ ObjectIds are small, likely unique, fast to generate, and ordered. It's a usually used as a PK of a document. ObjectId values are 12 bytes in length, consisting of a 4-bytes timestamp value representiong the ObjectId's creation, a 5-bytes random value, a 3-byte incrementing value</p></li><li><p>Code</p><p>​ Like javascript code</p></li></ul><h2 id="create-and-drop-database">Create and Drop Database</h2><p>you can create databse by:</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">use</span> &lt;database-<span class="hljs-keyword">name</span>&gt;<br></code></pre></td></tr></table></figure><p>It would select the DB, if not exists, it create the DB</p><p>Notes: The created databse will not be visible untill you insert any data into it</p><p>To drop the database:</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gcode">db.dropDatabase<span class="hljs-comment">()</span><br></code></pre></td></tr></table></figure><p>where <code>db</code> refer to the currently used database.</p><p>Before you drop the DB, makesure you select the DB first.</p><p>Some other commands related to create and drop</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-literal">show</span> databases -- list <span class="hljs-literal">all</span> visible DB<br>db -- <span class="hljs-literal">show</span> the <span class="hljs-literal">current</span> DB<br></code></pre></td></tr></table></figure><h2 id="create-and-drop-colletions">Create and drop colletions</h2><p>to create a collection in Database:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.create<span class="hljs-constructor">Collection(<span class="hljs-string">&quot;name&quot;</span>,<span class="hljs-params">options</span>)</span><br></code></pre></td></tr></table></figure><p>to drop a collection:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">db</span>.collection_name.<span class="hljs-keyword">drop</span>()<br></code></pre></td></tr></table></figure><p>to show collections in current DB</p><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dart"><span class="hljs-keyword">show</span> collections<br></code></pre></td></tr></table></figure><h2 id="insert-documents">Insert documents</h2><p>to create one documents:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>insert<span class="hljs-constructor">One(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;)</span>;<br></code></pre></td></tr></table></figure><p>to create many documents:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>insert<span class="hljs-constructor">Many([&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;B&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;C&quot;</span>&#125;])</span><br></code></pre></td></tr></table></figure><p>Note: the input of insertMany() should be a list[]</p><h2 id="update-documents">Update documents</h2><p>To update a documents</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.updateOne</span>(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;B&quot;</span>&#125;,&#123;<br>        <span class="hljs-variable">$set</span>:&#123;<br>            <span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;123456&quot;</span><br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><p>Where:</p><ul><li>the first parameter eplicts the documents to update</li><li>the second parameter explicts the operation to conduct</li></ul><p>Notes:This command only applies to the first document that meet the search condition(the first pararmeter)</p><p>to update many documents:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.updateMany</span>(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;123456&quot;</span>&#125;,&#123;<br>        <span class="hljs-variable">$set</span>:&#123;<br>            <span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span><br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><h2 id="read-data">Read data</h2><p>To read(and modify) data:</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-comment">-- find all documents in the collection</span><br>db.C1.find()<br><br><span class="hljs-comment">--find particular documents </span><br>db.C1.find(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>,<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span>&#125;) <br><br><span class="hljs-comment">-- find the first documents that fits the condition</span><br>db.C1.findOne(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;654321&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and delete the first documenttaht fits the condition</span><br>db.C1.findOneAndDelete(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;D&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and replace the first documenttaht fits the condition</span><br><span class="hljs-comment">-- the second parameter gives a whole documents(like the format in insert)</span><br>db.C1.findOneAndReplace(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>,<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;23456&quot;</span>&#125;)<br><br><span class="hljs-comment">-- find and update</span><br><span class="hljs-comment">-- the second parameter gives a the operation to apply on the  documents(like the format in update)</span><br>db.C1.findOneAndUpdate(&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>&#125;,&#123;<br>$<span class="hljs-keyword">set</span>:&#123;<br><span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span><br>&#125;<br>&#125;<br>   )<br>   <br><span class="hljs-comment">-- find and Modify</span><br><span class="hljs-comment">-- pass a json that tell the command to conduct multiple operaions</span><br>db.C1.findAndModify(<br>    &#123;<br>    <span class="hljs-string">&quot;query&quot;</span>: &#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;E&quot;</span>&#125;,<br>    <span class="hljs-string">&quot;update&quot;</span>:&#123;$<span class="hljs-keyword">set</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;A&quot;</span>&#125;&#125;,<br>    &#125;<br>)<br><br></code></pre></td></tr></table></figure><h2 id="delete-documents">Delete documents</h2><p>to delete documents from a collection:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">-- delete the first documetn fits the condition<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>delete<span class="hljs-constructor">One(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;)</span><br>-- delete all documents fit the condition<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>delete<span class="hljs-constructor">Many(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;)</span><br></code></pre></td></tr></table></figure><h2 id="query">Query</h2><p>The first parameter is called query condition, by passing a json map, you tell the command constraints of fields.</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>find(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>,<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;T&quot;</span>&#125;)<br></code></pre></td></tr></table></figure><p>you can also use the $and operand:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">db.C1.<span class="hljs-builtin-name">find</span>(<br>&#123;<br><span class="hljs-variable">$and</span>:[<br>&#123;<span class="hljs-string">&quot;mobile&quot;</span>:<span class="hljs-string">&quot;12345&quot;</span>&#125;,&#123;<span class="hljs-string">&quot;name&quot;</span>:<span class="hljs-string">&quot;T&quot;</span>&#125;<br>]<br>&#125;<br>)<br></code></pre></td></tr></table></figure><p>Other operand in this formats:</p><ul><li>or</li><li>nor</li></ul><p>to query documents with quantify condition:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">db.C1.<span class="hljs-builtin-name">find</span>(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:&#123;<span class="hljs-variable">$gte</span>:<span class="hljs-string">&quot;20000&quot;</span>&#125;&#125;)<br></code></pre></td></tr></table></figure><p>Other operand in this formats:</p><ul><li>lte: less than ot equal</li><li>gt: greater than</li><li>lt: less than</li><li>eq: equal</li><li>neq: not equal</li></ul><h2 id="select-specific-fields">Select specific fields</h2><p>to select specific fields:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">var</span> pipline = [<br>    &#123;<span class="hljs-variable">$sort</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;&#125;<br>]<br><span class="hljs-keyword">db</span>.C1.aggregate(pipline)<br></code></pre></td></tr></table></figure><p>Notes:</p><ul><li>the first parameter explicts the query condition</li><li>the second parameter explicts the fields you want or do not want(Projection)</li><li><strong>You can not mix inclusion and exclusion</strong> in the second parameter like {"name":0,"mobile":1}</li><li>**The only field that can be mixed is "_id"** of documents. {"name":1,"_id":0}</li></ul><h2 id="projection">Projection</h2><p>Projection is a mechanism allowing you to select specific fieds, like slice of an array.</p><h2 id="aggregation">Aggregation</h2><p>To perform aggregation on collections:</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">var</span> pipline = [<br>    &#123;<span class="hljs-variable">$sort</span>:&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;&#125;,<br>    &#123;<span class="hljs-variable">$limit</span>:4&#125;<br>]<br><span class="hljs-keyword">db</span>.C1.aggregate(pipline)<br></code></pre></td></tr></table></figure><p>where pip line is list of operations,</p><ul><li>$count</li><li>$group</li><li>$limit</li><li>$lookup</li><li>$match</li><li>$merge</li><li>$sort</li><li>project</li><li>unwind</li><li>unset</li></ul><h2 id="limit-and-skip">Limit and skip</h2><p>Limit the results returned by:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.limit</span>(<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><p>showing the first there results</p><p>Skip the first 2 results and show the rest:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.skip</span>(<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="sort">Sort</h2><p>to sort results:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.find</span>()<span class="hljs-selector-class">.sort</span>(&#123;<span class="hljs-string">&quot;name&quot;</span>:-<span class="hljs-number">1</span>&#125;)<br></code></pre></td></tr></table></figure><p>where the json map in sort() specify the sorting depending on which fields. 1 for ascending and -1 for descending. If you pass multiple fields, it sort the next fields in the groups of previous fields.</p><h2 id="create-and-drop-index">Create and Drop index</h2><p>Create indexes:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>create<span class="hljs-constructor">Index(&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;)</span><br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>create<span class="hljs-constructor">Indexes([&#123;<span class="hljs-string">&quot;name&quot;</span>:1&#125;,&#123;<span class="hljs-string">&quot;mobile&quot;</span>:1&#125;])</span><br></code></pre></td></tr></table></figure><p>Drop index:</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>drop<span class="hljs-constructor">Index(&#123;<span class="hljs-string">&quot;mobile&quot;</span>:1&#125;)</span><br><br>-- drop all indexes except _id<br>db.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">C1</span>.</span></span>drop<span class="hljs-constructor">Indexes()</span><br></code></pre></td></tr></table></figure><p>Group by:</p><p>Group by operand:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">db<span class="hljs-selector-class">.C1</span><span class="hljs-selector-class">.aggregate</span>(<br>    &#123;<br>        <span class="hljs-variable">$group</span>:&#123;<br>            _id:<span class="hljs-string">&quot;$name&quot;</span>,<br>            <span class="hljs-string">&quot;count&quot;</span>:&#123;<span class="hljs-variable">$sum</span>:<span class="hljs-number">1</span>&#125;<br>        &#125;<br>    &#125;<br>)<br></code></pre></td></tr></table></figure><p>Where:</p><ul><li>_id is an essential arguments, it explain group by which field</li><li>put a $ before the field to group by, it's necessary</li><li>if you want to group by multiple levels, use _id:["$name","$mobile"]</li><li>the "count" is an alias defined by user</li><li>{$sum:1} = count(*)</li><li>{$sum:"$field"} = sum(field)</li><li>other operans includes: $avg, $min, $max</li><li>$push: push all values of the given field in the group into one array</li><li>$addToSet: same as $push, but return a unique set</li><li>$first, $last: return the first/last value</li><li>if _id:null, then return all documents in one group</li></ul><h2 id="back-up-restore">Back up &amp; Restore</h2><p>to back up all databases</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">-- dump the current database<br>mongodump<br></code></pre></td></tr></table></figure><p>To restore all databases</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mongorestore<br></code></pre></td></tr></table></figure><h2 id="transaction">Transaction</h2><p>For transation realted content in MongoDB, please refer to <a href="https://zhuanlan.zhihu.com/p/71679945">this link</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Database</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NoSQL</tag>
      
      <tag>MongoDB</tag>
      
      <tag>Guidebook</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
